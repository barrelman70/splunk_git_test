{
    "title": "AWS Bill - Current Month Cost until Now",
    "search": "| savedsearch \"AWS Bill - Monthly Latest Snapshot\" | search RecordType=StatementTotal | eval date_month=strftime(_time, \"%Y-%m\") | eval current_month=strftime(now(), \"%Y-%m\") | where date_month=current_month | stats sum(TotalCost) as TotalCost, first(CurrencyCode) as CurrencyCode",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Current Month Cost until Now by Linked Account",
    "search": "| savedsearch \"AWS Bill - Monthly Latest Snapshot\" | search RecordType=AccountTotal | eval date_month=strftime(_time, \"%Y-%m\") | eval current_month=strftime(now(), \"%Y-%m\") | where date_month=current_month | stats sum(TotalCost) as TotalCost, first(CurrencyCode) as CurrencyCode by LinkedAccount",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Current Month Cost until Now by Service",
    "search": "| savedsearch \"AWS Bill - Monthly Latest Snapshot\" | search RecordType=LinkedLineItem | eval date_month=strftime(_time, \"%Y-%m\") | eval current_month=strftime(now(), \"%Y-%m\") | where date_month=current_month | stats sum(TotalCost) as TotalCost, first(CurrencyCode) as CurrencyCode by ProductName",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Daily Cost through Last Month - Blended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |   timechart span=1day sum(BlendedCost) as TotalCost",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Daily Cost through Last Month - Unblended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |   timechart span=1day sum(UnBlendedCost) as TotalCost",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Daily Cost through Last Month by Linked Account - Blended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |   timechart span=1day sum(BlendedCost) as TotalCost by LinkedAccount limit=20",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Daily Cost through Last Month by Linked Account - Unblended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |   timechart span=1day sum(UnBlendedCost) as TotalCost by LinkedAccount limit=20",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Daily Cost through Last Month by Region - Blended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |  timechart span=1day sum(BlendedCost) as TotalCost by AvailabilityZone limit=20",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Daily Cost through Last Month by Region - Unblended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |  timechart span=1day sum(UnBlendedCost) as TotalCost by AvailabilityZone limit=20",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Daily Cost through Last Month by Service - Blended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |   timechart span=1day sum(BlendedCost) as TotalCost by ProductName limit=20",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Daily Cost through Last Month by Service - Unblended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |   timechart span=1day sum(UnBlendedCost) as TotalCost by ProductName limit=20",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Detailed Cost Latest Snapshot",
    "search": "eventtype=aws_billing_detail_report  [search eventtype=aws_billing_detail_report RecordType=StatementTotal | dedup report_month sortby -_time | return 1000 S3KeyLastModified]",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "To change index, revise the search string: index=\"index\" eventtype=aws_billing_detail_report [seardch index=\"your index\" ...(same as the left part)...",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Monthly Cost",
    "search": "| savedsearch \"AWS Bill - Monthly Latest Snapshot\" | search RecordType=StatementTotal | timechart span=1mon sum(TotalCost) as TotalCost",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Monthly Cost by Linked Account",
    "search": "| savedsearch \"AWS Bill - Monthly Latest Snapshot\" | search RecordType=AccountTotal | timechart span=1mon sum(TotalCost) by LinkedAccount limit=20",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Monthly Cost by Service",
    "search": "| savedsearch \"AWS Bill - Monthly Latest Snapshot\" | search RecordType=LinkedLineItem  | timechart span=1mon sum(TotalCost) as TotalCost by ProductName limit=20",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Monthly Cost through Last Month by Region - Blended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |  timechart span=1mon sum(BlendedCost) as TotalCost by AvailabilityZone limit=20",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Monthly Cost through Last Month by Region - Unblended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |  timechart span=1mon sum(UnBlendedCost) as TotalCost by AvailabilityZone limit=20",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Monthly Latest Snapshot",
    "search": "eventtype=aws_billing_monthly_report [search eventtype=aws_billing_monthly_report | dedup report_month sortby -_time | return 1000 S3KeyLastModified]",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "To change index, revise the search string: index=\"index\" eventtype=aws_billing_monthly_report [search index=\"index\" ...(same as the left part)...",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Total Cost through Last Month by Region - Blended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |  stats sum(BlendedCost) as TotalCost by AvailabilityZone",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Total Cost through Last Month by Region - Unblended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |  stats sum(UnBlendedCost) as TotalCost by AvailabilityZone",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Total Cost until Now",
    "search": "| savedsearch \"AWS Bill - Monthly Latest Snapshot\" | search RecordType=StatementTotal | stats sum(TotalCost) as TotalCost, first(CurrencyCode) as CurrencyCode",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Total Cost until Now by Linked Account",
    "search": "| savedsearch \"AWS Bill - Monthly Latest Snapshot\" | search RecordType=AccountTotal  | stats sum(TotalCost) as TotalCost, first(CurrencyCode) as CurrencyCode by LinkedAccount",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Total Cost until Now by Service",
    "search": "| savedsearch \"AWS Bill - Monthly Latest Snapshot\" | search RecordType=LinkedLineItem | stats sum(TotalCost) as TotalCost, first(CurrencyCode) as CurrencyCode by ProductName",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Total Daytime Cost through Last Month - Blended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |  eval date_hour=strftime(_time, \"%H\") | search (date_hour>=7 AND date_hour<=17) | stats sum(BlendedCost) as TotalCost",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Total Daytime Cost through Last Month - Unblended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |  eval date_hour=strftime(_time, \"%H\") | search (date_hour>=7 AND date_hour<=17) | stats sum(UnBlendedCost) as TotalCost",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Total Nighttime Cost through Last Month - Blended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |  eval date_hour=strftime(_time, \"%H\") | search (date_hour < 7 OR date_hour > 17) | stats sum(BlendedCost) as TotalCost",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AWS Bill - Total Nighttime Cost through Last Month - Unblended",
    "search": "| savedsearch \"AWS Bill - Detailed Cost Latest Snapshot\" | search RecordType=LineItem |  eval date_hour=strftime(_time, \"%H\") | search (date_hour < 7 OR date_hour > 17) | stats sum(UnBlendedCost) as TotalCost",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Addon Metadata - Migrate AWS Accounts",
    "search": "| listawsaccounts | collect `aws-account-index`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Addon Metadata - Summarize AWS Inputs",
    "search": "| listawsinputs | collect `aws-input-index`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "0",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "ASDL_Splunk_TA_aws",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Core Dumps Disabled",
    "search": "`comment(\"Core dumps are disabled, if a crash occurs the Splunk Support team might not be able to assist without the core dump\")`\n`comment(\"https://answers.splunk.com/answers/223838/why-are-my-ulimits-settings-not-being-respected-on.html applies to core limits so if the server has been rebooted the init.d may need a ulimit -Hc/Sc setting for this as well...\")`\nindex=_internal \"WARN  ulimit - Core file generation disabled\" `splunkenterprisehosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) \n| stats max(_time) AS mostRecentlySeen by host\n| eval mostRecentlySeen = strftime(mostRecentlySeen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Core Dumps are disabled and this may make support cases more difficult as sometimes the core dump is required for troubleshooting purposes.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "27 7 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Detect LDAP groups that no longer exist",
    "search": "`comment(\"Find any LDAP groups that are reporting that they do not exist so can therefore be removed from the Splunk configuration\")`\n`comment(\"This appears to occur only after restarts of the Splunk server, however it is useful to know about as the authentication.conf can be cleaned up once found\")`\nindex=_internal sourcetype=splunkd `splunkenterprisehosts` `splunkadmins_splunkd_source` \"was not found on the LDAP server\"\n| eval message=coalesce(message,event_message)\n| stats max(_time) AS mostRecentlySeen, min(_time) AS firstSeen by message, host\n| eval mostRecentlySeen=strftime(mostRecentlySeen, \"%+\"), firstSeen=strftime(firstSeen, \"%+\")\n| fields host, mostRecentlySeen, firstSeen, message",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. An LDAP group is configured in Splunk that does not exist in LDAP, this is a minor issue but it can be fixed by removing it from authentication.conf",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 6 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Email Sending Failures",
    "search": "`comment(\"Find any failures to send emails due to either the size of the email or the email server not working or similar\")`\nindex=_internal `splunkenterprisehosts` \"stderr from \" python* sendemail.py sourcetype=splunkd (`splunkadmins_splunkd_source`)\n| eval message=coalesce(message,event_message)\n| dedup message \n| rex \"ssname=(?P<savedsearch>[^\\\"]+)\"\n| rex \"stderr from '[^']+':\\s+(?P<error>.*)\"\n| rex field=results_file \".*/dispatch/[^_]+__(?P<user>[^_]+)\"\n| eval time=strftime(_time, \"%+\")\n| stats count, values(time) AS time by error, savedsearch, user\n| table time, count, error, savedsearch, user",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Ideally this action shouldn't be using email but this should fire when the email server is throwing errors",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "3 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - File integrity check failure",
    "search": "`comment(\"One or more files did not pass the startup hash-check against the Splunk provided manifest, you can tune the limits.conf to control how the warning is logged or not logged\")`\nindex=_internal `splunkenterprisehosts` \"An installed * did not pass hash-checking due to\" (`splunkadmins_splunkd_source`) sourcetype=splunkd `splunkadmins_fileintegritycheck`\n| eval message=coalesce(message,event_message)\n| stats count, latest(_time) AS lastSeen by message, host\n| eval lastSeen=strftime(lastSeen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. File integrity check failure would generally mean a change has been made to parts of Splunk that will be wiped out next upgrade",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "33 9 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - KVStore Process Terminated",
    "search": "`comment(\"This ideally should never happen during normal runtime...\")`\nindex=_internal `searchheadhosts` \"KV Store process terminated\" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkadmins_kvstore_terminated`\n| fields _time, host, _raw",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Ideally you shouldn't see this error...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/15 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Losing Contact With Master Node",
    "search": "`comment(\"Either the master is down or the indexers are having issues contacting the master or search head to master is having issues\")` \nindex=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) (`splunkenterprisehosts` CMSearchHead OR GenerationGrabber OR (CMMasterProxy down)) OR (`indexerhosts` cluster master CMSlave WARN OR ERROR)\nNOT [`splunkadmins_shutdown_time(splunkadmins_clustermaster_oshost,30,60)`]\n| fillnull master value=\"N/A\"\n| rex \"(?s)^(\\S+\\s+){3}(?P<error>.*)\"\n| stats count, latest(_time) AS mostrecent, earliest(_time) AS firstseen, values(host) AS hosts by error, master\n| eval mostrecent=strftime(mostrecent, \"%+\"), firstseen=strftime(firstseen, \"%+\")\n| table hosts, count, master, firstseen, mostrecent, error\n| where (match(error,\"GenerationGrabber\") AND count>10) OR (match(error, \"(CMSearchHead|CMMasterProxy|CMSlave)\") AND count>1)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. One or more splunk indexers have lost contact with the splunk cluster master server. This may require additional investigation.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "11 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Low disk space",
    "search": "`comment(\"Use introspection data to monitor Splunk mount points, if you want to monitor non-Splunk directories use nmon or another monitoring system\")`\nindex=_introspection host=* component=Partitions `splunkadmins_lowdisk`\n| eval available='data.available', capacity='data.capacity', mount_point='data.mount_point'\n| eval percfree = round((available/capacity)*100,2)\n| stats min(percfree) AS percfree, min(available) AS minMBAvailable by mount_point, host\n| search `comment(\"Below 10% (default only, can be changed in the macro) is an issue unless it's an indexer, as 10% of the indexer is actually a very large amount of data...\")`\n(percfree<`splunkadmins_lowdisk_perc` NOT (`indexerhosts`)) OR (minMBAvailable<`splunkadmins_lowdisk_mb` (`indexerhosts`))",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-5m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Low disk space on one or more partitions of the Splunk enterprise servers...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/5 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Non-existent roles are assigned to users",
    "search": "`comment(\"Attempt to find where there are deleted roles assigned to users, this should only happen when the user was created with the Splunk authentication system. The fix is to open the user in the settings menu and find any user with the mentioned role, and then to save the user with no changes, this will wipe the non-existent roles from the user\")`\nindex=_internal sourcetype=splunkd `splunkenterprisehosts` `splunkadmins_splunkd_source` \"AuthorizationManager - Unknown role\"\n| eval message=coalesce(message,event_message)\n| stats max(_time) AS lastSeen, first(_raw) AS rawMessage by message\n| eval actionToTake=\"Find any users in the settings menu with the mentioned role and save them without changes to remove the role\"\n| eval lastSeen = strftime(lastSeen, \"%+\")\n| table lastSeen, message, rawMessage, actionToTake",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. This particular alert is harmless but can cause some very strange results if not resolved, the fix is documented within the alert",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 6 * * 1-5",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Replication Failures",
    "search": "`comment(\"Replication status failure on a search head, the search head may require a restart or investigation...\")`\nindex=_internal \"because replication was unsuccessful. replicationStatus Failed failure info:\" OR \"replicateDelta: failed for\" OR (ERROR DistributedBundleReplicationManager) OR (WARN DistributedBundleReplicationManager NOT \"however it took too long\") sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkenterprisehosts` `splunkadmins_repfailures`\n| search `comment(\"Exclude shutdown times\")` AND NOT [`splunkadmins_shutdown_keyword(indexerhosts,180,180)`]\n| eval event_message=coalesce(event_message,message) \n| stats count, max(_time) AS mostRecent by host, event_message \n| sort - mostRecent \n| eval mostRecent=strftime(mostRecent, \"%+\") \n| where (match(event_message, \"No auth token for peer\") AND count>1) OR NOT (match(event_message, \"No auth token for peer\")) \n| eval search_head=host \n| eval search_head_cluster=`search_head_cluster` \n| fields - search_head",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Replication failures often show a search head that is having issues after an indexer restart, the search head might require a restart to resolve this or investigation.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/15 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Splunk Scheduler excessive delays in executing search",
    "search": "`comment(\"These searches were scheduled to run at a particular time but the actual run time was more than X seconds later, which might indicate a search head performance issue\")` \n`comment(\"In the environment this search was created on restarts are not overly common so we assume any restart might relate to a scheduling delay to prevent false alarms from the alert. This may need further tuning or you may wish to remove the where clause in this if you want more alerting.\")`\nindex=_internal `splunkenterprisehosts` sourcetype=scheduler app=* scheduled_time=* source=*scheduler.log\n| search `comment(\"Exclude time periods where shutdowns were occurring\")` AND NOT [`splunkadmins_shutdown_list(splunkenterprisehosts,600,600)`]\n| eval time=strftime(_time,\"%+\") \n| eval delay_in_start = (dispatch_time - scheduled_time) \n| where delay_in_start>100\n| eval scheduled_time=strftime(scheduled_time,\"%+\") \n| eval dispatch_time=strftime(dispatch_time,\"%+\") \n| rename time AS endTime \n| table host,savedsearch_name,delay_in_start, scheduled_time, dispatch_time, endTime, run_time, status, user, app \n| sort -delay_in_start \n| dedup host,savedsearch_name,delay_in_start",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Long latency delays in scheduled searches may indicate an issue, however the scheduled time of the search is what determine the search window. Therefore this only shows when it has taken a long period of time to execute an actual search (there is another alert for skipped searches)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 6 * * 2",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Splunk Scheduler skipped searches and the reason",
    "search": "`comment(\"For some reason this search had to be skipped, this might be due to over scheduling the search or an inefficient search or similar\")` index=_internal sourcetype=scheduler status=skipped source=*scheduler.log `splunkenterprisehosts` \n| search `comment(\"Exclude unable to distribute to peer messages where we sent the shutdown signal to the peer\")` AND NOT [`splunkadmins_shutdown_list(splunkenterprisehosts,0,0)`]\n| search `comment(\"Skipped searches can be expected for up-to 10 minutes after an indexer(s) has been shutdown...\")` AND NOT [`splunkadmins_shutdown_time(indexerhosts,0,600)`]\n| fillnull concurrency_category concurrency_context concurrency_limit\n| stats count, earliest(_time) AS firstSeen, latest(_time) AS lastSeen by savedsearch_id, reason, app, concurrency_category, concurrency_context, concurrency_limit, search_type, user, host \n| eval firstSeen = strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. Provides the skipped searches and a list of reasons why they were skipped. To remove false alarms this alert now checks if any shutdown messages appear, this may require tweaking in your environment as it checks for *any* Splunk enterprise instance shutdown...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 2,6,10,14,18,22 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Splunk Servers throwing runScript errors",
    "search": "`comment(\"runScript errors are an indicator of a potential issue with an application\")`\nindex=_internal `splunkenterprisehosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) \n\"ERROR ScriptRunner - stderr from '*python* *runScript.py execute'\" OR \"ERROR ExecProcessor - message from \\\"python* *ERROR*\"\n`comment(\"Do not include INFO level messages from standard error/out\")`\nNOT \" INFO \" `splunkadmins_runscript` \n| cluster showcount=true \n| fields host, _raw, cluster_count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. Splunk Enterprise servers are throwing an error related to running a script, this may or may not be an issue...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 10 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Splunk Servers with resource starvation",
    "search": "`comment(\"Attempt to find entries in the splunkd logs that indiciate that Splunk is resource constrained and requires more CPU or similar\")`\nindex=_internal `indexerhosts`  sourcetype=splunkd source=*splunkd.log \"Might indicate hardware or splunk limitations\" OR \"took longer than\" `comment(\"This is useful for reporting but not so useful for alerting... OR \\\"WARN  PeriodicReapingTimeout\\\"\")` NOT \"Might indicate slow ldap server.\" `comment(\"Add in OR (WARN ConfMetrics) ?)\")` \n| rex \"^[\\d-]+ [\\d:\\.]+( )+[\\+-]?\\d+( )+[^ ]+( )+(?P<componentAndArea>([^ ]+( )+){3}).*\\((?P<number>\\d+) milliseconds\" \n| rex \"^[\\d-]+ [\\d:\\.]+( )+[\\+-]?\\d+( )+[^ ]+( )+(?P<componentAndArea2>DispatchManager\\s+([^ ]+( )+){3}).*elapsed_ms=(?P<number3>\\d+)\" \n| rex \"Spent (?P<number2>\\d+)\"\n| rex \"reaping (?P<area>([^ ]+ ){2})\"\n| eval componentAndArea=case(isnotnull(componentAndArea2),componentAndArea2,isnull(componentAndArea),component . \"_\" . area,1=1,componentAndArea), number=coalesce(number,number2,number3)\n| stats count, avg(number) AS avgTimeInSeconds, max(number) AS maxTimeInSeconds, max(_time) AS mostRecent, min(_time) AS firstSeen by componentAndArea, host\n| search `comment(\"Allow custom exclusions\")` `splunkadmins_resource_starvation`\n| sort - mostRecent\n| eval firstSeen=strftime(firstSeen, \"%+\"), mostRecent=strftime(mostRecent, \"%+\"), avgTimeInSeconds=round(avgTimeInSeconds/1000,2), maxTimeInSeconds=round(maxTimeInSeconds/1000,2)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-120m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Detect when a Splunk enterprise host is reporting that it is seeing excessive response times while running operations",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "13 */2 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Splunkd Crash Logs Have Appeared in Production",
    "search": "`comment(\"crash logs on the Splunk enterprise servers are usually an issue, this may require a support ticket\")`\nindex=_internal `splunkenterprisehosts` sourcetype=splunkd_crash_log\n| top source, host, sourcetype",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-7d@d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Production crashes are usually a problem",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "47 3 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Splunkd Log Messages Admins Only",
    "search": "index=_internal `splunkenterprisehosts` `splunkadmins_splunkd_log_messages` \n`comment(\"OR (TERM(DateParserVerbose) TERM(consecutive)) was previously included in the splunkd.log section but the message about unretrievable data does not appear to be accurate, Splunk auto-increments the timestamp by 1s every 200K events so this is not an issue as such...\")` \n    (sourcetype=splunkd (`splunkadmins_splunkd_source`) WARN OR ERROR MongoModificationsTracker OR TERM(SearchOperator:kv) OR AuditTrailManager OR IniFile OR GetBundleListTransaction OR GenericConfigKeyHandler OR AuthorizationManager OR GetRemoteAuthToken OR DistributedPeer OR (Archiver Permission) OR GetIndexListTransaction OR (DistributedPeerManager Timeout OR TERM(status=Down)) OR CalcFieldProcessor OR FieldAliaser OR (SearchScheduler OR DispatchManager \"minimum free disk space\") OR ApplicationUpdater OR (ScopedLDAPConnection NOT \"Might indicate slow ldap server\" NOT \"Converting non-UTF-8 value to\") OR regexExtractionProcessor OR (ProcessTracker NOT ConfMetrics) OR ConfReplication OR TailingProcessor OR \"Invalid cron_schedule\" OR \"Persistent file\" OR \"Too many indexes\" OR \"UserManagerPro - Strategy\" OR SearchProcessMemoryTracker OR SSLOptions OR (SHCRepJob misspelled) OR PivotEvaluator OR PropertiesMap OR HTTPAuthManager OR X509Verify OR FilesystemChangeWatcher OR PropsKeyHandler OR IndexProcessor OR BundleArchiver OR (ApplicationManager NOT \"Skipping update check for app id\" NOT \"This is expected if you push an app from the cluster master\") OR ISplunkDispatch OR TcpInputConfig OR (CollectionConfHandler Bad OR reload) OR SLConstants OR TERM(AdminHandler:AuthenticationHandler) OR (DispatchManager NOT (failedtostart OR quota OR QUEUED OR concurrency OR concurrently)) OR KVStoreBulletinBoardManager OR CMRestIndexerDiscoveryHandler\nOR KVStoreConfigurationProvider OR LMMasterRestHandler OR LMHttpUtil OR (DatabaseDirectoryManager Detecting) OR (No space NOT SHCRepJob NOT DispatchManager) OR (baseline configuration replicating) OR LMTracker OR IndexerDiscoveryHeartbeatThread OR ModularUtility OR ScriptRestHandler OR (S3Client httpStatusCode) OR BucketReplicator \nOR WorkloadConfig OR \"WARN  loader\" OR \"ERROR loader\" OR (TERM(AdminHandler:AuthenticationHandler) reasonable)\nOR (KVStoreLookup OR KVStoreProvider OR SingleLookupDriver OR outputcsv OR TERM(SearchOperator:inputcsv) NOT \"You have insufficient privileges\" NOT \"KV Store initialization\" NOT \"KV Store is shutting down\" NOT \"Found no results\" NOT \"lookup context\" NOT \"searchparsetmp\" NOT \"Invalid argument\" NOT \"must be followed by a search clause\") OR ConfigEncryptor OR AesGcm\nOR GenerationGrabber OR CMSearchHead OR DistHealthFetcher OR SpecFiles OR DeploymentServer OR DistributedPeerManagerHeartbeat OR MongodRunner OR (TERM(DS_DC_Common) NOT \"attributes cannot be handled by WebUI\" NOT \"Attribute unsupported by UI\") OR STMgr OR (heartbeat SHCSlave OR SHCMasterHTTPProxy OR failure) OR ServerInfoHandler OR BucketReplicator OR (TcpInputProc Stopping) OR StreamGroup \nOR (ScriptRunner Killing OR stderr) OR LMStackMgr OR (DatabaseDirectoryManager corrupt) OR (BucketMover exited) OR (\"KVStorageProvider\" NOT \"Result size too large\" NOT \"Too many rows in result\") OR DistributedPeerManager OR (HttpClientRequest NOT \"Broken pipe\") OR (UserManagerPro NOT \"Login failed\" NOT \"Failed to find ldapuser\" NOT \"Failed to get ldapuser\") OR (AutoLoadBalancedConnectionStrategy NOT \"Possible duplication\" NOT \"timed out\" NOT \"Applying quarantine\" NOT \"no raw data\") OR AppsDeployHandler OR SHCConfig OR (ClusterMasterControlHandler NOT \"No new dry run will be performed\") OR RaftSimpleFileStorage \nOR IConfCache OR (WorkloadManager NOT \"Failed to select user provided workload_pool\" NOT \"trans\") OR WorkloadClass OR AdminManagerExternal  OR SavedSearchAdminHandler OR JournalSlice OR PipelineComponent OR IndexConfig OR RawdataHashMarkReader OR ArchiveContext OR DateParser OR TimeoutHeap OR LMStackMgr OR AutoLookupDriver OR (TERM(spatial:PointInPolygonIndex) corruption) OR TERM(IntrospectionGenerator:resource_usage) OR PasswordHandler OR ConfigEncryptor OR AesGcm OR AutoLoadBalancedConnectionStrategy OR ModularInputs OR component=IndexerService \nOR ChunkedExternProcessor `comment(\"Note ChunkedExternProcessor introduces noise as well as legitimate errors\")` \n`comment(\"included in others alerts: CMMasterProxy, AutoLoadBalancedConnectionStrategy (data duplication/timeouts), ExecProcessor?\")` \n NOT (\"Configuration from app\" \"does not support reload\") `comment(\"This is a harmless error message, tsidx is optimized after this error appears\")` `comment(\"txn close did not succeed completely while flushing and closing a tsidx file rc=-8. Can be self-repaired in some cases but not all, so you may need to check on the bucket to see if it's an issue. It can relate to large >20MB+ events with slower IO for example\")` \n NOT \"Rounded off to 100% to handle the interval drift\" ) \n OR (sourcetype=scheduler source=*scheduler.log AlertNotifier WARN)\n OR (sourcetype=splunkd (`splunkadmins_splunkd_source`) INFO (IndexWriter paused `comment(\"May relate to maxConcurrentOptimizes in indexes.conf or perhaps maxRunningProcessGroups or spikes in data-per indexer\")`) OR (TERM(event=reclaimMemory) IndexProcessor OR StreamingBucketBuilder `comment(\"May relate to memPoolMB / maxMemMB setting in indexes.conf or the IndexWriter getting paused\")`)) \n| eval message=coalesce(message,event_message) \n| rex mode=sed field=message \"s/^\\([^\\)]+\\)\\s+(ProcessTracker\\s+-\\s+)?(\\([^\\)]+\\)\\s+)?IndexConfig/IndexConfig/g\"\n| rex mode=sed field=message \"s/^sid:[^ ]+//g\"\n| rex mode=sed field=message \"s/snapshot:\\s+[^;]+;\\s+Configurations changed while generating snapshot, original_latest_change=[^,]+, new_latest_change=[^,]+/snapshot: <bundledir> Configurations changed while generating snapshot original_latest_change=<removed>, new_latest_change=<removed>/\"\n| rex mode=sed field=message \"s/Error getting modtime:\\s+[^:]+/Error getting modtime: <dir>/g\"\n| rex field=message mode=sed \"s/uri=(https?:\\/\\/([^\\/]+\\/){4})\\S+/uri=\\1/\"\n| rex field=message mode=sed \"s/(<Resource>(\\/[^\\/]+){3}\\/)[^<]+/\\1/\"\n| rex field=message mode=sed \"s/<RequestId>[^<]+<\\/RequestId>/<RequestId>removed<\\/RequestId>/\"\n| rex field=message mode=sed \"s/transactionId=\\S+\\s+rTxnId=\\S+/transactionId=removed rTxnId=removed/\"\n| rex field=message mode=sed \"s/snapshot exists at op_id=\\S+/snapshot exists at op_id=removed/\"\n| rex field=message mode=sed \"s/(search_id=\\\"[^_]+_+[^_]+)[^\\\"]+/\\1/\"\n| rex field=message mode=sed \"s/bid=\\S+/bid=?/\" \n| rex field=message mode=sed \"s/JSON parse error at offset \\d+ of file \\\".*? Unexpected/JSON parse error at offset <x> of file: Unexpected/\" \n| eval search_head=host \n| eval search_head_cluster=`search_head_cluster` \n| stats count, latest(_time) AS mostrecent, earliest(_time) AS firstseen, values(component) AS component, values(log_level) AS log_level by message, search_head_cluster \n| eval comb_message = log_level . \" \" . component . \" \" . message \n| eval mostrecent=strftime(mostrecent, \"%+\"), firstseen=strftime(firstseen, \"%+\") \n| table comb_message, search_head_cluster, count, mostrecent, firstseen \n| cluster field=comb_message showcount=true t=0.9 \n| fields - cluster_label \n| sort - cluster_count, count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. This is an attempt to alert on almost any splunkd related log message which might be of interest to the admin team. Note that some items were excluded such as \"SearchOperator:savedsplunk\", while this exists in the splunkd log https://ideas.splunk.com/ideas/EID-I-796 advises why it is not useful as an error (vote if interested)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "43 4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - TCP or SSL Config Issue",
    "search": "`comment(\"A TcpInputConfig or SSLCommon error likely indicates a misconfiguration of a heavy forwarder or indexer, this may prevent the listening port from working as expected\")`\nindex=_internal ERROR \"TcpInputConfig\" OR \"SSLCommon\" OR \"Could not bind\" sourcetype=splunkd (`splunkadmins_splunkd_source`) (`indexerhosts`) OR (`heavyforwarderhosts`) \n| eval message=coalesce(message,event_message)\n| stats first(_time) AS mostRecent by host, source, sourcetype, message\n| table host, message, mostRecent\n| eval mostRecent=strftime(mostRecent, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Since the TCP listener port may not be working as expected you likely wish to run the appropriate checks on the forwarder/indexer",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "53 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Transparent Huge Pages is enabled and should not be",
    "search": "`comment(\"Detect when transparent huge pages is enabled on a Linux server, it should be disabled.\")`\n`comment(\"Redhat Linux has an issue where the transparent huge pages setting changes after Splunk starts if the server was rebooted, check /sys/kernel/mm/transparent_hugepage to confirm...\")`\n`comment(\"| rest /services/server/sysinfo is an alternative if you want the current search head + indexers, but this will ignore other search heads...\")`\nindex=_internal \"Linux transparent hugepage support, enabled=\" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkenterprisehosts` enabled!=\"never\"\n| eval error=\"This configuration of transparent hugepages is known to cause serious runtime problems with Splunk. Typical symptoms include generally reduced performance and catastrophic breakdown in system resp\nonsiveness under high memory pressure. Please fix by setting the values for transparent huge pages to \\\"madvise\\\" or preferably \\\"never\\\" via sysctl, kernel boot parameters, or other method recommended by your Linux distribution.\"\n| table _time, host, _raw, error",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Transparent huge pages should never be enabled on a Splunk enterprise server as per the http://docs.splunk.com/Documentation/Splunk/latest/Installation/Systemrequirements",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "14 2 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - Unable to dispatch searches due to disk space",
    "search": "`comment(\"Detect when the scheduler is unable to run search commands due to a lack of disk space on the filesystem\")`\nindex=_internal `splunkenterprisehosts` (sourcetype=splunkd `splunkadmins_splunkd_source` \"Search not executed: Dispatch Command: The minimum free disk space\") OR (sourcetype=scheduler \"The minimum free disk space * reached\")\n| eval message=coalesce(message,event_message)\n| stats count, min(_time) AS firstSeen, max(_time) AS mostRecent, max(_raw) AS lastExample by host, message\n| eval firstSeen=strftime(firstSeen, \"%+\"), mostRecent=strftime(mostRecent, \"%+\")\n| table host, count, firstSeen, mostRecent, lastExample",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Unless the disk space issue clears itself some action will be required either now or to prevent future failures.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "32 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - WARN iniFile Configuration Issues",
    "search": "`comment(\"Find potentially invalid configuration within the Splunk applications on search heads/indexers and warn about this...\")` \nindex=_internal WARN IniFile `splunkenterprisehosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkadmins_warninifile`\n| cluster showcount=true\n| fields _time, host, cluster_count, _raw",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. Detect configuration errors in the files that the indexer cluster or enterprise servers are throwing warnings about",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 4 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - error in stdout.log",
    "search": "index=_internal source=\"*/splunkd_stdout.log\" key `splunkenterprisehosts` \n| cluster t=0.9 \n| table _raw",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Key errors just advise of invalid syntax/configuration on startup so these can be a useful warning of items requiring attention",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 2,6,10,14,18,22 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - sendmodalert errors",
    "search": "`comment(\"sendmodalert or sendalert errors and warnings may be an issue relating to the creation of alerts via a script\")`\nindex=_internal `splunkenterprisehosts` \"ERROR sendmodalert - action=\" OR \"WARN sendmodalert - action\" OR \"Error in 'sendalert' command\" sourcetype=splunkd (`splunkadmins_splunkd_source`)\n`comment(\"If you need more context on the above errors add this snippet into the above search (remove the \\\\):\nOR \\\"sendmodalert - Invoking modular alert action\\\"\n\")`\n`splunkadmins_sendmodalert_errors`\n| rex field=results_file \"[/\\\\\\]dispatch[/\\\\\\](?P<sid>[^/]+)\"\n| eval sid=if(isnull(sid),\"NOMATCH\",sid)\n| join sid type=outer [search index=_internal source=\"*scheduler.log\" sourcetype=scheduler `splunkenterprisehosts` | table sid, savedsearch_name, app, user]\n| cluster showcount=true\n| table host, savedsearch_name, app, user, _raw, _time, cluster_count\n| eval mostRecent = strftime(mostRecent, \"%+\")\n| sort - _time",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. sendmodalert errors from Splunk might advise of a failure in an alert action, also see SearchHeadLevel - Script failures in the last day",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/15 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkEnterpriseLevel - ulimit on Splunk enterprise servers is below 8192",
    "search": "`comment(\"Any Splunk enterprise servers running less than 8192 file descriptors can result in a crash, therefore we watch the ulimit numbers on startup\")` \n`comment(\"You could do | rest /services/server/sysinfo | table ulimit* or similar but that will not cover all Splunk enterprise servers that are in the _internal index...\")`\nindex=_internal \"ulimit\" \"open files:\" `splunkenterprisehosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) \n| rex \"(?P<nooffiles>\\d+) files\" \n| where nooffiles<8192\n| fields _time, _raw, host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. ulimit should be 8192 or above as per the http://docs.splunk.com/Documentation/Splunk/latest/Installation/Systemrequirements",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "17 3 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel - Application Installation Failures From Deployment Manager",
    "search": "`comment(\"Deployment clients can pull applications down but they may not install the application so we watch for this error to see if an application failed to install\")` \nindex=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) action=Install OR action=download log_level=WARN OR log_level=ERROR\n| cluster t=0.9 showcount=true \n| where cluster_count>1 \n| lookup dnslookup clientip as ip \n| eval message=coalesce(message, event_message) \n| table cluster_count, clienthost, app, ip, component, message",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Applications have failed to install from the deployment server and this may require investigation",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 9 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel - Data Loss on shutdown",
    "search": "`comment(\"Waiting for TcpOutputGroups to shutdown followed by the message 'Forcing TcpOutputGroups to shutdown after timeout' is not something that can be controlled as of 7.2.1. It also generally correlates with data loss as the forwarder in question could not get the data through the queues and out of the forwarder/indexer before shutdown. Increasing parsingQueue/aggQueue or similar may help in this scenario\")`\nindex=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) \"Forcing TcpOutputGroups to shutdown after timeout\"\n| stats count, earliest(_time) AS firstSeen, latest(_time) AS lastSeen by host \n| eval firstSeen = strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. This alert usually indicates that at least some data was lost during the shutdown of the forwarder (universal or heavy), while nothing can be done about the lost data the queues may be tweaked to improve this scenario if the servers were up",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "14 7 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel - DeploymentServer Application Installation Error",
    "search": "`comment(\"Failed to install app can appear *if* the application is installed to a particular targetRepositoryLocation (for example /opt/splunk/etc/deployment-apps or similar) and stateOnClient=noop is not added to the application within serverclass.conf\")`\nindex=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) \"ERROR DeployedServerclass - name=* Failed to install\" OR \"DeployedApplication - Installing app=\"\n| eventstats count(eval(log_level=\"ERROR\")) AS errorCount, count(eval(log_level=\"INFO\")) AS successCount by host, app \n| where errorCount>0 AND successCount<1",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. The deployment server sent out a new application but for some reason it has failed to install",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "11 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel - No recent metrics.log data",
    "search": "`comment(\"This alert attempts to detect when a forwarder or Splunk server stops sending logs for an extended period of time outside a shutdown...\")`\n| tstats count where index=_internal `splunkenterprisehosts` source=*metrics.log by host, _time span=5m \n| search `splunkenterprisehosts`\n| search `comment(\"Exclude the shutdown times\")` NOT \n    [ `splunkadmins_shutdown_list(splunkenterprisehosts,30,30)`] \n| timechart limit=0 aligntime=latest span=5m sum(count) AS count by host \n| fillnull \n| untable _time, host, count \n| stats max(_time) AS mostRecent, min(_time) AS firstSeen, last(count) AS lastCount by host \n| where lastCount=0 \n| eval logMessages=\"Zero log entries found at this time, check that the Splunk server is still running/working as expected\" \n| fields - lastCount \n| eval mostRecent = strftime(mostRecent, \"%+\"), firstSeen=strftime(firstSeen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. If the metrics.log disappears for a period of time either the indexing tier is very busy or the forwarder in question has failed and stopped sending metrics.log files",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "53 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel - Splunk forwarders that are not talking to the deployment server",
    "search": "| tstats count where index=_internal groupby host \n| fields host \n| search `comment(\"This is an attempt to find any universal forwarders that send data into the indexers but do not phone home to the expected deployment server\")` `splunkadmins_forwarders_nottalking_ds`\n| eval shortname=mvindex(split(host, \".\"), 0) \n| eval talking=0 \n| table shortname, host, talking \n| append \n    [ search index=_internal `deploymentserverhosts` source=\"*splunkd_access.log\" sourcetype=splunkd_access \n    | rex field=uri \"/services/broker/phonehome/connection_[^_]+_[89][0-9]{3}_[^_]+(_[0-9][^_]+)?_(?P<hostname>[^_]+)_\" \n    | eval host=hostname \n    | eval shortname=mvindex(split(host, \".\"), 0) \n    | eval talking=1 \n    | dedup shortname, host, talking \n    | table shortname, host, talking]\n| append\n    [ search index=_internal `deploymentserverhosts` source=\"*splunkd_access.log\" sourcetype=splunkd_access\n    | rex field=uri \"/services/broker/phonehome/connection_(?P<ipaddr>[^_]+)_[89][0-9]{3}_[^_]+(_[0-9][^_]+)?_[^_]+_\"\n    | rename ipaddr AS host\n    | eval shortname=host\n    | eval talking=1\n    | dedup shortname, host, talking\n    | table shortname, host, talking]\n| reverse | dedup shortname, host \n| search NOT (`splunkenterprisehosts`) \n| search talking=0 \n| fields - talking \n| lookup dnslookup clienthost AS host \n| search clientip!=''",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. All forwarders should talk to the deployment server unless they have a special reason for an exclusion...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 8 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel - TCP Output Processor has paused the data flow",
    "search": "`comment(\"A paused TCP output processor is a potential indicator of an index performance issue, you may wish to ignore the shorter pause times such as 10 seconds if this is creating too many alerts...\")`\n`comment(\"On the indexer side you may see 'WARN TcpInputProc - Stopping all listening ports. Queues blocked for more than...' OR 'WARN TcpInputProc - Started listening on tcp ports. Queues unblocked', if there are indexer performance issues...\")`\nindex=_internal \"The TCP output processor has paused the data flow. Forwarding to output group\" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) `splunkadmins_tcpoutput_paused`\n| search `comment(\"Exclude shutdown times\")` AND NOT [`splunkadmins_shutdown_time(indexerhosts,60,60)`]\n| rex \"has been blocked for (?P<timeperiod>\\d+)\"\n| eval message=coalesce(message,event_message)\n| stats count, min(_time) AS firstSeen, max(_time) AS lastSeen, first(message) AS message, max(timeperiod) AS maxInSeconds, avg(timeperiod) AS avgTimePeriod by host\n| eval firstSeen=strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\"), avgTimePeriod=round(avgTimePeriod)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. A potential indicator of poor index performance or an overloaded forwarder",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "44 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel - TailReader Ignoring Path",
    "search": "`comment(\"Ignoring path is quite literal, the TailReader process will ignore the said log file and never index it. If you want the said file indexed then you will need to create a sourcetype for it...\")` \nindex=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) \"Ignoring path\" earliest=-24h `splunkadmins_tailreader_ignorepath` \n| regex path!=\"\\.\\d$\" \n| stats latest(_time) AS lastSeen, earliest(_time) AS firstSeen, last(_raw) AS lastmessage by host, path \n| eval firstSeen=strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. In this alert the TailReader is ignoring files, therefore it you need them to be indexed you will likely need to create a props.conf entry for the required sourcetype",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "7 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel - Time skew on Splunk Servers",
    "search": "`comment(\"A time skew issue likely shows an issue on the endpoint forwarder rather than a Splunk server but it is useful to watch for\")`\nindex=_internal \"A time skew of approximately\" (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) sourcetype=splunkd `splunkadmins_timeskew`\n| rex \"Peer:https?://(?P<hostname>[^:]+).*A time skew of approximately (?P<seconds>-?\\d+)\"\n| eval negativeSeconds=if(substr(seconds,0,1)=\"-\",\"true\", \"false\"), seconds=abs(seconds)\n| stats values(negativeSeconds) AS negativeSeconds, first(_raw) AS _raw, values(host) AS reportingHost, max(_time) AS lastSeen, min(_time) AS firstSeen, avg(seconds) AS avgSkew, max(seconds) AS maxSkew by hostname \n| eval avgSkew=if(negativeSeconds=\"true\",\"-\" . avgSkew,avgSkew), maxSkew=if(negativeSeconds=\"true\",\"-\" . maxSkew,maxSkew) \n| eval lastSeen = strftime(lastSeen, \"%+\"), firstSeen = strftime(firstSeen, \"%+\") \n| table hostname, reportingHost, _raw, lastSeen, firstSeen, avgSkew, maxSkew",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. A time skew should not exist, if we see this alert then something is not working in NTP...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "53 2,6,10,14,18,22 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel - Unable To Distribute to Peer",
    "search": "`comment(\"Unable to distribute to peer messages often indicate downtime or serious performance issues. The Unable to distribute to peer named status=Down scenario can also result from having many indexers and this may require an increase to the timeouts in distsearch.conf\")`\nindex=_internal \"Unable to distribute to peer named\" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkenterprisehosts` `splunkadmins_unable_distribute_to_peer`\n| rex \"(?P<message>Unable to distribute to peer named (?P<peer>[^: ]+))\"\n| bin _time span=1m\n| join type=outer peer\n    [ rest /services/search/distributed/peers \n    | fields peerName, title\n    | rex field=title \"(?P<title>[^:]+)\"\n    | rename title AS peer ]\n| eval targetHost=if(isnotnull(peerName),peerName,peer)\n| search `comment(\"Exclude unable to distribute to peer messages where we sent the shutdown signal to the peer\")` AND NOT [`splunkadmins_shutdown_list(splunkenterprisehosts,0,0)`]\n| stats count, values(message) AS message, values(host) AS reportingHostList by _time, targetHost \n| eval reportingHostList=mvindex(reportingHostList,0,9)\n| sort - _time\n| where count>1",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. A Splunk instance is advising that it cannot distribute to a peer node (indexer, another search head in the cluster or similar)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "9,24,39,54 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel - Unexpected termination of a Splunk process unix",
    "search": "`comment(\"A retrospective alert to advise that the Splunk process was terminated and this likely requires further investigation\")` \nindex=_internal index=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) \n`comment(\"Expecting 'FATAL ProcessRunner - Unexpected EOF from process runner child!' OR 'ProcessRunner - helper process seems to have died (child killed by signal 9: Killed)!' which can occur by the OOM killer for example\")` \n\"Unexpected EOF from process runner\" OR \"helper process seems to have died\" \n| eval event_message=coalesce(event_message,message) \n| stats count, earliest(_time) AS firstSeen, latest(_time) AS lastSeen, values(event_message) AS event_message by host\n| eval firstSeen = strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-20m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. A Splunk process on Unix was terminated (this alert is after restart/retrospective only)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/20 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel - Unexpected termination of a Splunk process windows",
    "search": "`comment(\"A retrospective alert to advise that the Splunk process was terminated and restarted multiple times and this likely requires further investigation\")` \nindex=`splunkadmins_wineventlog_index` sourcetype=WinEventLog:Application OR sourcetype=XmlWinEventLog:Application SourceName=\"Application Error\" splunk \n| stats count, earliest(_time) AS firstSeen, latest(_time) AS lastSeen by host, Faulting_application_path \n| where count > `splunkadmins_unexpected_term_count` \n| eval firstSeen = strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. A Splunk process on Windows was terminated multiple times (this is a retrospective alert) contributed by Chris Bell",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "9 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "AllSplunkLevel execprocessor errors",
    "search": "`comment(\"An attempt to find the execprocessor errors these are often scripts or application which are having some kind of issue...\")`\nindex=_internal \"ERROR ExecProcessor\" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) NOT \"Ignoring: \\\"\" `splunkadmins_execprocessor`\n| eval message=coalesce(message,event_message)\n| dedup message, host | fields host _raw",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. This alert can be very noisy, this will return any execprocessor errors from any script on any Splunk server!",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "28 5 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Bucket Copy Trigger",
    "search": "| archivebuckets",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "*",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "Triggers bucket copying",
    "eai:acl.app": "splunk_archiver",
    "dispatch.latest_time": "",
    "cron_schedule": "17 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ClusterMasterLevel - Per index status",
    "search": "| rest /services/cluster/master/indexes `splunkadmins_clustermaster_host`\n| foreach searchable_copies_tracker.*.actual_copies_per_slot \n    [ eval expectedMatchesActual_<<MATCHSTR>>=if('searchable_copies_tracker.<<MATCHSTR>>.expected_total_per_slot'=='searchable_copies_tracker.<<MATCHSTR>>.actual_copies_per_slot',\"true\",\"false\") ]\n| fields is_searchable, expectedMatchesActual_*, num_buckets, searchable*, title\n| eval failureCount=0\n| foreach expectedMatchesActual_*\n    [ eval failureCount=if('expectedMatchesActual_<<MATCHSTR>>'==\"false\",failureCount+1,failureCount) ]\n| where failureCount > `splunkadmins_clustermaster_failurecount` OR is_searchable=0",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Check if the is_searchable flag is set to false *or* detect when an index is not matching the search factor of at least 1 copy between the sites. Changed to 5 min intervals to pass certification you may want to run this more regularly. Cluster master specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/5 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ClusterMasterLevel - Primary bucket count per peer",
    "search": "| rest /services/cluster/master/buckets `splunkadmins_clustermaster_host` \n| search `comment(\"Note in larger environments this can cause an issue due to the number of peers returning data, so use with caution. The idea for this query comes from Splunk support & https://answers.splunk.com/answers/234717/how-to-get-list-of-buckets-which-are-having-issues.html , attempt to determine the count of primary buckets per peer for site0. This report is designed to provide 1 example of a useful REST endpoint\")` standalone=0 frozen=0\n| rename primaries_by_site.site0 AS peerGUID\n| join type=outer peerGUID [ rest /services/cluster/master/peers splunk_server=local\n| fields active_* host* label title status site\n| eval PeerName= site + \":\" + label + \":\" + host_port_pair\n| rename title AS peerGUID\n| rename site AS peerSite\n| table peerGUID PeerName peerSite]\n| stats count by PeerName\n| chart sum(count) AS count by PeerName",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Graph the number of primary buckets for site0 on each peer. Note in an environment with a large number of peers/buckets this query will be very expensive (memory wise) and should be used with caution",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ClusterMasterLevel - excess buckets on master",
    "search": "| rest /services/cluster/master/indexes splunk_server=local f=total_excess* | table title, total_excess_bucket_copies, total_excess_searchable_copies \n| addcoltotals",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Cluster master specific? Yes. As of 7.3.4/8.0.1 excess buckets do not clear themselves, Excess buckets have appeared it is recommended to clear them after some period of time \nSince the clear all excess buckets button can cause performance issues you may like to run: \nsplunk list excess-buckets | grep \"index\" | cut -d \"=\" -f2 > tmp.txt \nfor z in `cat tmp.txt `; do echo $z; /opt/splunk/bin/splunk remove excess-buckets ${z}; sleep 10; done ; \nAlso adjust the sleep time as appropriate for the number of buckets/cluster size... \nOr more a human readable version splunk list excess-buckets | egrep \"index|Total number of excess replication\"| cut -d= -f2 | tac | paste -d\" \" - -",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/20 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "CrowdStrike FDR:SQS Logs - 30 Days",
    "search": "index=_internal  sourcetype=\"tacrowdstrikefalcondatareplicator:log\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-30d@d",
    "eai:acl.sharing": "global",
    "description": "Collects the CrowdStrike FDR:SQS Technical Add-on logs for the past 30 days",
    "eai:acl.app": "TA-crowdstrike-falcon-data-replicator-sqs",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "CrowdStrike Falcon Devices Indexed vs Event Time",
    "search": "`cs_fd_get_index` | eval \"Indexed Time\"=strftime(_indextime,\"%Y-%m-%d %H:%M:%S\")  | eventstats count by \"Indexed Time\"  | rename  count as \"Number of CrowdStrike Devices Events\"  | eval \"Devices Event Time\" = strftime(_time, \"%Y-%m-%d %H:%M:%S\")|table \"Indexed Time\", \"Device Event Time\", \"Number of CrowdStrike Devices Events\" |sort  -\"Indexed Time\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "user",
    "dispatch.earliest_time": "-7d@h",
    "eai:acl.sharing": "global",
    "description": "Shows when the CrowdStrike Falcon Device data was indexed, the event timestamp and the count of events. Timespan needs to be large enough to include the Event Time stamp.",
    "eai:acl.app": "TA-crowdstrike-falcon-devices",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "CrowdStrike Falcon Devices Logs - 30 Days",
    "search": "index=_internal sourcetype=\"tacrowdstrikefalcondevices:log\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "user",
    "dispatch.earliest_time": "-30d@d",
    "eai:acl.sharing": "global",
    "description": "Collects the CrowdStrike Falcon Devices Technical Add-on logs for the past 30 days",
    "eai:acl.app": "TA-crowdstrike-falcon-devices",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "CrowdStrike Falcon FDR: SQS Data Indexed vs Event Time",
    "search": "index=* sourcetype=\"crowdstrike:fdr*:sqs:json\" | eval \"Indexed Time\"=strftime(_indextime,\"%Y-%m-%d %H:%M:%S\")  | eventstats count by \"Indexed Time\" | rename count as \"Number of CrowdStrike FDR Events\"  | eval \"FDR Event Time\" = strftime(_time, \"%Y-%m-%d %H:%M:%S\")| table  \"Indexed Time\", \"FDR Event Time\", sourcetype, \"Number of CrowdStrike FDR Events\"  | sort  -\"Indexed Time\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "user",
    "dispatch.earliest_time": "-7d@h",
    "eai:acl.sharing": "global",
    "description": "Shows when the CrowdStrike FDR:SQS data was indexed, the event timestamp, the sourcetype and the count of events. Timespan needs to be large enough to include the Event Time stamp.",
    "eai:acl.app": "TA-crowdstrike-falcon-data-replicator-sqs",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "CrowdStrike Falcon Spotlight Data Indexed vs Event Time",
    "search": "`cs_spotlight_get_index` | eval \"Indexed Time\"=strftime(_indextime,\"%Y-%m-%d %H:%M:%S\")  | eventstats count by \"Indexed Time\"  | rename  count as \"Number of CrowdStrike Spotlight Data Events\"  | eval \"Spotlight Event Time\" = strftime(_time, \"%Y-%d-%m %H:%M:%S\")|table \"Indexed Time\", \"Spotlight Event Time\", \"Number of CrowdStrike Spotlight Data Events\" |sort  -\"Indexed Time\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-7d@h",
    "eai:acl.sharing": "global",
    "description": "Shows when the CrowdStrike Falcon Spotlight was indexed, the event timestamp and the count of events. Timespan needs to be large enough to include the Event Time stamp.",
    "eai:acl.app": "TA-crowdstrike-falcon-spotlight-data",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "CrowdStrike Spotlight Logs - 30 Days",
    "search": "index=_internal  sourcetype=\"tacrowdstrikefalconspotlightdata:log\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "user",
    "dispatch.earliest_time": "-30d@d",
    "eai:acl.sharing": "global",
    "description": "Collects the CrowdStrike Spotlight Vulnerability Data Technical Add-on logs for the past 30 days",
    "eai:acl.app": "TA-crowdstrike-falcon-spotlight-data",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "CrowdStrike Token Refresh Check",
    "search": "`cs_es_tc_input(1)` | stats count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "global",
    "description": "Monitors the CrowdStrike Event Streams log file to detect if an input has stopped running.",
    "eai:acl.app": "TA-crowdstrike-falcon-event-streams",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/60 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Alert - Abnormal State of Indexer Processor",
    "search": "| rest splunk_server_group=dmc_group_indexer /services/server/introspection/indexer \n| fields splunk_server, average_KBps, status, reason \n| where status != \"normal\" \n| eval average_KBps = round(average_KBps, 0) \n| eval status= if(status==\"normal\", status, status.\" - \".reason) \n| fields - reason \n| rename splunk_server as Instance, average_KBps as \"Average KB/s (last 30s)\", status as Status",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "One or more of your indexers is reporting an abnormal state.",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "",
    "cron_schedule": "3,8,13,18,23,28,33,38,43,48,53,58 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Alert - Critical System Physical Memory Usage",
    "search": "| rest splunk_server_group=dmc_group_* /services/server/status/resource-usage/hostwide \n| eval percentage=round(mem_used/mem,3)*100 \n| where percentage > 90 \n| fields splunk_server, percentage, mem_used, mem \n| rename splunk_server AS Instance, mem AS \"Physical memory installed (MB)\", percentage AS \"Memory used (%)\", mem_used AS \"Memory used (MB)\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "One or more instances has exceeded 90% memory usage.",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "",
    "cron_schedule": "3,8,13,18,23,28,33,38,43,48,53,58 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Alert - Expired and Soon To Expire Licenses",
    "search": "| rest splunk_server_group=dmc_group_license_master /services/licenser/licenses \n| join type=outer group_id splunk_server [ \n    rest splunk_server_group=dmc_group_license_master /services/licenser/groups \n    | where is_active = 1 \n    | rename title AS group_id \n    | fields is_active group_id splunk_server] \n| where is_active = 1 \n| eval days_left = floor((expiration_time - now()) / 86400) \n| where NOT (quota = 1048576 OR label == \"Splunk Enterprise Reset Warnings\" OR label == \"Splunk Lite Reset Warnings\") \n| eventstats max(eval(if(days_left >= 14, 1, 0))) as has_valid_license by splunk_server \n| where has_valid_license == 0 AND (status == \"EXPIRED\" OR days_left < 15) \n| eval expiration_status = case(days_left >= 14, days_left.\" days left\", days_left < 14 AND days_left >= 0, \"Expires soon: \".days_left.\" days left\", days_left < 0, \"Expired\") \n| eval total_gb=round(quota/1024/1024/1024,3) \n| fields splunk_server label license_hash type group_id total_gb expiration_time expiration_status \n| convert ctime(expiration_time) \n| rename splunk_server AS Instance label AS \"Label\" license_hash AS \"License Hash\" type AS Type group_id AS Group total_gb AS Size expiration_time AS \"Expires On\" expiration_status AS Status",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "You have instances with licenses that expired or will expire within 2 weeks. No other valid licenses are installed.",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "",
    "cron_schedule": "3 0 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Alert - Missing forwarders",
    "search": "| inputlookup dmc_forwarder_assets\n| search status=\"missing\" \n| rename hostname as Instance",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "One or more forwarders are missing.",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "",
    "cron_schedule": "*/15 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Alert - Near Critical Disk Usage",
    "search": "| rest splunk_server_group=dmc_group_* /services/server/status/partitions-space \n| eval free = if(isnotnull(available), available, free) \n| eval usage = capacity - free \n| eval pct_usage = floor(usage / capacity * 100) \n| where pct_usage > 80 \n| stats first(fs_type) as fs_type first(capacity) AS capacity first(usage) AS usage first(pct_usage) AS pct_usage by splunk_server, mount_point \n| eval usage = round(usage / 1024, 2) \n| eval capacity = round(capacity / 1024, 2) \n| rename splunk_server AS Instance mount_point as \"Mount Point\", fs_type as \"File System Type\", usage as \"Usage (GB)\", capacity as \"Capacity (GB)\", pct_usage as \"Usage (%)\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "You have used 80% of your disk capacity.",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "",
    "cron_schedule": "3,33 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Alert - Saturated Event-Processing Queues",
    "search": "| rest splunk_server_group=dmc_group_indexer /services/server/introspection/queues \n| search title=tcpin_queue OR title=parsingQueue OR title=aggQueue OR title=typingQueue OR title=indexQueue \n| eval fifteen_min_fill_perc = round(value_cntr3_size_bytes_lookback / max_size_bytes * 100,2) \n| fields title fifteen_min_fill_perc splunk_server \n| where fifteen_min_fill_perc > 90 \n| rename splunk_server as Instance, title AS \"Queue name\", fifteen_min_fill_perc AS \"Average queue fill percentage (last 15min)\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "One or more of your indexer queues is reporting a fill percentage, averaged over the last 15 minutes, of 90% or more.",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "",
    "cron_schedule": "3,13,23,33,43,53 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Alert - Search Peer Not Responding",
    "search": "| rest splunk_server=local /services/search/distributed/peers/ \n| where status!=\"Up\" AND disabled=0 \n| fields peerName, status \n| rename peerName as Instance, status as Status",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "One or more of your search peers is currently down.",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "",
    "cron_schedule": "3,8,13,18,23,28,33,38,43,48,53,58 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Alert - Total License Usage Near Daily Quota",
    "search": "| rest splunk_server_group=dmc_group_license_master /services/licenser/pools \n| join type=outer stack_id splunk_server [rest splunk_server_group=dmc_group_license_master /services/licenser/groups | search is_active=1 | eval stack_id=stack_ids | fields splunk_server stack_id is_active] \n| search is_active=1 \n| fields splunk_server, stack_id, used_bytes \n| join type=outer stack_id splunk_server [rest splunk_server_group=dmc_group_license_master /services/licenser/stacks | eval stack_id=title | eval stack_quota=quota | fields splunk_server stack_id stack_quota] \n| stats sum(used_bytes) as used_bytes max(stack_quota) as stack_quota by splunk_server \n| eval usedGB=round(used_bytes/1024/1024/1024,3) \n| eval totalGB=round(stack_quota/1024/1024/1024,3) \n| eval percentage=round(usedGB / totalGB, 3)*100 \n| fields splunk_server, percentage, usedGB, totalGB \n| where percentage > 90 \n| rename splunk_server AS Instance, percentage AS \"License quota used (%)\", usedGB AS \"License quota used (GB)\", totalGB as \"Total license quota (GB)\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "You have used 90% of your total daily license quota.",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "",
    "cron_schedule": "3,33 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Asset - Build Full",
    "search": "| rest splunk_server=local /services/search/distributed/peers \n| search status=Up disabled=0 \n| eval os = os_name \n| fields guid title peerName host host_fqdn server_roles search_groups cpu_arch os numberOfCores physicalMemoryMB version \n| rename title AS peerURI peerName AS serverName host_fqdn AS machine numberOfCores AS cpu_count physicalMemoryMB AS mem version AS splunk_version server_roles AS inherited_server_roles \n| where isnotnull(mvfind(search_groups,\"dmc_group_\")) \n| join type=outer peerURI [ \n    | rest splunk_server=local /servicesNS/nobody/splunk_monitoring_console/configs/conf-splunk_monitoring_console_assets \n    | fields title host host_fqdn \n    | rename title AS peerURI host_fqdn AS machine] \n| mvexpand search_groups \n| append [ | `dmc_get_local_instance_asset_in_distributed_mode` ] \n| fields peerURI serverName host machine search_groups \n| rename search_groups AS search_group",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "user",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Asset - Build Standalone Asset Table",
    "search": "| `dmc_get_local_instance_asset` \n| rename search_groups AS search_group \n| inputlookup append=true dmc_assets \n| stats last(*) AS * by peerURI, search_group \n| fields peerURI serverName host machine search_group",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "user",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "This search establishes an updated cache of metadata necessary for the localhost to be included in DMC dashboards. This search will be disabled when user go throught the setup process. and will be re-enabled when user resets to factory mode.",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "",
    "cron_schedule": "*/1 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Asset - Build Standalone Computed Groups Only",
    "search": "| `dmc_get_local_instance_asset` \n| rename search_groups AS search_group \n| fields peerURI serverName host machine search_group",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "user",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC Forwarder - Build Asset Table",
    "search": "`dmc_build_forwarder_assets(1m)` \n| inputlookup append=true dmc_forwarder_assets \n| stats values(forwarder_type) as forwarder_type, max(version) as version, values(arch) as arch, values(os) as os, max(last_connected) as last_connected, values(new_sum_kb) as sum_kb, values(new_avg_tcp_kbps_sparkline) as avg_tcp_kbps_sparkline, values(new_avg_tcp_kbps) as avg_tcp_kbps, values(new_avg_tcp_eps) as avg_tcp_eps by guid, hostname \n| addinfo \n| eval status = if(isnull(sum_kb) or (sum_kb <= 0) or (last_connected < (info_max_time - 900)), \"missing\", \"active\") \n| eval sum_kb = round(sum_kb, 2) \n| eval avg_tcp_kbps = round(avg_tcp_kbps, 2) \n| eval avg_tcp_eps = round(avg_tcp_eps, 2) \n| fields guid, hostname, forwarder_type, version, arch, os, status, last_connected, sum_kb, avg_tcp_kbps_sparkline, avg_tcp_kbps, avg_tcp_eps \n| outputlookup dmc_forwarder_assets",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "user",
    "dispatch.earliest_time": "-16m@m",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "-1m@m",
    "cron_schedule": "3,18,33,48 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DMC License Usage Data Cube",
    "search": "index=_internal source=*license_usage.log* type=\"Usage\" | eval h=if(len(h)=0 OR isnull(h),\"(SQUASHED)\",h) | eval s=if(len(s)=0 OR isnull(s),\"(SQUASHED)\",s) | eval idx=if(len(idx)=0 OR isnull(idx),\"(UNKNOWN)\",idx) | bin _time span=1d | stats sum(b) as b by _time, host, pool, s, st, h, idx",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-31d",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_monitoring_console",
    "dispatch.latest_time": "-0d",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DeploymentServer - Application Not Found On Deployment Server",
    "search": "`comment(\"This usually indicates a misconfigured serverclass.conf or a missing application from the deployment-apps directory\")` \nindex=_internal `deploymentserverhosts` \"ERROR Serverclass - Failed to load app.\" sourcetype=splunkd (`splunkadmins_splunkd_source`) \n| bin _time span=20m \n| top Application, path, _time",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. The application was not found on the deployment server or another deployment server error occurred",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "14 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DeploymentServer - Error Found On Deployment Server",
    "search": "`comment(\"This usually indicates a misconfigured serverclass.conf or a missing application from the deployment-apps directory\")` \nindex=_internal `deploymentserverhosts` \"ERROR Serverclass\" OR \"ERROR DSManager\" OR (\"WARN  DeploymentServer\") OR CASE(\" FATAL \") OR (TERM(DS_DC_Common) NOT \"attributes cannot be handled by WebUI\") sourcetype=splunkd (`splunkadmins_splunkd_source`) \n| cluster showcount=true \n| table _time, _raw, cluster_count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. An application was not found or another deployment server error has occurred, this is more generic than the specific DeploymentServer - alerts",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "14 */4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DeploymentServer - Forwarder has changed properties on phone home",
    "search": "`comment(\"This looks for unusual changes on the phone home to the deployment server, this alert can also be completely harmless\")` \nindex=_internal `deploymentserverhosts` \"has changed some of its properties on the latest phone home.Old properties are\" (`splunkadmins_splunkd_source`) sourcetype=splunkd `splunkadmins_changedprops`\n| rex \"Client with Id '(?P<clientid>[^']+)\" \n| sort clientid \n| eventstats count by clientid \n| where count>`splunkadmins_changedprops_count` \n| stats values(ip) AS \"IP List\", values(dns) AS \"DNS names\", values(hostname) AS \"Hostname List\", values(uts) AS uts by name \n| eval numberOfIPs=mvcount(\"IP List\"), numberOfHostnames=mvcount(\"Hostname List\") \n| search `comment(\"Having multiple DNS names for an IP address is almost normal here, however multiple IPs or hostnames might be a real issue. Ignoring multiple DNS names only\")` numberOfIPs>1 OR numberOfHostnames>1",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-7d@d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Only detect when a forwarder has switched IP's or something strange has happened, ignore multiple DNS names for the same IP",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "37 6 * * 3",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DeploymentServer - Unsupported attribute within DS config",
    "search": "`comment(\"A DS_DC_Common warning normally relates to a typo within the serverclass.conf, most likely due to manual editing, this will prevent the GUI from been used for the forwarder management configuration...\")`\nindex=_internal \"WARN\" \"DS_DC_Common\" `deploymentserverhosts` sourcetype=splunkd (`splunkadmins_splunkd_source`)\n| eval message=coalesce(message,event_message)\n| stats first(_time) AS lastSeen by message, host\n| eval lastSeen=strftime(lastSeen, \"%+\")\n| table lastSeen, message, host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. A syntax error from manually editing the serverclass.conf will normally need to be fixed. Note that this alert is only useful if you are manually editing your serverclass.conf, if you only use the GUI then it is unlikely that this alert will ever be triggered.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "48 */4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "DeploymentServer - btool validation failures occurring on deployment server",
    "search": "`comment(\"This alert detects when the deployment server is throwing some kind of warning about an application it is going to deploy. The exclusion list includes lines that are not really relevant as they appear later in the log entries\")` \nindex=_internal `deploymentserverhosts` \"WARN  Application\" sourcetype=splunkd (`splunkadmins_splunkd_source`)\n NOT \"There were the following errors in btool check:\" \n`splunkadmins_btoolvalidation_ds` \n| eval message=coalesce(message,event_message)\n| dedup message | fields _time _raw host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d@d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Email about any btool validation errors on the deployment server",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "28 11 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Errors in the last 24 hours",
    "search": "error OR failed OR severe OR ( sourcetype=access_* ( 404 OR 500 OR 503 ) )",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "search",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Errors in the last hour",
    "search": "error OR failed OR severe OR ( sourcetype=access_* ( 404 OR 500 OR 503 ) )",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "search",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Bandwidth Throttling Occurring",
    "search": "`comment(\"This alert detects universal or heavy forwarders that have hit the maxKbps setting in the limits.conf and might need to be investigated\")` \nindex=_internal \"has reached maxKBps. As a result, data forwarding may be throttled\" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) `splunkadmins_bandwidth`\n| bin _time span=1h\n| stats count as countPerHost by host, _time\n| where countPerHost > 1",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Cases where the Splunk forwarder is delayed from sending the data to Splunk due to the maxKbps limit",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "37 02 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Channel churn issues",
    "search": "index=_internal source=*metrics.log TERM(name=pipelineinputchannel) new_channels sourcetype=splunkd `comment(\"As per https://answers.splunk.com/answers/825663/why-did-ingestion-slow-way-down-after-i-added-thou.html , having too many channels created/removed can cause issues\")`\n| bin _time span=1m\n| stats avg(new_channels) AS avg_new_channels avg(removed_channels) AS avg_removed_channels by host, _time\n| where avg_new_channels>5000 AND avg_removed_channels>1000 \n| stats count, max(avg_new_channels) AS max_avg_new_channels, max(avg_removed_channels) AS max_avg_removed_channels, max(_time) AS _time by host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. This relates to channel churn issues, and is likely no longer required after 8.0.6 and above, https://answers.splunk.com/answers/825663/why-did-ingestion-slow-way-down-after-i-added-thou.html has more details on this...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "47 4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Data dropping duration",
    "search": "index=_internal sourcetype=splunkd `splunkadmins_splunkd_source` \"Queue for group\" \"has begun dropping\" OR \"has stopped dropping events\" `heavyforwarderhosts` OR `indexerhosts` \n| rex \"group\\s+(?P<group>\\S+).*?(?P<state>(stopped|begun))\" \n| sort 0 _time\n| streamstats current=f global=f window=1 values(state) AS prev_state, min(_time) AS start by host, group\n| search state=\"stopped\" AND prev_state=\"begun\"\n| eval duration=_time-start\n| eval shorthost=replace(host, \"^([^\\.]+).*\", \"\\1\")\n| eval combined = shorthost. \"_\" . group\n| timechart limit=50 max(duration) AS duration by combined",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report will measure if an output queue has dropped data and for the duration of the data drop. This is normally relevant when you are cloning data to more than 1 output location in outputs.conf. Note that as of 8.0.3 the pipeline dropping the data is not recorded but the drop is per-pipeline, per-output queue, frequent \"dropping\" results in pausing of the TCP output queue which can cause issues for all upstream queues...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "38 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - File Too Small to checkCRC occurring multiple times",
    "search": "`comment(\"An experimental alert to detect the seekcrc too small errors in the splunkd.log file occurring a bit too regularly\")` \nindex=_internal \"File too small to check seekcrc, probably truncated\" \nsourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) `splunkadmins_toosmall_checkcrc`\n`comment(\"Older universal forwarders have a variety of logs that will never be more than zero sized, therefore this error is legitimate for them\")`\nNOT (file=\"'/*/splunkforwarder/var/log/splunk/license_usage.log'\" OR file=\"'/*/splunkforwarder/var/log/splunk/license_usage_summary.log'\" OR file=\"'/*/splunkforwarder/var/log/splunk/mongod.log'\" OR file=\"'/*/splunkforwarder/var/log/splunk/remote_searches.log'\" OR file=\"'/*/splunkforwarder/var/log/splunk/scheduler.log'\" OR file=\"'/*/splunkforwarder/var/log/splunk/searchhistory.log'\" OR file=\"'/*/splunkforwarder/var/log/splunk/splunkd_ui_access.log'\" OR file=\"'/*/splunkforwarder/var/log/splunk/crash-*'\" OR file=\"'/*/splunkforwarder/var/log/splunk/btool.log'\" OR file=\"'/*/splunkforwarder/var/log/splunk/license_audit.log'\")\n`comment(\"Older windows based universal forwarders can also have these same zero sized log files, therefore this error is legitimate for them\")`\nNOT (file=\"'\\\\*\\\\SplunkUniversalForwarder\\\\var\\\\log\\\\splunk\\\\license_usage.log'\" OR file=\"'\\\\*\\\\SplunkUniversalForwarder\\\\var\\\\log\\\\splunk\\\\license_usage_summary.log'\" OR file=\"'\\\\*\\\\SplunkUniversalForwarder\\\\var\\\\log\\\\splunk\\\\mongod.log'\" OR file=\"'\\\\*\\\\SplunkUniversalForwarder\\\\var\\\\log\\\\splunk\\\\remote_searches.log'\" OR file=\"'\\\\*\\\\SplunkUniversalForwarder\\\\var\\\\log\\\\splunk\\\\scheduler.log'\" OR file=\"'\\\\*\\\\SplunkUniversalForwarder\\\\var\\\\log\\\\splunk\\\\searchhistory.log'\" OR file=\"'\\\\*\\\\SplunkUniversalForwarder\\\\var\\\\log\\\\splunk\\\\splunkd_ui_access.log'\" OR file=\"'\\\\*\\\\SplunkUniversalForwarder\\\\var\\\\log\\\\splunk\\\\crash-*'\" OR file=\"'\\\\*\\\\SplunkUniversalForwarder\\\\var\\\\log\\\\splunk\\\\btool.log'\" OR file=\"'\\\\*\\\\SplunkUniversalForwarder\\\\var\\\\log\\\\splunk\\\\license_audit.log'\")\n`comment(\"Splunk enterprise instances running on non-official hostnames\")`\nNOT (file=\"'/opt/splunk/var/log/splunk/license_usage.log'\" OR file=\"'/opt/splunk/var/log/splunk/license_usage_summary.log'\" OR file=\"'/opt/splunk/var/log/splunk/mongod.log'\" OR file=\"'/opt/splunk/var/log/splunk/remote_searches.log'\" OR file=\"'/opt/splunk/var/log/splunk/scheduler.log'\" OR file=\"'/opt/splunk/var/log/splunk/searchhistory.log'\" OR file=\"'/opt/splunk/var/log/splunk/splunkd_ui_access.log'\" OR file=\"'/opt/splunk/var/log/splunk/crash-*'\" OR file=\"'/opt/splunk/var/log/splunk/btool.log'\" OR file=\"'/opt/splunk/var/log/splunk/license_audit.log'\")\n`comment(\"Regex for filename now replaces the default field extraction due to Windows based filenames containing spaces..\")`\n| rex \"file=(?P<file>.+)\\)\\.\"\n| stats sum(linecount) as numberOfEntries by host, file\n| where numberOfEntries > 10",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. CRC checksum errors multiple times in may indicate a problem with the crc checksum on the particular file, it's also possible we are seeing a zero sized file or a rolled file...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0,15,30,45 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Forwarders connecting to a single endpoint for extended periods",
    "search": "`comment(\"Detect forwarders stuck connecting to a single indexer or heavy forwarder for an extended period of time...Assuming more than 60 seconds of continuous traffic is a problem...this may need to be customised for your environment\")`\n`comment(\"Note that the number of metrics defaults to the top 10 measured every 30 seconds, so if this is customised you will need to change this alert\")`\nindex=_internal sourcetype=splunkd source=*metrics.log sourcetype=splunkd group=tcpin_connections Metrics `indexerhosts` OR `heavyforwarderhosts`\n| eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, \"none\")\n| streamstats time_window=60s count by hostname, host, ingest_pipe\n| where count>4\n| eval combined=hostname . \" host:\" . host . \" pipe:\" . ingest_pipe\n| timechart span=10m useother=false max(count) by combined",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Experimental alert (report for now). This detects when a forwarder spent an extended period of time connecting to a single endpoint, this suggests that the EVENT_BREAKER (on a HF or a LINE_BREAKER on a HF) may assist in forcing the connection to switch between endpoints more regularly.\nNote this there are multiple variables that change this query depending on your environment setup.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Forwarders connecting to a single endpoint for extended periods UF level",
    "search": "`comment(\"Detect forwarders stuck connecting to a single indexer or heavy forwarder for an extended period of time...Assuming more than 60 seconds of continuous traffic is a problem...this may need to be customised for your environment\")`\n`comment(\"Note that the number of metrics defaults to the top 10 measured every 30 seconds, so if this is customised you will need to change this alert\")`\nindex=_internal sourcetype=splunkd source=*metrics.log sourcetype=splunkd group=tcpin_connections Metrics `indexerhosts` OR `heavyforwarderhosts`\n| eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, \"none\")\n| streamstats time_window=60s count by name, host, ingest_pipe\n| where count>4\n| eval combined=name . \" host:\" . host . \" pipe:\" . ingest_pipe\n| timechart span=10m useother=false max(count) by combined",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Experimental alert (report for now). This detects when a forwarder spent an extended period of time connecting to a single endpoint, this suggests that the EVENT_BREAKER may assist in forcing the connection to switch between endpoints more regularly.\nNote this there are multiple variables that change this query depending on your environment setup.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Forwarders in restart loop",
    "search": "`comment(\"If a forwarder restarts more than 5 times in 15 minutes there might be a problematic script that is restarting it too often\")` \nindex=_internal \"Received shutdown signal.\" sourcetype=splunkd (`splunkadmins_splunkuf_source`)\n| stats count as restartCount by host \n| where restartCount > 5",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Attempt to detect universal forwarders that are restarting too often",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0,15,30,45 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Read operation timed out expecting ACK",
    "search": "`comment(\"This read operation timed out expecting ACK will likely result in the forwarder re-sending at least some data to the indexer. This can be caused by lack of CPU on the forwarder and potentially other issues...\")`\nindex=_internal \"Read operation timed out expecting ACK from\" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`)\n| rex \"from (?P<indexer>\\S+)\"\n| stats count, max(_time) AS mostRecent by host, indexer\n| eval mostRecent=strftime(mostRecent, \"%+\")\n| search `comment(\"Allow exclusions such as ignoring a count per host or similar...\")` `splunkadmins_readop_expectingack`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-2h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. Acknowledgement from the indexers should ideally never timeout, the time out may cause duplication issues",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "47 */2 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - SSL Errors In Logs (Potential Universal Forwarder and License Issue)",
    "search": "`comment(\"Excessive SSL errors may relate to a bug in the universal forwarder, if the SSL errors relate to duplication this could cause a license usage issue\")` \nindex=_internal sourcetype=splunkd (`splunkadmins_splunkuf_source`) NOT (`splunkenterprisehosts`)\n\"sock_error = 10054. SSL Error = error:00000000:lib(0):func(0):reason(0)\"\n| top limit=500 host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. SSL errors from Windows forwarder sin the past have resulted in duplication and excessive license usage, this alert exists to detect this scenario.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "53 22 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Splunk Forwarder Down",
    "search": "| metadata type=hosts index=_internal \n| search `comment(\"Find forwarders that have recently stopped talking to the indexers / appear to be down\")`\nNOT (`splunkenterprisehosts`) `splunkadmins_forwarderdown`\n| eval age=now()-recentTime | eval status=if(age<1200,\"UP\",\"DOWN\") \n| eval \"Last Active On\"=strftime(recentTime, \"%+\") \n| rename age as Age \n| eval Hour=round(Age/3600,0)\n| eval Minute=round((Age%3600)/60,0)\n| eval Age=\"-\".Hour.\"h\".\" : \".Minute.\"m\" \n| table host, status, \"Last Active On\", Age \n| search status=DOWN \n| lookup dnslookup clienthost AS host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. Splunk Forwarders Down (excluding timeshift servers and AWS cloud forwarders)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Splunk HEC issues",
    "search": "`comment(\"Find if the HEC is throwing errors. Refer to https://docs.splunk.com/Documentation/Splunk/latest/Data/TroubleshootHTTPEventCollector for more information\")` \nindex=_internal \"ERROR HttpInputDataHandler\" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkenterprisehosts` \n| eval event_message=coalesce(event_message,message) \n| rex field=event_message mode=sed \"s/(http_input_body_size=)\\d+|(totalRequestSize=)\\d+/\\1/\" \n| rex field=event_message mode=sed \"s/(channel=)[^,]+/channel=/\" \n| bin _time span=4h \n| fillnull reply \n| stats count by host, event_message, _time, reply \n| cluster t=0.999 field=event_message showcount=true \n| lookup splunkadmins_hec_reply_code_lookup status_code AS reply \n| eval count=count+cluster_count \n| fields - cluster_* \n| sort _time",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. HEC failures mean that the client is expected to handle the failure, this may occur when queues on the forwarder are full. Note that the lookup splunkadmins_hec_reply_code_lookup is based on https://docs.splunk.com/Documentation/Splunk/8.2.3/Data/TroubleshootHTTPEventCollector and this may change over time",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 3 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Splunk HTTP Listener Overwhelmed",
    "search": "`comment(\"Find if the HTTP listener cannot cope with the incoming load of data. Refer to https://docs.splunk.com/Documentation/Splunk/latest/Troubleshooting/HTTPthreadlimitissues for more information\")` \nindex=_internal \"HttpListener - Can't handle request for\" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkenterprisehosts`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. HTTP listeners should not be overwhelmed with incoming connections, the thread/socket limits may have been reached",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "53 3 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Splunk Heavy logging sources",
    "search": "`comment(\"Find splunk sources sending excessive amounts of logs in and then conditionally email the right team members\")` \nindex=_internal `splunkadmins_license_usage_source` type=Usage `licensemasterhost` sourcetype=splunkd `splunkadmins_heavylogging`\n| stats sum(b) as totalBytes by s, h, idx, st \n| eval totalMBInPast30Mins=round(totalBytes/1024/1024) \n| where totalMBInPast30Mins>500\n| table s, h, idx, st, totalMBInPast30Mins",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-30m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. Sources that are sending a large amount of log data...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "4,34 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Splunk Insufficient Permissions to Read Files",
    "search": "| tstats count groupby host, source\n| append \n    [ search\n        `comment(\"This search looks for insufficient permissions errors, the problem here is that we might have insufficient permissions to read a file but we might later obtain the correct permissions and then read the file (as permissions changes can happen *after* the file creation...this is why there is both a tstats listing all files (only done because I cannot find a nicer way to do this, map is possibly more compute intensive), and then a search for files\")`\n        index=_internal \"Insufficient permissions to read file\" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`)\n    | rex \"\\(hint: (?P<hint>[^\\)]+)\"\n    | stats min(_time) AS firstSeen, max(_time) AS mostRecent, values(hint) AS hint by file, host \n    | rex field=file \"'(?P<source>[^']+)'\" \n    | eval insufficientpermissions=\"true\" \n    | fields firstSeen, mostRecent, source, host, insufficientpermissions, hint] \n| search `comment(\"Ignore any files requested by the macro, i.e. source!= or host!= or similar...\")` `splunkadmins_permissions`\n| stats sum(count) AS count, min(firstSeen) AS firstSeen, max(mostRecent) AS mostRecent, values(insufficientpermissions) AS insufficientpermissions, values(hint) AS hint by host, source \n| search `comment(\"If we have an insufficient permissions error, did we see no data from our tstats command?\")` insufficientpermissions=\"true\" NOT count=* `splunkadmins_insufficient_permissions`\n    `comment(\"At this point if we see an insufficient permissions line, and we cannot see a result from the tstats showing indexed data from that file, then we have an issue, if not there is no issue with permisisons!\")` \n    `comment(\"Insufficient permissions to read file + hint: No such file or directory when the file exists on a Splunk enterprise instance might require TAILING_SKIP_READ_CHECK = 1 in the splunk-launch.conf refer to splunk support for more info\")` \n| eval invesSource=replace(source, \"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\") \n| addinfo \n| eval investigationQuery=\"index=_internal \\\"Insufficient permissions to read file\\\" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) earliest=\" . info_min_time . \" latest=\" . info_max_time . \" host=\" . host . \" file=\\\"'\" . invesSource . \"'\\\"\"\n| eval firstSeen=strftime(firstSeen, \"%+\"), mostRecent=strftime(mostRecent, \"%+\") \n| fields host, source, firstSeen, mostRecent, hint, investigationQuery",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d@d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. An insufficient permissions to read files error was thrown...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "@d",
    "cron_schedule": "51 6 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Splunk Universal Forwarders Exceeding the File Descriptor Cache",
    "search": "`comment(\"The file descriptor cache full message is a potential indicator that we are monitoring directories with many files and this might cause the forwarder to utilize extra CPU\")`\nindex=_internal \"TailReader - File descriptor cache is full\" (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) sourcetype=splunkd `splunkadmins_exceeding_filedescriptor`\n| eval message=coalesce(message,event_message)\n| stats values(message), count by host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. These forwarders may need an increase in their file descriptor cache limits",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 11 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Splunk Universal Forwarders that are time shifting",
    "search": "`comment(\"Detect universal forwarders that appear to be moving their clocks backwards/into the past or forwards, into the future. Timeshifting servers may need to be excluded from Splunk\")` \n`comment(\"The string \\\"WARN TimeoutHeap - Either time adjusted forwards by, or event loop was descheduled for\\\" looks similar but tends to relate to a poorly performing server rather than a time shift...\")`\nindex=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) \"Detected system time adjusted\" OR \"System time went \" `splunkadmins_uf_timeshifting`\n| rex \"by (?P<timePeriod>\\d+)ms\\.$\"\n| rex \"by (?P<timePeriodInSecs>[\\d\\.]+) seconds$\"\n| eval timePeriod=if(isnotnull(timePeriodInSecs),timePeriodInSecs*1000,timePeriod)\n| where timePeriod > 100000 \n| dedup host\n| fields host, _raw",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. The clock has changed many times on this server and may indicate a timeshfiting test environment",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 0 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Splunk forwarders are having issues with sending data to indexers",
    "search": "`comment(\"A could not send data to output queue from the indexers or heavy forwarders often indicates a performance issue\")` \nindex=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) \"Could not send data to output queue\" (`indexerhosts`) OR (`heavyforwarderhosts`) `splunkadmins_sending_data`\n| search `comment(\"Exclude shutdown times\")` AND NOT [`splunkadmins_shutdown_time(indexerhosts,0,0)`]\n| bin _time span=20m\n| stats count by host, _time\n| search (count>`splunkadmins_sending_data_nonhf_count` NOT `heavyforwarderhosts`) OR (count>`splunkadmins_sending_data_hf_count` `heavyforwarderhosts`)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. A level of these alerts just mean the indexer is busy / not receiving data fast enough, many alerts indicate the indexer is having serious issues.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "28 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Splunk forwarders failing due to disk space issues",
    "search": "`comment(\"Detect universal forwarders that do not have any disk space left and therefore cannot work as expected\")` \nindex=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) \"No space left on device\" \n| top host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. A universal forwarder has run out of disk space",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "45 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Splunk universal forwarders with ulimit issues",
    "search": "`comment(\"Detect universal forwarders that have the ulimit set too low for the number of file descriptors (ulimit -n)\")` \nindex=_internal log_level=WARN sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) component=ulimit \"Splunk may not work due to low file size limit\" \n| dedup host \n| fields host _raw",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Universal forwarder with ulimit issues",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 10 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - SplunkStream Errors",
    "search": "index=_internal source=\"*streamfwd.log\" ERROR OR FATAL `splunkadmins_streamerrors`\n| search `comment(\"Exclude time periods where shutdowns were occurring\")` AND NOT [`splunkadmins_shutdown_time(searchheadhosts,0,0)`]\n| cluster showcount=true\n| fields host, _raw",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Errors from the Splunk stream forwarders will normally require an action. Note that this search assumes your search heads are the ones hosting the stream application...you may need to customise this.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "9 11 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Stopping all listening ports",
    "search": "index=_internal `heavyforwarderhosts` OR `indexerhosts` sourcetype=splunkd source=*splunkd.log TERM(WARN) TERM(Stopping) \n| bin _time span=1m \n| stats count by host, _time \n| sort - _time",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. If the TCP listener ports stop temporarily it might be an issue with the downstream indexers (on a forwarder) if they stop for a long period or often enough than this is likely to cause issues with upstream forwarders",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "42 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - Unusual number of duplication alerts",
    "search": "`comment(\"The number of warnings about duplication seems unusually high and may require investigation\")` \n`comment(\"The duplication warnings will occur with indexer acknowledgement enabled and indexer shutdowns. Other circumstances likely require some kind of investigation, the issue may also appear if the forwarder is having trouble getting CPU time...\")`\nindex=_internal sourcetype=splunkd (`splunkadmins_splunkuf_source`) NOT (`splunkenterprisehosts`) \"duplication\" `splunkadmins_unusual_duplication`\n| search `comment(\"Exclude shutdown times\")` AND NOT [`splunkadmins_shutdown_time(indexerhosts,60,60)`]\n| stats count by host \n| where count > `splunkadmins_unusual_duplication_count`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. An unusual number of duplication alerts has appeared from these universal forwarders",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "07 22 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "ForwarderLevel - crcSalt or initCrcLength change may be required",
    "search": "`comment(\"Look for issues relating to CRC salt on any files...the universal forwarder settings may need tweaking to ensure the file is read as expected, or it may be a rolled file\")` \nindex=_internal \"You may wish to use larger initCrcLen for this sourcetype\" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) `splunkadmins_crcsalt_initcrc`\n`comment(\"Attempt to exclude rolled files from the check by looking for the most common pattern (.1, .2, .10 or similar)\")` \n`comment(\"This alert aims to find files where crcSalt = <SOURCE> might be required in the inputs.conf file or a tweak to the initCrcLen...\")`\n`comment(\"Regex for filename now replaces the default field extraction due to Windows based filenames containing spaces..\")`\n| rex \"file=(?P<file>.+)\\)\\.\"\n| regex file!=\"\\.\\d+$\" \n| eval message=coalesce(message,event_message)\n| top limit=500 file, host, message",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. The forwarder is advising the crcSalt = <SOURCE> or an initCrcLength change may be required on these files therefore these should be investigated.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "53 2 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Generate Data Availability ML Model for Latency",
    "search": "| tstats count values(source) as source avg(_indextime) as indextime where \n    [| inputlookup data_inventory_products_lookup \n    | eval category=case(isnotnull(daily_event_volume) AND daily_event_volume > 50000, \"highvolumelowlatency\", stage!=\"step-sourcetype\", \"normal\", 1=1,\"unused\") \n    | lookup SSE-data-availability-products-categorization productId OUTPUT category as category_override \n    | eval category=coalesce(category_override, category) \n    | search category=\"highvolumelowlatency\" OR category=\"normal\" \n    | eval termsearch = if(len(termsearch)>0,\"(\" . termsearch . \")\",null()) \n    | stats values(termsearch) as search \n    | eval search=\"(\" . mvjoin(search, \" OR \") . \")\" ] earliest=-30d by index sourcetype _time span=5m \n| eval reduce_factor_to_limit_memory_explosion = 10\n| where random()%reduce_factor_to_limit_memory_explosion = 1 \n| eval lag = indextime - _time - 180 \n| stats values(lag) as lag values(source) as source by index sourcetype _time \n| where lag > 0 \n| eval productId=null, \n    [| inputlookup data_inventory_products_lookup \n    | eval category=case(isnotnull(daily_event_volume) AND daily_event_volume > 50000, \"highvolumelowlatency\", stage!=\"step-sourcetype\", \"normal\", 1=1,\"unused\") \n    | lookup SSE-data-availability-products-categorization productId OUTPUT category as category_override \n    | eval category=coalesce(category_override, category) \n    | search category=\"highvolumelowlatency\" OR category=\"normal\" \n    | eval termsearch = if(len(termsearch)>0,\"(\" . termsearch . \")\",null()) \n    | rex field=termsearch mode=sed \"s/\\\"/\\\\\\\"/g\" \n    | eval search = \"productId=if(searchmatch(\\\"\" . termsearch . \"\\\"), mvappend(productId, \\\"\" . productId . \"\\\"), productId)\" \n    | stats values(search) as search \n    | eval search=mvjoin(search, \", \")]\n| mvexpand productId\n| fit DensityFunction lag  by productId upper_threshold=0.01 dist=norm into app:sse_event_lag_model\n| stats count as baseline_num_data_samples avg(lag) as baseline_avg_lag min(_time) as baseline_earliest max(_time) as baseline_latest by productId | eval baseline_update_last_run = now(), baseline_lag_at_last_update = now() - baseline_latest  | outputlookup SSE-data_availability_latency_status.csv",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "Splunk_Security_Essentials",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Generate Datamodels Lookup",
    "search": "| datamodel \n| spath input=_raw output=modelName path=modelName \n| spath input=_raw output=objectName path=objects{}.objectName \n| spath input=_raw output=parentName path=objects{}.parentName \n| spath input=_raw output=tags path=objects{}.comment \n| fields modelName objectName parentName tags \n| eval objectName=mvappend(objectName,\"ROOT - \"+modelName) \n| eval tags=mvappend(tags,mvindex(tags,0)) \n| eval nodeNameMerge=mvzip(objectName,tags) \n| mvexpand nodeNameMerge \n| eval nodeName=mvindex(split(nodeNameMerge,\",\"),0) \n| eval tags=mvindex(split(nodeNameMerge,\",{\\\"tags\\\":\"),1) \n| eval tags=replace(tags,\"[\\[\\]\\\"\\}]\",\"\") \n| eval datamodel=if(like(nodeName,\"ROOT%\"),modelName,modelName.\".\".nodeName) \n| eval nodeName=if(like(nodeName,\"ROOT%\"),null(),nodeName) \n| table datamodel modelName nodeName tags \n| join modelName \n    [| rest splunk_server=local count=0 /servicesNS/-/-/admin/datamodel-files \n    | rename title AS modelName,eai:acl.app AS app \n    | table modelName, app \n    | search NOT app IN(SplunkAppForWebAnalytics, search) \n    | join app \n        [| rest /services/apps/local \n        | table title version \n        | rename title as app\n            ]\n        ] \n| join datamodel type=outer \n    [| ssedata config=\"data_inventory\" \n    | fields data_source data_source_category data_source_category_tags data_source_category_datamodel \n    | eval data_source_category_datamodel=split(data_source_category_datamodel,\"|\") \n    | stats values(data_source_category) AS data_source_category BY data_source_category_datamodel \n    | rename data_source_category_datamodel AS datamodel \n    | eval data_source_category=mvjoin(data_source_category,\"|\")\n        ]\n| outputlookup datamodels",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "Splunk_Security_Essentials",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Generate Local Saved Search Lookup",
    "search": "| rest splunk_server=local count=0 /servicesNS/-/-/saved/searches \n| rex field=qualifiedSearch \".*datamodel(=\\\"|:\\\"|\\(\\\"|\\s|:|=)(\\\"|)(?<datamodel>[a-z,A-Z,_]*)\"\n| rex field=qualifiedSearch \".*datamodel(=\\\"|:\\\"|\\(\\\"|\\s|:|=)(\\\"|)[a-z,A-Z,_]*(\\\"|)\\.(\\\"|)(?<nodeName>[a-z,A-Z,_]*)\"\n| eval datamodel=if(isnotnull(nodeName), datamodel.\".\".nodeName, datamodel)\n| rex field=qualifiedSearch max_match=0 \"sourcetype=(\\\"|)(?<sourcetype>[^\\\"\\s]*)(|\\\")\" \n| rex field=qualifiedSearch max_match=0 \"source=(\\\"|)(?<source>[^\\\"]*)(|\\\")\" \n| rex field=qualifiedSearch max_match=0 \"`(?<macro>[^`]*)`\" \n| rex field=qualifiedSearch max_match=0 \".*(?:input| )lookup (?:[a-z,A-Z,_]*=[a-z,A-Z,_]{1,4} |)(?<lookup>[a-z,A-Z,_,0-9]*)\" \n\n| eval isCorrelationSearch=if('action.notable'=1 OR 'action.risk'=1 OR 'action.correlationsearch.enabled'=1,\"true\",\"false\")\n| eval isEnabled=if('disabled'=\"0\",\"true\",\"false\")\n| eval isScheduled=if(len('cron_schedule') >= 9,\"true\",\"false\")\n| rename title AS name\n| rename \nqualifiedSearch AS search,\naction.correlationsearch.label as displayTitle, \ncron_schedule AS schedule,\neai:acl.owner AS owner,\neai:acl.app as app,\naction.correlationsearch.annotations AS annotations\n| eval displayTitle=coalesce(name,displayTitle)\n| eval link=id\n| lookup datamodels datamodel\n| lookup SSE-STRT-macros-to-data_source_categories.csv macro OUTPUTNEW eventtypeId AS data_source_category\n| eval data_source_category=mvdedup(data_source_category)\n| eval sourcetype=mvjoin(mvdedup(sourcetype),\"|\")\n| eval source=mvjoin(mvdedup(source),\"|\")\n| eval macro=mvjoin(mvdedup(macro),\"|\")\n| eval lookup=mvjoin(mvdedup(lookup),\"|\")\n| eval datamodel=mvjoin(mvdedup(datamodel),\"|\")\n\n| eval sourcetype_regex_match=if(isnull(data_source_category) AND isnotnull(sourcetype),sourcetype,null())\n| eval source_regex_match=if(isnull(data_source_category) AND isnotnull(source),source,null())\n\n| rex field=\"sourcetype_regex_match\" max_match=0 \n    [| inputlookup SSE-default-data-inventory-products.csv \n    | eval eventtypeId=split(eventtypeId,\"|\") \n    | mvexpand eventtypeId \n    | search regex_pattern=* \n    | search NOT productId IN (MSSQL*, SQL*,MySQL*, Oracle*, ESXi* , VMWare*, Linux*, FireEye*, Fortinet*, Cylance*, Check*, Juniper*)\n    | sort 0 eventtypeId \n    | streamstats count \n    | eval row_id=\"row_\"+count \n    | fields row_id regex_pattern \n    | eval rexValue=\"(?<P_\"+row_id+\">\"+regex_pattern+\")\" \n    | stats values(rexValue) AS regexMerge \n    | eval regexMerge=mvjoin(regexMerge,\"|\") \n    | eval search=\"\\\"(\".regexMerge.\")\\\"\" \n    | fields search \n        ]\n| rex field=\"source_regex_match\" max_match=0 \n    [| inputlookup SSE-default-data-inventory-products.csv \n    | eval eventtypeId=split(eventtypeId,\"|\") \n    | mvexpand eventtypeId \n    | search regex_pattern=* \n    | search NOT productId IN (MSSQL*, SQL*,MySQL*, Oracle*, ESXi* , VMWare*, Linux*, FireEye*, Fortinet*, Cylance*, Check*, Juniper*)\n    | sort 0 eventtypeId \n    | streamstats count \n    | eval row_id=\"row_\"+count \n    | fields row_id regex_pattern \n    | eval rexValue=\"(?<P_\"+row_id+\">\"+regex_pattern+\")\" \n    | stats values(rexValue) AS regexMerge \n    | eval regexMerge=mvjoin(regexMerge,\"|\") \n    | eval search=\"\\\"(\".regexMerge.\")\\\"\" \n    | fields search \n        ]\n| foreach P_* \n    [| eval PatternStringMatch=if('<<FIELD>>'!=\"\", mvappend(PatternStringMatch,'<<FIELD>>'),PatternStringMatch) \n    | eval PatternStringDescription=if('<<FIELD>>'!=\"\", mvappend(PatternStringDescription,\"<<MATCHSTR>>\"),PatternStringDescription)\n        ] \n| fields - P_*\n| join PatternStringDescription type=outer[\n| inputlookup SSE-default-data-inventory-products.csv \n    | eval eventtypeId=split(eventtypeId,\"|\") \n    | mvexpand eventtypeId \n    | search regex_pattern=* \n    | search NOT productId IN (MSSQL*, SQL*,MySQL*, Oracle*, ESXi* , VMWare*, Linux*, FireEye*, Fortinet*, Cylance*, Check*, Juniper*)\n    | sort 0 eventtypeId \n    | streamstats count \n    | eval PatternStringDescription=\"row_\"+count \n    \n    | fields PatternStringDescription eventtypeId productId\n    | eventstats values(eventtypeId) AS eventtypeId  BY productId\n    | eval eventtypeId=mvjoin(eventtypeId,\"|\")\n    ]\n    | fields - Pattern*\n| eval data_source_category=coalesce(data_source_category,eventtypeId)\n| table app description displayTitle id isCorrelationSearch isEnabled isScheduled link name next_scheduled_time owner schedule search updated annotations data_source_category datamodel sourcetype source macro lookup\n| table app description displayTitle id isCorrelationSearch isEnabled isScheduled link name next_scheduled_time owner schedule search updated annotations data_source_category datamodel sourcetype source macro lookup\n| outputlookup createinapp=t SSE-local_savedsearches.csv",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "Splunk_Security_Essentials",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Generate MITRE Enterprise List",
    "search": "| mitremap output=\"list\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "This search formats the MITRE ATT&CK framework table into a list format.",
    "eai:acl.app": "Splunk_Security_Essentials",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Generate MITRE Environment Count",
    "search": "| sseanalytics \n| lookup use_cases.csv title AS \"usecase\" OUTPUT order_title AS use_case_order_title \n| eval \"Use Case Orig\"=if(isnull('usecase'),\"Other\",'usecase') \n| eval \"Use Case\"=if(isnull(use_case_order_title), \"9_Other\",use_case_order_title) \n| eval \"Has Data\"=if(data_available=\"Good\" OR data_available=\"Mixed\", \"Yes\", \"No\") \n| eval displayapp=mvappend(displayapp,\"Any\")\n| eval Status=case(\n    'enabled'=\"Yes\" AND 'Has Data'=\"Yes\",\"Active\",\n    'enabled'!=\"Yes\" AND 'Has Data'=\"Yes\",\"Available\",\n    1=1,\"Needs data\"\n    ) \n| eval App=displayapp, Enabled=enabled, Title=name, Journey=journey, \"Data Source\"=datasource, \"Data Source Category\"=data_source_categories_display \n| where (mitre_tactic_display!=\"\" AND mitre_tactic_display!=\"None\") OR (mitre_technique_display!=\"\" AND mitre_technique_display!=\"None\" AND mvcount(mitre_technique_display)>0) \n| eval mitre_sub_technique_display=if(mitre_sub_technique_display=\"None\",\"\",mitre_sub_technique_display) \n| eval \"MITRE Tactic\"=if('mitre_tactic_display'=\"\",\"-\",'mitre_tactic_display'), \"MITRE Technique\"=mitre_technique_display, \"MITRE Sub-Technique\"=mitre_sub_technique_display, \"MITRE Threat Group\"=mitre_threat_groups, \"MITRE Platforms\"=mitre_platforms, Industry=split(industryMapping,\"|\") \n| table mitre_matrix \"MITRE Tactic\" \"MITRE Technique\" \"MITRE Sub-Technique\" Status \"Data Source\" App bookmark_status mitre_id \n| rename mitre_matrix AS \"MITRE Matrix\" \n| mvexpand \"mitre_id\" \n| lookup mitre_enterprise_list TechniqueIdCombined AS \"mitre_id\" OUTPUT Sub_Technique AS \"MITRE Sub-Technique\" Technique AS \"MITRE Technique\"  Tactic AS \"MITRE Tactic\" \n| mvexpand \"MITRE Tactic\" \n| eval \"MITRE Sub-Technique\"=mvdedup('MITRE Sub-Technique'),\"MITRE Technique\"=mvdedup('MITRE Technique'),\"MITRE Tactic\"=mvdedup('MITRE Tactic') \n| eval mitre_technique_datasource_merge=mvzip('MITRE Technique','Data Source',\"::\") \n| eventstats sum(eval(if(len('MITRE Sub-Technique')=1,1,0))) AS MITRE_Technique_Datasource_Total BY \"mitre_technique_datasource_merge\" \n| eval mitre_technique_datasource_merge=mvzip('Data Source','MITRE_Technique_Datasource_Total',\"::\") \n\n| eventstats sum(eval(if(len('MITRE Sub-Technique')<=1,1,0))) AS MITRE_Tactic_Total BY \"MITRE Tactic\" \n| eventstats sum(eval(if(len('MITRE Sub-Technique')<=1,1,0))) AS MITRE_Technique_Total BY \"MITRE Technique\" \"MITRE Tactic\" \n| eventstats count AS MITRE_Technique_Total BY \"MITRE Technique\" \"MITRE Tactic\" \n| eventstats sum(eval(if(len('MITRE Sub-Technique')>1,1,0))) AS MITRE_Sub_Technique_Total BY \"MITRE Tactic\" \"MITRE Sub-Technique\" \n| eventstats sum(eval(if(len('MITRE Sub-Technique')<=1,1,0))) AS MITRE_Tactic_Technique_Total BY \"MITRE Tactic\" \"MITRE Technique\" \n| stats delim=\",\"\n    sum(eval(if(len('MITRE Sub-Technique')<=1 AND Status=\"Active\",1,0))) AS \"Active\",\n    sum(eval(if(len('MITRE Sub-Technique')<=1 AND Status=\"Available\",1,0))) AS \"Available\",\n    sum(eval(if(len('MITRE Sub-Technique')<=1 AND Status=\"Needs data\",1,0))) AS \"Needs data\",\n    sum(eval(if(len('MITRE Sub-Technique')<=1 AND bookmark_status!=\"none\",1,0))) AS \"Bookmarked_Technique\",\n    sum(eval(if(len('MITRE Sub-Technique')>1 AND Status=\"Active\",1,0))) AS \"Active_SubTechnique\",\n    sum(eval(if(len('MITRE Sub-Technique')>1 AND Status=\"Available\",1,0))) AS \"Available_SubTechnique\",\n    sum(eval(if(len('MITRE Sub-Technique')>1 AND Status=\"Needs data\",1,0))) AS \"Needs data_SubTechnique\",\n    sum(eval(if(len('MITRE Sub-Technique')>1 AND bookmark_status!=\"none\",1,0))) AS \"Bookmarked_SubTechnique\",\n    max(MITRE_Tactic_Total) AS MITRE_Tactic_Total,\n    max(MITRE_Technique_Total) AS MITRE_Technique_Total\n    max(MITRE_Sub_Technique_Total) AS MITRE_Sub_Technique_Total,\n    max(MITRE_Tactic_Technique_Total) AS MITRE_Tactic_Technique_Total,\n    values(mitre_technique_datasource_merge) AS \"Data Source\"\n    BY \"MITRE Tactic\",\"MITRE Technique\",\"MITRE Sub-Technique\",\"MITRE Matrix\" App\n| eval c=\"This fixes the counts for Techniques that have Sub-Techniques\" \n| eventstats delim=\",\"\n    sum(\"Active\") AS \"Active_TechniqueTotal\",\n    sum(\"Available\") AS \"Available_TechniqueTotal\",\n    sum(\"Needs data\") AS \"Needs data_TechniqueTotal\",\n    BY \"MITRE Tactic\",\"MITRE Technique\",\"MITRE Matrix\" App\n| eval \"Active\"=if(isnull('Sub-Technique'), 'Active_TechniqueTotal', 'Active') \n| eval \"Available\"=if(isnull('Sub-Technique'), 'Available_TechniqueTotal', 'Available') \n| eval \"Needs data\"=if(isnull('Sub-Technique'), 'Needs data_TechniqueTotal', 'Needs data') \n| eval \"Active\"=if(len('MITRE Sub-Technique')>1, 'Active_SubTechnique', 'Active') \n| eval \"Available\"=if(len('MITRE Sub-Technique')>1, 'Available_SubTechnique', 'Available') \n| eval \"Needs data\"=if(len('MITRE Sub-Technique')>1, 'Needs data_SubTechnique', 'Needs data') \n| eval \"Bookmarked\"='Bookmarked_Technique'+'Bookmarked_SubTechnique' \n| fields - *_TechniqueTotal *_SubTechnique \n| rename \"MITRE *\" AS *,\"MITRE_*\" AS * \n| eval \"Sub-Technique\"=if('Sub-Technique'=\"\", \"-\", 'Sub-Technique') \n| eval IsSubTechnique=if('Sub-Technique'=\"-\", \"No\", \"Yes\") \n| table Matrix Tactic Tactic_Total Technique Technique_Total \"Sub-Technique\" \"Sub_Technique_Total\" IsSubTechnique Active \"Available\" \"Needs data\" Technique_Total \"Data Source\" App Bookmarked \n| eval c=\"This appends all techniques and sub-techniques for which we have no detections\" \n| lookup mitre_enterprise_list Matrix Tactic Technique Sub_Technique AS \"Sub-Technique\" OUTPUT Technique_Order Sub_Technique_Order \n| append \n    [| inputlookup mitre_enterprise_list \n    | fields Matrix Tactic Sub_Technique Technique Technique_Order Sub_Technique_Order \n    | rename Sub_Technique AS \"Sub-Technique\"] \n| eventstats values(App) AS AllApps\n| eval Technique_Order=coalesce(Technique_Order,\"-\"),Sub_Technique_Order=coalesce(Sub_Technique_Order,\"-\"),App=coalesce(App,AllApps)  \n| stats delim=\",\"\n    sum(Active) AS Active,\n    sum(Available) AS Available,\n    sum(\"Needs data\") AS \"Needs data\",\n    sum(Sub_Technique_Total) AS Sub_Technique_Total,\n    sum(Tactic_Total) AS Tactic_Total,\n    sum(Technique_Total) AS Technique_Total,\n    sum(Bookmarked) AS Bookmarked,\n    values(\"Data Source\") AS \"Data Source\",\n    values(\"IsSubTechnique\") AS \"IsSubTechnique\"\n    BY\n    Matrix, Tactic Technique Sub-Technique Technique_Order Sub_Technique_Order, App\n| fillnull Active Available \"Needs data\" Sub_Technique_Total Tactic_Total Technique_Total value=\"0\" \n| eval IsSubTechnique=if('Sub-Technique'=\"-\", \"No\", \"Yes\") \n| eval Bookmarked=coalesce(Bookmarked, 0) \n| sort 0 Technique_Order Sub_Technique_Order \n| fields - Technique_Order Sub_Technique_Order\n| outputlookup mitre_environment_count.csv",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "0",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "Splunk_Security_Essentials",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "admin",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Generate STRT Macros to Data Source Categories Lookup",
    "search": "| inputlookup sse_json_doc_storage_lookup \n| eval len=len(json), key=_key \n| search key=\"*macros*\" \n| table key description version len json \n| spath input=json path=\"macros{}\" output=\"macros\" \n| fields macros \n| mvexpand macros \n| spath input=macros \n| table name definition \n| rename name AS macro\n\n| search definition=*source* OR definition=*eventtype*\n| rex field=definition max_match=0 \"sourcetype=(\\\"|)(?<sourcetype>[^\\\"\\s]*)(|\\\")\" \n| rex field=definition max_match=0 \"source=(\\\"|)(?<source>[^\\\"\\s]*)(|\\\")\" \n| eval sourcetype=case(\nlike(definition,\"%eventtype%\") AND like(definition,\"%cisco_ios%\"),\"eventtype=cisco_ios\",\nlike(definition,\"%eventtype%\") AND like(definition,\"%osquery-process%\"),\"osquery:results\",\nlike(definition,\"%eventtype%\") AND like(definition,\"%okta_log%\"),\"Okta\",\n1=1,sourcetype\n)\n| eval source=case(\nlike(definition,\"%eventtype%\") AND like(definition,\"%wineventlog_security%\"),\"WinEventLog:Security\",\nlike(definition,\"%eventtype%\") AND like(definition,\"%wineventlog_system%\"),\"WinEventLog:System\",\n1=1,source\n)\n| rex field=\"sourcetype\" max_match=0 \n    [| inputlookup SSE-default-data-inventory-products.csv \n    | eval eventtypeId=split(eventtypeId,\"|\") \n    | mvexpand eventtypeId \n    | search regex_pattern=* \n    | search NOT productId IN (MSSQL*, SQL*,MySQL*, Oracle*, ESXi* , VMWare*, Linux*, FireEye*, Fortinet*, Cylance*, Check*, Juniper*)\n    | sort 0 eventtypeId \n    | streamstats count \n    | eval row_id=\"row_\"+count \n    | fields row_id regex_pattern \n    | eval rexValue=\"(?<P_\"+row_id+\">\"+regex_pattern+\")\" \n    | stats values(rexValue) AS regexMerge \n    | eval regexMerge=mvjoin(regexMerge,\"|\") \n    | eval search=\"\\\"(\".regexMerge.\")\\\"\" \n    | fields search \n        ]\n| rex field=\"source\" max_match=0 \n    [| inputlookup SSE-default-data-inventory-products.csv \n    | eval eventtypeId=split(eventtypeId,\"|\") \n    | mvexpand eventtypeId \n    | search regex_pattern=* \n    | search NOT productId IN (MSSQL*, SQL*,MySQL*, Oracle*, ESXi* , VMWare*, Linux*, FireEye*, Fortinet*, Cylance*, Check*, Juniper*)\n    | sort 0 eventtypeId \n    | streamstats count \n    | eval row_id=\"row_\"+count \n    | fields row_id regex_pattern \n    | eval rexValue=\"(?<P_\"+row_id+\">\"+regex_pattern+\")\" \n    | stats values(rexValue) AS regexMerge \n    | eval regexMerge=mvjoin(regexMerge,\"|\") \n    | eval search=\"\\\"(\".regexMerge.\")\\\"\" \n    | fields search \n        ]\n| foreach P_* \n    [| eval PatternStringMatch=if('<<FIELD>>'!=\"\", mvappend(PatternStringMatch,'<<FIELD>>'),PatternStringMatch) \n    | eval PatternStringDescription=if('<<FIELD>>'!=\"\", mvappend(PatternStringDescription,\"<<MATCHSTR>>\"),PatternStringDescription)\n        ] \n| fields - P_*\n| join PatternStringDescription type=outer[\n| inputlookup SSE-default-data-inventory-products.csv \n    | eval eventtypeId=split(eventtypeId,\"|\") \n    | mvexpand eventtypeId \n    | search regex_pattern=* \n    | search NOT productId IN (MSSQL*, SQL*,MySQL*, Oracle*, ESXi* , VMWare*, Linux*, FireEye*, Fortinet*, Cylance*, Check*, Juniper*)\n    | sort 0 eventtypeId \n    | streamstats count \n    | eval PatternStringDescription=\"row_\"+count \n    \n    | fields PatternStringDescription eventtypeId productId\n    | eventstats values(eventtypeId) AS eventtypeId  BY productId\n    | eval eventtypeId=mvjoin(eventtypeId,\"|\")\n    ]\n    | fields - Pattern*\n```Filter our matches against non-Vendor specific data source categories. If a detection uses this specific vendor technology it will only rely on that and not of datamodels```\n| eval eventtypeId=split(eventtypeId,\"|\")\n| eval eventtypeId_VendorOnly=mvfilter(match(eventtypeId, \"Vendor\"))\n| eval eventtypeId=coalesce(eventtypeId_VendorOnly,eventtypeId)\n| fields - eventtypeId_VendorOnly\n```END ```\n| outputlookup createinapp=t SSE-STRT-macros-to-data_source_categories",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "Splunk_Security_Essentials",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Buckets changes per day",
    "search": "`comment(\"May not be 100% accurate, still under testing. deleteBucket frozen=false is usually excess bucket removal\")`\nindex=_internal \"Creating hot bucket\" OR (\"CMSlave - deleteBucket\" AND \"frozen=false\") OR \"freeze succeeded\" sourcetype=splunkd source=*splunkd.log `indexerhosts`\n`comment(\"Multiplying by replication factor as each hot bucket is duplicated\")`\n`comment(\"To split by index\n| rex \"(/[^/]+){2}/(?P<idx2>[^/]+)\"\n| eval idx=if(isnull(idx),idx2,idx)\n\")`\n| timechart span=1d count(eval(searchmatch(\"Creating hot bucket\"))) AS created, count(eval(searchmatch(\"(\\\"CMSlave - deleteBucket\\\" AND \\\"frozen=false\\\") OR \\\"freeze succeeded\\\"\"))) AS frozenCount\n| eval change=(created*`splunkadmins_replicationfactor`)-frozenCount\n| fields - created, frozenCount",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Attempt to count the number of buckets added/removed within an indexer cluster in order to forecast potential capacity issues with the cluster master",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Buckets have being frozen due to index sizing",
    "search": "`comment(\"The indexer is freezing buckets due to disk space pressure before the frozenTimePeriodInSecs limit has been reached, this could be a problem if it is not expected...\")`\n`comment(\"_introspection defaults to size based so exclude it\")` \nindex=_internal `indexerhosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) \"BucketMover - will attempt to freeze\" NOT \"because frozenTimePeriodInSecs=\" \n`splunkadmins_bucketfrozen`\n| rex field=bkt \"(rb_|db_)(?P<newestDataInBucket>\\d+)_(?P<oldestDataInBucket>\\d+)\"\n| eval newestDataInBucket=strftime(newestDataInBucket, \"%+\"), oldestDataInBucket = strftime(oldestDataInBucket, \"%+\") \n| eval message=coalesce(message,event_message)\n| table message, oldestDataInBucket, newestDataInBucket",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-5h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. One or more indexes have hit the index size limit and are now been frozen due to this",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "33 3,7,11,15,19,23 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Buckets in cache",
    "search": "| rest `splunkindexerhostsvalue` /services/admin/cacheman f=cm:bucket* count=0\n| rex field=title \"bid\\|(?P<index>[^~]+)\" \n| stats min(cm:bucket.earliest_time) AS mintime by index\n| eval days=round((now()-mintime)/60/60/24)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is an example search, note that since cacheman is a /admin/ endpoint it is undocumented and this may not work in all versions of Splunk (tested on 8.2.2.1)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "33 */4 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Buckets rolling more frequently than expected",
    "search": "`comment(\"Buckets are moving from warm to cold very quickly and this could be an issue related to the sizing not been valid for the indexes...\")` \n`indexerhosts` index=_internal \"Will chill bucket\" (`splunkadmins_splunkd_source`) sourcetype=splunkd \"/db/db\"  \n| rex \"=/.*?(?P<indexname>[^/]+)(/[^/]+){2} \" \n| stats count by indexname \n| sort - count \n| where (count>`splunkadmins_bucketrolling_count`)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Indexer level issues - Buckets are moving out of the warm state quicker than expected and this may (or may not) be an issue, this could indicate that hot is undersized or there are too many buckets in the warm area. In Splunk 7.2 the monitoring console has introduced Health warning - The percentage of small of buckets created (x) over the last hour is very high, this new warning will likely replace this alert",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "8 8 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - ClusterMaster Advising SearchOrRep Factor Not Met",
    "search": "| rest /services/cluster/searchhead/generation splunk_server=local | where is_searchable!=1 OR replication_factor_met!=1 OR search_factor_met!=1 | table is_searchable replication_factor_met, search_factor_met\n| search `comment(\"If the cluster master advises there is an issue, you probably want to check why\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. The cluster master shows that either not all data is searchable or rep/search factors are not met",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/10 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Cold data location approaching size limits",
    "search": "| rest `splunkindexerhostsvalue` /services/data/indexes/ \n| search\n    `comment(\"This search attempts to find indexes which are about to start rolling buckets to frozen due to disk space issues by checking how much percentage of the allocated cold section of disk is used. It does not take into account any volume sizing...\")`\n    `comment(\"This is the more proactive form of IndexerLevel - Buckets are been frozen due to index sizing\")` \n| join title splunk_server type=outer \n    [| rest `splunkindexerhostsvalue` /services/data/indexes-extended/] \n| stats values(bucket_dirs.cold.bucket_size) AS currentColdSizeMB, values(bucket_dirs.home.warm_bucket_size) AS currentWarmSizeMB, max(bucket_dirs.home.event_max_time) AS latestTime, min(bucket_dirs.cold.event_min_time) AS earliestTime, min(bucket_dirs.home.event_min_time) AS earliestTimeHot, values(coldPath.maxDataSizeMB) AS coldPathSizeLimitMB, values(currentDBSizeMB) AS hotSizeMB, values(maxTotalDataSizeMB) AS maxTotalDataSizeMB, values(frozenTimePeriodInSecs) AS frozenTimePeriodInSecs, values(maxDataSize) AS maxDataSize, values(homePath.maxDataSizeMB) AS hotPathMaxDataSizeMB, values(bucket_dirs.home.warm_bucket_count) AS currentWarmCount, values(maxWarmDBCount) AS maxWarmDBCount by name, splunk_server \n| eval currentColdSizeMB=coalesce(currentColdSizeMB,currentWarmSizeMB), earliestTime=coalesce(earliestTime,earliestTimeHot)\n| eval \"Days of data based on epoch values\"=round((latestTime-earliestTime)/3600/24) \n| rename name AS index, splunk_server AS indexer\n| search  `comment(\"Things do get a little bit messy here, if the cold path size is unlimited, the remaining data is the maxTotalSizeMB minus what we have already used in the hot section (not a perfect calculation but close enough for our purposes\")` \n| eval warm_bucket_percent = (100 / maxWarmDBCount) * currentWarmCount \n| eval coldPathSizeLimitMB=case(warm_bucket_percent>95 AND coldPathSizeLimitMB==0,maxTotalDataSizeMB-currentWarmSizeMB,coldPathSizeLimitMB==0 AND hotPathMaxDataSizeMB==0,maxTotalDataSizeMB,coldPathSizeLimitMB==0 AND hotPathMaxDataSizeMB!=0,maxTotalDataSizeMB-hotPathMaxDataSizeMB,1=1,coldPathSizeLimitMB) \n| eval percUsed=round((currentColdSizeMB/coldPathSizeLimitMB)*100,2) \n| eval frozenTimeInDays=frozenTimePeriodInSecs/60/60/24 \n| eval maxDataSize=case(maxDataSize=\"auto\",\"750\",maxDataSize=\"auto_high_volume\",\"10240\",true(),maxDataSize)\n| eval worstCaseBucketCountLeft=floor((coldPathSizeLimitMB-currentColdSizeMB)/maxDataSize)\n| where percUsed>`splunkadmins_colddata_percused` \n| search `splunkadmins_colddata`\n| table index, indexer, currentColdSizeMB, coldPathSizeLimitMB, percUsed, frozenTimeInDays, \"Days of data based on epoch values\", worstCaseBucketCountLeft",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. One or more indexes are approaching the disk limits on their cold data, therefore the buckets will roll to frozen once this limit is reached...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "13 7 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Corrupt buckets via DBInspect",
    "search": "| dbinspect corruptonly=true index=*\n| regex path!=\"[\\\\\\\\/](\\d+_|hot_)[^ ]+$\"\n| table bucketId, corruptReason, index, modTime, path, splunk_server, state\n| eval command=\"splunk fsck repair --one-bucket --bucket-path=\" + path + \" &\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-7d@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Run the dbinspect command to determine which buckets appear to be corrupt, the search ignores hot/streaming hot buckets as they appear to be corrupt when tested in 7.0.5, note that this appears to check metadata not journal files so splunk fsck may still be required",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Data parsing error",
    "search": "`comment(\"LineBreakingProcessor ERROR's are usually related to misconfiguration/errors in the LINE_BREAKER= setup in props.conf and are therefore an issue. This version of the Aggregator error often relates to date time config files\")`\nindex=_internal (WARN CsvLineBreaker) OR (\"ERROR\" (\"JsonLineBreaker\" OR \"LineBreakingProcessor\" OR \"AggregatorMiningProcessor\") NOT \"WARN  AggregatorMiningProcessor\") sourcetype=splunkd (`splunkadmins_splunkd_source`) (`indexerhosts`) OR (`heavyforwarderhosts`) `splunkadmins_dataparsing_error` \n| rex \"(?s)^(\\S+\\s+){3}(?P<error>.*)\"\n| stats count, latest(_time) AS mostrecent, earliest(_time) AS firstseen, values(host) AS hosts by error\n| eval mostrecent=strftime(mostrecent, \"%+\"), firstseen=strftime(firstseen, \"%+\")\n| table count, mostrecent, firstseen, hosts, error",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-10m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. This alert advises there is an error with the LINE_BREAKER or Aggregator, this generally relates to a misconfiguration that requires a fix...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/10 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Failures To Parse Timestamp Correctly (excluding breaking issues)",
    "search": "`comment(\"Timestamp parsing has failed, and it doesn't appear to be related to the event been broken due to having too many lines, that is a separate alert that may trigger a timestamp parsing issue (excluded from this alert as that issue needs to be resolved first)\")` \n`comment(\"Please note that you may see this particular warning on data that is sent to the nullQueue using a transforms.conf. Obviously you won't see this in the index but you will see the warning because the time parsing occurs before the transforms.conf occurs\")`\n`comment(\"This alert now checks for at least 2 failures, and header entries can often trigger 2 entries in the log files about timestamp parsing failures...\")`\n`comment(\"Finally one strange edge case is a newline inserted into the log file (by itself with no content before/afterward) can trigger the warning but nothing will get indexed, multiline_event_extra_waittime, time_before_close and EVENT_BREAKER can resolve this edge case\")`\nindex=_internal sourcetype=splunkd (\"Failed to parse timestamp\" \"Defaulting to timestamp of previous event\") OR \"Breaking event because limit of \" OR \"outside of the acceptable time window\" (`splunkadmins_splunkd_source`) (`indexerhosts`) OR (`heavyforwarderhosts`) `splunkadmins_failuretoparse_timestamp`\n| bin _time span=`splunkadmins_failuretoparse_timestamp_binperiod` \n| eval host=data_host, source=data_source, sourcetype=data_sourcetype\n| rex \"source::(?P<source>[^|]+)\\|host::(?P<host>[^|]+)\\|(?P<sourcetype>[^|]+)\" \n| eventstats count(eval(isnotnull(data_host))) AS hasBrokenEventOrTuncatedLine, count(eval(searchmatch(\"outside of the acceptable time window\"))) AS outsideTimewindow by _time, host, source, sourcetype\n| where hasBrokenEventOrTuncatedLine=0 AND isnull(data_host) AND NOT searchmatch(\"outside of the acceptable time window\")\n| search `comment(\"To investigate further we want the previous timestamp that Splunk used for the event in question, that way we can see what it looks like in raw format...\")`\n| rex \"Defaulting to timestamp of previous event \\((?P<previousTimeStamp>[^)]+)\"\n| eval previousTimeStamp=strptime(previousTimeStamp, \"%a %b %d %H:%M:%S %Y\")\n| stats count, min(_time) AS firstSeen, max(_time) AS mostRecent, first(previousTimeStamp) AS recentExample, sum(outsideTimewindow) AS outsideTimewindow by host, sourcetype, source\n| where count>`splunkadmins_failuretoparse_timestamp_count`\n| stats sum(count) AS count, min(firstSeen) AS firstSeen, max(mostRecent) AS mostRecent, first(recentExample) AS recentExample, values(source) AS sourceList, sum(outsideTimewindow) AS outsideTimewindow by host, sourcetype\n| search `comment(\"Allow exclusions based on count or similar...\")` `splunkadmins_failuretoparse_timestamp2`\n| eval invesEnd=recentExample+1\n| eval invesDataSource=sourceList\n| eval invesDataSource=if(mvcount(invesDataSource)>1,mvjoin(invesDataSource,\"\\\" OR source=\\\"\"),invesDataSource)\n| eval invesDataSource = \"source=\\\"\" + invesDataSource + \"\\\"\"\n| eval invesDataSource = replace(invesDataSource, \"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\")\n| eval investigationQuery=\"`comment(\\\"The investigation query may find zero data if the data was sent to the null queue by a transforms.conf as the time parsing occurs before the transforms occur. If this source/sourcetype has a null queue you may need to exclude it from this alert\\\")` `comment(\\\"Note that the host= can be inaccurate if host overrides are in use in transforms.conf, if this query finds no results remove host=...\\\")` index=* host=\" . host . \" sourcetype=\\\"\" . sourcetype . \"\\\" \" . invesDataSource . \" earliest=\" . recentExample . \" latest=\" . invesEnd . \" | eval indextime=strftime(_indextime, \\\"%+\\\")\" \n| eval mostRecent=strftime(mostRecent, \"%+\"), firstSeen=strftime(firstSeen, \"%+\")\n| eval outsideAcceptableTimeWindow=if(outsideTimewindow!=0,\"Timestamp parsing failed due to been outside the acceptable time window\",\"No\")\n| fields - recentExample, invesEnd, invesDataSource, outsideTimewindow\n| sort - count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Failures to parse incoming log file timestamps, this excludes a timestamp failure due to the event been broken (there is a separate alert for breaking issues)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 3 * * 5",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Future Dated Events that appeared in the last week",
    "search": "`comment(\"Data should not appear from the future...this alert finds that data so it can be investigated\")` `comment(\"This query could be | tstats count, max(_time), min(_time) where index=* earliest=+5m latest=+10y groupby index, sourcetype, _time, _indextime span=1d, and then drop the field named 'ahead', but in 8.2.5 this is slower than the index= version\")` \nindex=* earliest=+5m latest=+10y `splunkadmins_future_dated`\n| eval ahead=abs(now() - _time)\n| eval indextime=_indextime\n| bin span=1d indextime \n| eval timeToLookBack=now()-(60*60*24*7)\n| stats avg(ahead) as averageahead, max(_time) AS maxTime, min(_time) as minTime, count, first(timeToLookBack) AS timeToLookBack by host, sourcetype, index, indextime\n| where indextime>timeToLookBack AND averageahead > 1000\n| eval averageahead =tostring(averageahead, \"duration\")\n| eval invesMaxTime=if(minTime=maxTime,maxTime+1,maxTime)\n| eval investigationQuery=\"index=\" . index . \" host=\" . host . \" sourcetype=\\\"\" . sourcetype . \"\\\" earliest=\" . minTime . \" latest=\" . invesMaxTime . \" _index_earliest=\" . timeToLookBack . \" |  eval indextime=strftime(_indextime, \\\"%+\\\")\"\n| eval indextime=strftime(indextime, \"%+\"), maxTime = strftime(maxTime, \"%+\"), minTime = strftime(minTime, \"%+\")\n| table host, sourcetype, index, averageahead, indextime, minTime, maxTime, count, investigationQuery",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Search for any data that has future based time-stamping, this likely shows a date parsing issue or a server sending logs with a date in the future",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 0 * * 2",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Index not defined",
    "search": "`comment(\"Detect if data is been sent to the indexers to an index which is not yet configured\")`\nindex=_internal \"Received event for unconfigured\" sourcetype=splunkd `splunkadmins_splunkd_source` \"IndexerService - Received event for unconfigured\" `indexerhosts`\n| rex \"index=(?P<index>[^ ]+).*source=\\\"source::(?P<source>[^\\\"]+)\\\" host=\\\"host::(?P<host>[^\\\"]+)\"\n| eval message=coalesce(message,event_message)\n| stats min(_time) AS firstSeen, max(_time) AS lastSeen, values(source) AS sourceList, values(host) AS hostsSendingToThisIndex, first(_raw) AS message by index\n| eval sources=mvjoin(sources, \", \"), firstSeen=strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Either the remote forwarder is sending to the wrong index name or the index has not been defined, either way the data will be rejected by the indexer",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "33 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - IndexConfig Warnings from Splunk indexers",
    "search": "`comment(\"IndexConfig warnings are generally a problem\")` \nindex=_internal \"WARN  IndexConfig\" OR \"ERROR IndexConfig\" OR (ClusterMasterControlHandler \" ERROR \" OR \" WARN \" NOT \"*No new dry run will be performed\") (`splunkadmins_splunkd_source`) `indexerhosts` OR `cluster_masters` `splunkadmins_indexconfig_warn` \n| eval message=coalesce(message,event_message) \n| stats count by message, host \n| eval why=\"Bundle validation failure on indexing tier, please investigate\" \n| table why, message, count, host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. IndexConfig warnings are usually a problem so should be investigated...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 2 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - IndexWriter pause duration",
    "search": "index=_internal sourcetype=splunkd `splunkadmins_splunkd_source` `indexerhosts` INFO (IndexWriter paused) OR Released \n| eval state=if(searchmatch(\"Released indexing throttle\"),\"stopped\",\"started\") \n| sort 0 _time \n| streamstats current=f global=f window=1 values(state) AS prev_state, latest(_time) AS start by host \n| streamstats current=f global=f window=1 values(bucket) AS bucket by host, idx \n| search state=\"stopped\" AND prev_state=\"started\" \n| eval duration=_time-start \n| table host, idx, duration, _time, start, bucket \n| eval start=strftime(start, \"%Y-%m-%d %H:%M:%S.%3N\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report will measure the period of time that is mentioned as \"paused\" by the IndexWriter and then \"Released\" from the throttle",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "38 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Indexer Out Of Disk Space",
    "search": "`comment(\"The indexer has run out of disk space, this requires immediate investigation...\")` \nindex=_internal \"event=onFileWritten err=\\\"disk out of space\\\"\" OR \"event=replicationData status=failed err=\\\"onFileWritten failed\\\"\" `indexerhosts` (`splunkadmins_splunkd_source`) sourcetype=splunkd \n| top host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. The indexer has run out of disk space while attempting to write to the filesystem",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "4,19,34,49 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Indexer Queues May Have Issues",
    "search": "`comment(\"This alert is borrowed from the monitoring console. When the queues are filled there is an issue in the indexer cluster!\")`\nindex=_internal `indexerhosts` source=*metrics.log sourcetype=splunkd group=queue \n| eval ingest_pipe = if(isnotnull(ingest_pipe), ingest_pipe, \"none\") | search ingest_pipe=*\n| eval name=case(name==\"aggqueue\",\"2 - Aggregation Queue\",\n name==\"indexqueue\", \"4 - Indexing Queue\",\n name==\"parsingqueue\", \"1 - Parsing Queue\",\n name==\"typingqueue\", \"3 - Typing Queue\",\n name==\"splunktcpin\", \"0 - TCP In Queue\",\n name==\"tcpin_cooked_pqueue\", \"0 - TCP In Queue\") \n| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size) \n| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size) \n| eval fill_perc=round((curr/max)*100,2) \n| eval combined = host . \"_pipe_\" . ingest_pipe\n| bin _time span=1m\n| stats Median(fill_perc) AS \"fill_percentage\" by combined, _time, name \n| where (fill_percentage>`splunkadmins_indexerqueue_fillperc_nonindexqueue` AND name!=\"4 - Indexing Queue\") OR (fill_percentage>`splunkadmins_indexerqueue_fillperc_indexqueue` AND name=\"4 - Indexing Queue\") \n| eventstats dc(combined) AS servercount \n| eventstats count AS totalcount by combined, name \n| where totalcount>`splunkadmins_indexerqueue_count`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-11m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. One or more indexer queues have been filled for a period of time and may require investigation.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/11 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Indexer not accepting TCP Connections",
    "search": "`comment(\"The indexer not accepting TCP connections is either a serious performance issue or downtime\")` \nindex=_internal TcpOutputFd \"connection refused\" sourcetype=splunkd (`splunkadmins_splunkd_source`) OR (`splunkadmins_splunkuf_source`) \n| rex \"Connect to (?P<clientip>[^:]+)\" \n| top clientip \n| lookup dnslookup clientip \n| where count>10",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. The indexer is either overloaded or down and not accepting TCP connections...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "3,18,33,48 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Indexer replication queue issues to some peers",
    "search": "`comment(\"If the replication queue is full, then depending on the replication factor this can stop / slow indexing. Note this alert has been set to find many results to remove false alarms...\")` \n`comment(\"Unfortunately this setting is not tunable, at the time of writing (7.0.0) the queue size is 20. If the \\\"has room now\\\" appears shortly afterward this is not an issue.\")`\nindex=_internal `indexerhosts` \"replication queue for \" \"full\" sourcetype=splunkd (`splunkadmins_splunkd_source`)\n| rename peer AS guid \n| join guid [| rest /services/search/distributed/peers splunk_server=local | fields guid peerName]\n| bin _time span=10m \n| stats count by peerName, _time \n| where count>`splunkadmins_indexer_replication_queue_count`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. Indexer replication queue issues to some peers may prevent indexing of data and result in a large index queue",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/11 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Knowledge bundle upload stats",
    "search": "`comment(\"Query the indexing tier to determine how often they are receiving new knowledge bundles from the various search tiers. From here calculate the time period between uploads, how long it takes and how many bundles during the time period\")`\n    `comment(\"Alternatives would be to check the bundle uploads via: 'index=_internal sourcetype=splunkd source=*metrics.log group=bundles_uploads' or the bundle downloads with group=bundles_downloads, in particular the baseline_count/delta_count appear to show the different methods used but it appears to be more accurate to check the splunkd access logs on the indexing tier itself in 7.0.x. In 8.0.x the cascading bundle adds various complications, if no cascading bundle is in use you can drop the appends completely\")`\n    index=_internal `indexerhosts` sourcetype=splunkd_access source=*splunkd_access.log (/services/receivers/bundle OR /services/replication/cascading/upload/payload) method=POST \n| rex field=uri \"/services/receivers/(?P<type>[^/]+)/(?P<guid>[^/]+)\" \n| rex field=uri \"/cascading/upload/payload/(?P<planid>[^/]+)$\" \n| append \n    [ search index=_internal `indexerhosts` sourcetype=splunkd source=*metrics.log TERM(\"group=cascading\") TERM(\"name=per_peer_replication\") OR TERM(\"name=plan_metadata\") \n    | rex \"https://(?P<indexer_ip>[^:]+)\" \n    | lookup dnslookup clientip AS indexer_ip OUTPUT clienthost AS indexer \n    | eventstats values(endpoint) AS type, values(init_server) AS guid by planid \n    | stats count, values(guid) AS guid, values(type) AS type, latest(_time) AS _time by planid, indexer \n    | eval replication_mode=\"cascading\" \n    | rename indexer AS host ] \n| eval planid=upper(planid)\n| eventstats values(guid) AS guid, values(type) AS type by planid\n| sort 0 _time\n| eval type=case(type==\"delta_bundle\",\"delta-bundle\",type==\"full_bundle\",\"full-bundle\",type==\"bundle-delta\",\"delta-bundle\",type==\"bundle\",\"full-bundle\",1=1,type)\n| eval guid=if(match(guid,\"\\.\"),guid,upper(guid)) \n| streamstats global=false window=1 current=f last(_time) AS lastBundle by host, guid, type \n| eval delta = if(isnotnull(lastBundle), _time - lastBundle,null()) \n| fillnull delta value=\"N/A\" \n| eventstats count AS bundleUploadCount by host, guid, type \n| append \n    [ search index=_internal `indexerhosts` sourcetype=splunkd source=*metrics.log TERM(\"group=bundle_replication\") \n    | rex field=bundle_id \"^(?P<guid>.*?)-\\d+$\" \n    | stats earliest(_time) AS earliesttime, latest(_time) AS mostrecenttime, values(guid) AS guid, max(apply_time_msec) AS max_apply_time_msec, avg(apply_time_msec) AS avg_apply_time_msec by bundle_id, bundle_type \n    | eval deploy_time=mostrecenttime-earliesttime \n    | rename bundle_type AS type \n    | fields guid, type, deploy_time, max_apply_time_msec, avg_apply_time_msec, replication_mode ] \n| search `comment(\"The metrics.log uses delta_bundle, the other logs use bundle-delta or a variation of it\")` \n| eval type=case(type==\"delta_bundle\",\"delta-bundle\",type==\"full_bundle\",\"full-bundle\",type==\"bundle-delta\",\"delta-bundle\",type==\"bundle\",\"full-bundle\",1=1,type) \n| eval guid=if(match(guid,\"\\.\"),guid,upper(guid)) \n| stats latest(_time) AS mostRecent, max(bundleUploadCount) AS bundleUploadsInTimePeriod, max(delta) AS largestTimeDeltaInSeconds, min(delta) AS minTimeDeltaInSeconds, avg(avg_apply_time_msec) AS avgApplySeconds, max(max_apply_time_msec) AS maxApplySeconds, min(deploy_time) AS min_deploy_time, max(deploy_time) AS max_deploy_time, avg(deploy_time) AS avg_deploy_time, values(replication_mode) AS replication_mode by guid, type \n| fillnull replication_mode value=\"classic\" \n| eval mostRecent=strftime(mostRecent, \"%+\"), avgSeconds = round(avgSeconds,3), avgApplySeconds=round(avgApplySeconds/1000,3), maxApplySeconds=round(maxApplySeconds/1000,3) \n| addinfo \n| eval minsBetweenUploads=round(((info_max_time-info_min_time)/60) / bundleUploadsInTimePeriod) \n| table guid, type, mostRecent, bundleUploadsInTimePeriod, minsBetweenUploads, minTimeDeltaInSeconds, largestTimeDeltaInSeconds, min_deploy_time, max_deploy_time, avg_deploy_time, avgApplySeconds, maxApplySeconds, replication_mode",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Attempt to query the indexing tier to determine how often they are receiving new knowledge bundles from the various search tiers. From here calculate the time period between uploads, how long it takes and how many bundles during the time period",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Large multiline events using SHOULD_LINEMERGE setting",
    "search": "| tstats max(linecount) AS maxLineCount, min(_time) AS firstSeen, max(_time) AS mostRecent, values(host) AS hosts, count AS occurrenceCount where index=*, linecount>250 groupby sourcetype\n| search `comment(\"This search detects sourcetypes with greater than 250 lines which have SHOULD_LINEMERGE set to true, this might cause blocking in the indexer aggregation queue if there are a large number\")`\n`comment(\"of events with hundreds of lines or very large events such as >5000 lines of data. This alert is designed to give hints about where SHOULD_LINEMERGE=false / LINE_BREAKER=... might be more appropriate\")`\n`comment(\"Note that the REST API will return every instance of sourcetype, it's not quite as accurate a btool so this can generate false alarms if there are multiple props.conf definitions of a sourcetype\")`\n`splunkadmins_multiline_linemerge`\n| join [| rest `splunkindexerhostsvalue` /servicesNS/-/-/configs/conf-props\n| fields title SHOULD_LINEMERGE\n| search SHOULD_LINEMERGE = 1\n| dedup title | rename title AS sourcetype]\n| where maxLineCount > 260 AND occurrenceCount>30\n| eval hostList=if(mvcount(hosts)>1,mvjoin(hosts,\" OR host=\"),hosts)\n| eval hostList=\"host=\" . hostList\n| eval investigationQuery=\"index=* sourcetype=\" . sourcetype . \" \" . hostList . \" linecount>250 earliest=\" . firstSeen . \" latest=\" . mostRecent\n| sort - occurrenceCount, maxLineCount\n| table sourcetype, maxLineCount, occurrenceCount, investigationQuery",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. This alert advises that a multi-line event is appearing in Splunk that is large enough that the default SHOULD_LINEMERGE = true setting may cause blocking in the indexer aggregation queue, it's much more efficient to configure the SHOULD_LINEMERGE = false and LINE_BREAKER = ... if possible, note the TRUNCATE settings will likely need to be much larger to deal with the LINE_BREAKER change.\nPlease update the props.conf for this sourcetype to LINE_BREAKER if applicable (and the TRUNCATE setting).",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "42 7 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Maximum memory utilisation per search",
    "search": "`comment(\"As originally found on https://answers.splunk.com/answers/500973/how-to-improve-my-search-to-identify-queries-which.html / DalJeanis with minor modifications. Max memory used per search process at search head level\")`\nindex=_introspection `indexerhosts` sourcetype=splunk_resource_usage component=PerProcess data.search_props.sid=*\n| stats max(data.mem_used) AS peak_mem_usage,\n    latest(data.search_props.mode) AS mode,\n    latest(data.search_props.type) AS type,\n    latest(data.search_props.role) AS role,\n    latest(data.search_props.app) AS app,\n    latest(data.search_props.user) AS user,\n    latest(data.search_props.provenance) AS provenance,\n    latest(data.search_props.label) AS label,\n    latest(host) AS splunk_server,\n    min(_time) AS min_time,\n    max(_time) AS max_time\n    by data.search_props.sid, host\n| sort - peak_mem_usage\n| head 50\n| table provenance, peak_mem_usage, label, mode, type, role, app, user, min_time, max_time, data.search_props.sid splunk_server\n| eval min_time=strftime(min_time, \"%+\"), max_time=strftime(max_time, \"%+\")\n| rename data.search_props.sid AS sid,\n    peak_mem_usage AS \"Peak Physical Memory Usage (MB)\",\n    min_time AS \"First time seen\",\n    max_time AS \"Last time seen\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. As found on SplunkAnswers check the maximum memory usage per-search ( https://answers.splunk.com/answers/500973/how-to-improve-my-search-to-identify-queries-which.html )",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Old data appearing in Splunk indexes",
    "search": "| tstats max(_time) AS mostRecentlySeen, max(_indextime) AS mostRecentlyIndexed, min(_time) AS earliestSeen, min(_indextime) AS earliestIndexTime , count \n    where _index_earliest=`splunkadmins_olddata_lookback`, earliest=`splunkadmins_olddata_earliest`, latest=`splunkadmins_olddata_latest` \n    groupby source, sourcetype, index, host\n| search `comment(\"Find data that appears to be logged in the past, this may indicate poor timestamp parsing (or we're just ingesting really old data\")` `splunkadmins_olddata`\n| eval invesDataSource = replace(source, \"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\"), invesLatestTime=mostRecentlySeen+1, invesLatestIndexTime=mostRecentlyIndexed+1\n| eval investigationQuery=\"`comment(\\\"Narrow down to the older part of the timeline after this query runs to see the potential issue...\\\")` index=\" . index . \" source=\\\"\" . invesDataSource . \"\\\" sourcetype=\\\"\" . sourcetype . \"\\\" host=\" . host . \" earliest=\" . earliestSeen . \" latest=\" . invesLatestTime . \" _index_earliest=\" . earliestIndexTime . \" _index_latest=\" . invesLatestIndexTime . \" | eval indextime=strftime(_indextime, \\\"%+\\\")\" \n| eval mostRecentlySeen=strftime(mostRecentlySeen, \"%+\"), mostRecentlyIndexed=strftime(mostRecentlyIndexed, \"%+\")\n| sort index, host, sourcetype\n| table index, source, sourcetype, host, mostRecentlySeen, mostRecentlyIndexed, count, investigationQuery",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-10y",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. A slightly more complex alert that attempts to find recently indexed data that is been indexed with older timestamps, an attempt to find invalid date parsing for Splunk inputs",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "33 7 * * 0",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Peer will not return results due to outdated generation",
    "search": "`comment(\"If the error Peer ... will not return any results for this search, because the search head is using an outdated generation then either the peer requires a restart or there is another issue here. Assuming the issue does not resolve itself quickly...\")`\nindex=_internal sourcetype=splunkd `splunkadmins_splunkd_source` `indexerhosts` \"because the search head is using an outdated generation\"\n| eval message=coalesce(message,event_message)\n| stats count, min(_time) AS firstSeen, max(_time) AS lastSeen, values(host) AS host by message\n| eval diff=lastSeen-firstSeen\n| where diff>60\n| eval firstSeen = strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\") \n| table host, message, firstSeen, lastSeen, count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. In general this error should not appear for a long period of time, so if it does there is likely an issue. Note this can be replaced by \"AllSplunkEnterpriseLevel - Losing Contact With Master Node\"",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/15 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - RemoteSearches Indexes Stats",
    "search": "`comment(\"Attempt to determine index access via the remote_searches.log file, useful for when you cannot see the audit logs of all incoming search heads\")`\n    index=_internal sourcetype=splunkd_remote_searches source=\"/opt/splunk/var/log/splunk/remote_searches.log\" terminated `comment(\"Note that TERM(starting) has the apiStartTime, apiEndTime stats, but lacks the useful stats from a search that is complete. Also note that on indexers scan_count=events_count (in my testing). Finally the elapsedTime sometimes failed to auto-extract, perhaps due to length...\")` \n| rex \"(?s) elapsedTime=(?P<elapsedTime>[0-9\\.]+), search='(?P<search>.*?)(', savedsearch_name|\\\", drop_count=\\d+)\" \n| regex search!=\"^(pretypeahead|copybuckets)\" \n| rex \"drop_count=[0-9]+, scan_count=(?P<scan_count>[0-9]+)\" \n| rex \"total_slices=[0-9]+, considered_buckets=(?P<considered_count>[0-9]+)\" \n| rex \"(,|}\\.\\.\\.) savedsearch_name=\\\"(?P<savedsearch_name>[^\\\"]*)\\\",\" \n| rex \"terminated: search_id=(?P<search_id>[^,]+)\" \n| regex search=\"^(litsearch|mcatalog|mstats|mlitsearch|litmstats|tstats|presummarize)\" \n| rex field=search max_match=50 \"(?s)\\|?\\s*(mlitsearch)\\s+.*?\\[(?P<subsearch>.*?)\\]\\s*(\\||$)\" \n| rex field=search \"(?s)(?P<prepipe>\\s*\\|?([^\\|]+))\" \n| nomv subsearch \n| eval subsearch=if(isnull(subsearch),\"\",subsearch) \n| eval prepipe = prepipe . \" \" . subsearch \n| eval search=prepipe \n| search `comment(\"The (index=* OR index=_*) index=<specific index> is a common use case for enterprise security, also some individuals like doing a similar trick so remove the index=*... as this is not a wildcard index search\")` \n| rex field=search \"(?P<esstylewildcard>\\(\\s*index=\\*\\s+OR\\s+index=_\\*\\s*\\))\" \n| rex mode=sed field=search \"s/search index=\\s*\\S+\\s+index\\s*=/search index=/\" \n| search `comment(\"Extract out index= or index IN (a,b,c) but avoid NOT index in (...) and NOT index=... and also NOT (...anything) statements\")` \n| rex field=search \"(?s)(NOT\\s+index(\\s*=\\s*|::)[^ ]+)|(NOT\\s+\\([^\\)]+\\))|(index(\\s*=\\s*|::)(?P<indexregex>[\\*A-Za-z0-9-_]+))\" max_match=50 \n| rex field=search \"(?s)(NOT\\s+index(\\s*=\\s*|::)[^ ]+)|(NOT\\s+\\([^\\)]+\\))|(index(\\s*=\\s*|::)\\\"?(?P<indexregex2>[\\*A-Za-z0-9-_]+))\" max_match=50 \n| rex field=search \"\\s+(?P<skipping>\\.\\.\\.\\{skipping \\d+ bytes\\}\\.\\.\\.)\" \n| search `comment(\"If skipping is in the logs as in index=abc- ...{skipping 46464 bytes}..., then drop the last index found in the regex as it is likely invalid\")` \n| eval indexregex=if(isnotnull(skipping),mvindex(indexregex,0,-2),indexregex) \n| eval indexregex2=if(isnotnull(skipping),mvindex(indexregex2,0,-2),indexregex2) \n| eval indexes=mvappend(indexregex,indexregex2) \n| eval indexes=if(isnotnull(esstylewildcard),mvfilter(NOT match(indexes,\"^_?\\*$\")),indexes) \n| eval multi=if(mvcount(mvdedup(indexes))>1,\"true\",\"false\") \n| rex field=search_id \"^remote_(?P<sid>.*)\" \n| rex \"search_id=[^,]+,\\s+server=(?P<server>[^,]+)\" \n| eval server_with_underscore = server. \"_\" \n| eval sid=replace(sid, server_with_underscore, \"\") \n| eval search_head=server \n| `search_type_from_sid(sid)` \n| `base64decode(base64username)` \n| eval username3=\"unknown\" \n| eval user=coalesce(username, base64username, username3) \n| rex field=search \"^(?P<presummarize>presummarize)\\s+\" \n| eval type=if(isnotnull(presummarize),\"acceleration\",type) \n| eval search_head_cluster=`search_head_cluster` \n| eval indexer_cluster=`indexer_cluster_name(host)` \n| search `comment(\"If you use the TERM(starting) you get the apiStartTime/apiEndTime, or you could join them in stats or similar...however this works to obtain which indexes are used. Note that you would need to build something similar to 'SearchHeadLevel - Search Queries summary non-exact match' to be able to translate the wildcards into something more useful, but there would be a lot of guesswork involved if you do not have usernames+server names+roles...(which is why audit logs work better for this)\")`\n| rex \"search_rawdata_bucketcache_error=[^,]+, search_rawdata_bucketcache_miss=(?P<cache_rawdata_miss>[^,]+), search_index_bucketcache_error=[^,]+, search_index_bucketcache_hit=(?P<cache_index_hit>[^,]+), search_index_bucketcache_miss=(?P<cache_index_miss>[^,]+), search_rawdata_bucketcache_hit=(?P<cache_rawdata_hit>[^,]+), search_rawdata_bucketcache_miss_wait=(?P<cache_rawdata_miss_wait>[^,]+), search_index_bucketcache_miss_wait=(?P<cache_index_miss_wait>[^,]+)\" \n| `base64decode(base64appname)` \n| eval app3=\"N/A\" \n| eval app=coalesce(app,base64appname,app3) \n| stats dc(search_id) AS count, avg(elapsedTime) AS avg_total_run_time, max(elapsedTime) AS max_total_run_time, median(elapsedTime) AS median_total_run_time, avg(scan_count) AS avg_scan_count, max(scan_count) AS max_scan_count, min(scan_count) AS min_scan_count, median(scan_count) AS median_scan_count, sum(cache_rawdata_miss) AS cache_rawdata_miss, sum(cache_index_hit) AS cache_index_hit, sum(cache_index_miss) AS cache_index_miss, sum(cache_rawdata_hit) AS cache_rawdata_hit, sum(cache_rawdata_miss_wait) AS cache_rawdata_miss_wait, sum(cache_index_miss_wait) AS cache_index_miss_wait by user, search_head_cluster, indexes, indexer_cluster, type, multi, app \n| eval indexes=lower(indexes) \n| regex indexes!=\"\\*\" \n| stats sum(count) AS count, avg(avg_total_run_time) AS avg_total_run_time, max(max_total_run_time) AS max_total_run_time, median(median_total_run_time) AS median_total_run_time, avg(avg_scan_count) AS avg_scan_count, max(max_scan_count) AS max_scan_count, min(min_scan_count) AS min_scan_count, median(median_scan_count) AS median_scan_count, sum(cache_rawdata_miss) AS cache_rawdata_miss, sum(cache_index_hit) AS cache_index_hit, sum(cache_index_miss) AS cache_index_miss, sum(cache_rawdata_hit) AS cache_rawdata_hit, sum(cache_rawdata_miss_wait) AS cache_rawdata_miss_wait, sum(cache_index_miss_wait) AS cache_index_miss_wait by indexes, indexer_cluster, user, search_head_cluster, type, multi, app \n| eval prefix=\"platform_stats.remote_searches.per_index.exact.\" \n| addinfo \n| rename info_max_time AS _time \n| fields - info_* \n| eval short=\"False\" \n| search `comment(\"| mcollect index=a_metrics_index split=true prefix_field=prefix search_head_cluster, indexer_cluster, type, user, indexes, multi, app, short. If using Splunk 8.0.x delete the below lines and use mcollect, if not you can use summary indexing with metrics\")` \n| rename * AS platform_stats.remote_searches.per_index.exact.* \n| rename platform_stats.remote_searches.per_index.exact.search_head_cluster AS search_head_cluster platform_stats.remote_searches.per_index.exact.indexer_cluster AS indexer_cluster, platform_stats.remote_searches.per_index.exact.type AS type, platform_stats.remote_searches.per_index.exact.user AS user, platform_stats.remote_searches.per_index.exact.indexes AS indexes, platform_stats.remote_searches.per_index.exact.multi AS multi, platform_stats.remote_searches.per_index.exact.short AS short, platform_stats.remote_searches.per_index.exact.app AS app \n| fields - prefix",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This is an example of using the remote_searches.log on the indexers to determine which indexes are in use",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "38 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - RemoteSearches Indexes Stats Wilcard",
    "search": "`comment(\"Attempt to determine index access via the remote_searches.log file, useful for when you cannot see the audit logs of all incoming search heads. This version looks for wildcards and it is not expected to be super-accurate, as while we can determine the incoming server, and sometimes the incoming user from the search id we cannot accurately determine the roles of the user without building yet more lookups and complexity. Therefore this search exists only to roughly summarize if an index was ever accessed via wildcards or not at the indexing tier\")` \n    index=_internal sourcetype=splunkd_remote_searches source=\"/opt/splunk/var/log/splunk/remote_searches.log\" terminated `comment(\"Note that TERM(starting) has the apiStartTime, apiEndTime stats, but lacks the useful stats from a search that is complete. Also note that on indexers scan_count=events_count (in my testing). Finally the elapsedTime sometimes failed to auto-extract, perhaps due to length...\")` \n| rex \"(?s) elapsedTime=(?P<elapsedTime>[0-9\\.]+), search='(?P<search>.*?)(', savedsearch_name|\\\", drop_count=\\d+)\" \n| regex search!=\"^(pretypeahead|copybuckets)\" \n| rex \"drop_count=[0-9]+, scan_count=(?P<scan_count>[0-9]+)\" \n| rex \"total_slices=[0-9]+, considered_buckets=(?P<considered_count>[0-9]+)\" \n| rex \"(,|}\\.\\.\\.) savedsearch_name=\\\"(?P<savedsearch_name>[^\\\"]*)\\\",\" \n| rex \"terminated: search_id=(?P<search_id>[^,]+)\" \n| regex search=\"^(litsearch|mcatalog|mstats|mlitsearch|litmstats|tstats|presummarize)\" \n| rex field=search max_match=50 \"(?s)\\|?\\s*(mlitsearch)\\s+.*?\\[(?P<subsearch>.*?)\\]\\s*(\\||$)\" \n| rex field=search \"(?s)(?P<prepipe>\\s*\\|?([^\\|]+))\" \n| nomv subsearch \n| eval subsearch=if(isnull(subsearch),\"\",subsearch) \n| eval prepipe = prepipe . \" \" . subsearch \n| eval search=prepipe \n| rex mode=sed field=search \"s/search index=\\s*\\S+\\s+index\\s*=/search index=/g\" \n| search `comment(\"Extract out index= or index IN (a,b,c) but avoid NOT index in (...) and NOT index=... and also NOT (...anything) statements\")` \n| search `comment(\"The (index=* OR index=_*) index=<specific index> is a common use case for enterprise security, also some individuals like doing a similar trick so remove the index=*... as this is not a wildcard index search\")` \n| rex field=search \"(?P<esstylewildcard>\\(\\s*index=_\\*\\s+OR\\s+index=\\*\\s+\\))\" \n| search `comment(\"Extract out index= or index IN (a,b,c) but avoid NOT index in (...) and NOT index=... and also NOT (...anything) statements\")` \n| rex field=search \"(?s)(NOT\\s+index(\\s*=\\s*|::)[^ ]+)|(NOT\\s+\\([^\\)]+\\))|(index(\\s*=\\s*|::)(?P<indexregex>[\\*A-Za-z0-9-_]+))\" max_match=50 \n| rex field=search \"(?s)(NOT\\s+index(\\s*=\\s*|::)[^ ]+)|(NOT\\s+\\([^\\)]+\\))|(index(\\s*=\\s*|::)\\\"?(?P<indexregex2>[\\*A-Za-z0-9-_]+))\" max_match=50 \n| rex field=search \"\\s+(?P<skipping>\\.\\.\\.\\{skipping \\d+ bytes\\}\\.\\.\\.)\" \n| search `comment(\"If skipping is in the logs as in index=abc- ...{skipping 46464 bytes}..., then drop the last index found in the regex as it is likely invalid\")` \n| eval indexregex=if(isnotnull(skipping),mvindex(indexregex,0,-2),indexregex) \n| eval indexregex2=if(isnotnull(skipping),mvindex(indexregex2,0,-2),indexregex2) \n| eval indexes=mvappend(indexregex,indexregex2) \n| eval indexes=if(isnotnull(esstylewildcard),mvfilter(NOT match(indexes,\"^_?\\*$\")),indexes) \n| eval multi=if(mvcount(mvdedup(indexes))>1,\"true\",\"false\") \n| eval short=mvmap(indexes,if(len(indexes)<=3,\"True\",null())) \n| eval short=if(isnull(short),\"False\",\"True\") \n| rex field=search_id \"^remote_(?P<sid>.*)\" \n| rex \"search_id=[^,]+,\\s+server=(?P<server>[^,]+)\" \n| eval server_with_underscore = server. \"_\" \n| eval sid=replace(sid, server_with_underscore, \"\") \n| eval search_head=server \n| `search_type_from_sid(sid)` \n| `base64decode(base64username)` \n| eval username3=\"unknown\" \n| eval user=coalesce(username, base64username, username3) \n| rex field=search \"^(?P<presummarize>presummarize)\\s+\" \n| eval type=if(isnotnull(presummarize),\"acceleration\",type) \n| eval search_head_cluster=`search_head_cluster` \n| eval indexer_cluster=`indexer_cluster_name(host)` \n| search `comment(\"If you use the TERM(starting) you get the apiStartTime/apiEndTime, or you could join them in stats or similar...however this works to obtain which indexes are used. Note that you would need to build something similar to 'SearchHeadLevel - Search Queries summary non-exact match' to be able to translate the wildcards into something more useful, but there would be a lot of guesswork involved if you do not have usernames+server names+roles...(which is why audit logs work better for this)\")` \n| rex \"search_rawdata_bucketcache_error=[^,]+, search_rawdata_bucketcache_miss=(?P<cache_rawdata_miss>[^,]+), search_index_bucketcache_error=[^,]+, search_index_bucketcache_hit=(?P<cache_index_hit>[^,]+), search_index_bucketcache_miss=(?P<cache_index_miss>[^,]+), search_rawdata_bucketcache_hit=(?P<cache_rawdata_hit>[^,]+), search_rawdata_bucketcache_miss_wait=(?P<cache_rawdata_miss_wait>[^,]+), search_index_bucketcache_miss_wait=(?P<cache_index_miss_wait>[^,]+)\" \n| `base64decode(base64appname)` \n| eval app3=\"N/A\"  \n| eval app=coalesce(app,base64appname,app3) \n| stats dc(search_id) AS count, avg(elapsedTime) AS avg_total_run_time, max(elapsedTime) AS max_total_run_time, median(elapsedTime) AS median_total_run_time, avg(scan_count) AS avg_scan_count, max(scan_count) AS max_scan_count, min(scan_count) AS min_scan_count, median(scan_count) AS median_scan_count, sum(cache_rawdata_miss) AS cache_rawdata_miss, sum(cache_index_hit) AS cache_index_hit, sum(cache_index_miss) AS cache_index_miss, sum(cache_rawdata_hit) AS cache_rawdata_hit, sum(cache_rawdata_miss_wait) AS cache_rawdata_miss_wait, sum(cache_index_miss_wait) AS cache_index_miss_wait by user, search_head_cluster, indexes, indexer_cluster, type, multi, short, app \n| regex indexes=\"\\*\" \n| eval indexes=lower(indexes) \n| lookup splunkadmins_indexlist_by_cluster indexer_cluster \n| makemv index tokenizer=(\\S+) \n| streamfilterwildcard pattern=indexes fieldname=indexes index \n| makemv indexes tokenizer=(\\S+) \n| stats sum(count) AS count, avg(avg_total_run_time) AS avg_total_run_time, max(max_total_run_time) AS max_total_run_time, median(median_total_run_time) AS median_total_run_time, avg(avg_scan_count) AS avg_scan_count, max(max_scan_count) AS max_scan_count, min(min_scan_count) AS min_scan_count, median(median_scan_count) AS median_scan_count, sum(cache_rawdata_miss) AS cache_rawdata_miss, sum(cache_index_hit) AS cache_index_hit, sum(cache_index_miss) AS cache_index_miss, sum(cache_rawdata_hit) AS cache_rawdata_hit, sum(cache_rawdata_miss_wait) AS cache_rawdata_miss_wait, sum(cache_index_miss_wait) AS cache_index_miss_wait by indexes, indexer_cluster, user, search_head_cluster, type, multi, short, app \n| eval prefix=\"platform_stats.remote_searches.per_index.nonexact.\" \n| addinfo \n| rename info_max_time AS _time \n| fields - info_* \n| search `comment(\"| mcollect index=a_metrics_index split=true prefix_field=prefix search_head_cluster, indexer_cluster, type, user, indexes, multi, short, app. Below is useful if you instead use summary indexing for metrics in newer Splunk versions...in Splunk 8.0.x delete the below lines\")` \n| rename * AS platform_stats.remote_searches.per_index.nonexact.* \n| rename platform_stats.remote_searches.per_index.nonexact.search_head_cluster AS search_head_cluster platform_stats.remote_searches.per_index.nonexact.indexer_cluster AS indexer_cluster, platform_stats.remote_searches.per_index.nonexact.type AS type, platform_stats.remote_searches.per_index.nonexact.user AS user, platform_stats.remote_searches.per_index.nonexact.indexes AS indexes, platform_stats.remote_searches.per_index.nonexact.multi AS multi, platform_stats.remote_searches.per_index.nonexact.short AS short, platform_stats.remote_searches.per_index.nonexact.app AS app \n| fields - prefix",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This is an example of using the remote_searches.log on the indexers to determine which indexes are in use, this version matches wildcards and will be inaccurate due to the lack of role information per user (the audit  logs SearchHeadLevel - Search Queries summary non-exact match will work better for that purpose. This example search was to check if an index is ever accessed via wildcards. Note this report requires SearchHeadLevel - Index list by cluster report, to run / output a lookup. Note that this search utilises the streamfilterwildcard custom search command included in the TA-Alerts for SplunkAdmins application on SplunkBase (or github)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "38 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - RemoteSearches find all time searches",
    "search": "index=_internal `splunkadmins_indexer_remotesearches_alltime` source=*remote_searches.log sourcetype=splunkd_remote_searches StreamedSearch TERM(starting:) NOT TERM(terminated:)  \"search='litsearch\" OR \"search='mlitsearch\" OR \"search='mcatalog\" OR \"search='mstats\" OR \"search='mlitsearch\" OR \"search='litmstats\" OR \"search='tstats\" OR \"search='presummarize\" NOT \"litsearch index=mobieos | fields\" \n| regex search!=\"^presummarize (tstats=t )?maintain=\\\"\" \n| eval start_time=strptime(apiStartTime, \"%a %b %d %H:%M:%S %Y\") \n| eval start_time=if(apiStartTime=\"ZERO_TIME\",\"ZERO_TIME\",start_time) \n| eval now=now() \n| where start_time<(now-31622400) OR start_time=\"ZERO_TIME\" \n| rex field=search \"earliest=(?P<earliest_time_field>\\S+)\" \n| eval earliest_time2=if(isnotnull(earliest_time_field),earliest_time_field,\"-1s\") \n| eval start_time_relative=relative_time(now(), earliest_time2) \n| eval diff=now() - start_time_relative \n| where diff>31622400 OR isnull(earliest_time_field) \n| eval start_time=if(isnotnull(earliest_time_field),strftime(start_time_relative, \"%a %b %d %H:%M:%S %Y\"),apiStartTime) \n| stats latest(_time) AS _time, values(search) AS search, values(savedsearch_name) AS savedsearch_name, values(start_time) AS start_time values(apiStartTime) AS apiStartTime, values(apiEndTime) AS apiEndTime by search_id, server \n| table _time, server, search_id, apiStartTime, start_time, savedsearch_name, search, apiEndTime",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. The remote_searches.log is showing that there may be an all time search running on the indexing tier, this may or may not be an issue. Note this can also be detected via the audit.log should you have access to the audit.log of all search heads. This log does miss the scenario where _index_earliest is passed via API as per the comments on https://ideas.splunk.com/ideas/E-I-49 . Note that you probably want to run this on a single indexer...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - RemoteSearches find datamodel acceleration with wildcards",
    "search": "index=_internal `indexerhosts` source=*remote_searches.log sourcetype=splunkd_remote_searches source=\"/opt/splunk/var/log/splunk/remote_searches.log\" StreamedSearch TERM(starting:) NOT TERM(terminated:) search_id=*SummaryDirector* \n| regex \"index=\\\"?\\*\" \n| rex mode=sed field=search \"s/\\(\\s+index=\\*\\s+OR\\s+index=_\\*\\s+\\).*?index(=|\\s+IN\\s+\\()/ESstylewildcard index=/g\" \n| regex search=\"index=\\\"?\\*\" \n| rex \"search_id=[^,]+,\\s+server=(?P<search_head>[^,]+)\" \n| eval search_head_cluster=`search_head_cluster` \n| eval indexer_cluster=`indexer_cluster_name(host)` \n| stats count, values(search) AS search by search_head_cluster, indexer_cluster",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. The remote_searches.log is showing that an accelerated datamodel appears to be using an index=* wildcard, when using SmartStore this can cause serious issues  with object store request numbers https://ideas.splunk.com/ideas/EID-I-677",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Report on bucket corruption",
    "search": "`comment(\"For the alert version of this report refer to IndexerLevel - Unclean Shutdown - Fsck, you may also wish to try IndexerLevel - Corrupt buckets via DBInspect\")`\n`comment(\"Attempt to find bucket corruption errors in the splunkd logs. this can also be found at search head level via the info.csv (not indexed by default). In a clustered environment a message such as \\\"06-12-2018 07:31:47.160 +0000 INFO  ProcessTracker - (child_407__Fsck)  Fsck - (entire bucket) Rebuild for bucket='/opt/splunk/var/lib/splunk/indexname/db/db_1528466340_1520517600_38_A25ECA32-B33E-4469-8C76-22190FDCC8CB' took 86.26 seconds\\\" may appear once the auto-repair has occurred. Finally it may appear if log_search_messages is set in limits.conf, the sourcetype will be splunk_search_messages\")`\nindex=_internal `indexerhosts` sourcetype=splunkd `splunkadmins_splunkd_source` IndexerService OR HotBucketRoller \"corrupt\" newly `comment(\"newly appears to show corruption, previously may be the term for when it is fixed...\")`\n| stats values(Bucket) AS bucketList by idx \n| eval bucketCount=mvcount(bucketList)\n| addcoltotals",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Refer to IndexerLevel - Unclean Shutdown - Fsck for an alert for this issue, this just lists out all corrupt buckets.Bucket corruption is rare and in a clustered environment this should self-repair via the cluster master fixup list over time. If non-clustered refer to the documentation for splunk fsck in a non-clustered environment, you may also wish to try IndexerLevel - Corrupt buckets via DBInspect",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Rolling Hot Bucket Failure",
    "search": "`comment(\"If this alert fires, we are potentially out of disk in the hot section or something else has gone wrong\")` \nindex=_internal `indexerhosts` \"Not rolling hot buckets on further errors to this target\" sourcetype=splunkd (`splunkadmins_splunkd_source`)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Hot buckets are throwing errors while trying to roll",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0,15,30,45 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - S2SFileReceiver Error",
    "search": "`comment(\"An attempt to detect excessive numbers of S2SFileReceiver / TcpInputProc failures on the indexing tier, these may indicate an issue. We are only looking for errors about replication data\")`\n`comment(\"An indexer peer that was constantly logging \\\"S2SFileReceiver...type=data_model already exists!\\\" / \\\"S2SFileReceiver...type=report_acceleration already exists!\\\" has been fixed by restart (once so far)\")`\nindex=_internal sourcetype=splunkd `indexerhosts` sourcetype=splunkd `splunkadmins_splunkd_source` \"ERROR TcpInputProc - event=replicationData\" OR \"ERROR S2SFileReceiver - error alerting slave about\" OR \"ERROR S2SFileReceiver - error adding new summary replica to slave\"\n| rex \"(?P<error>ERROR [^-]+- [^=]+=)(?P<postEquals>[^ ]+) (?P<postEquals2>[^=]+=[^ ]+)\"\n| rex field=err \"(?P<type>type=.*)\"\n| eval error=if(postEquals==\"onFileAborted\" OR postEquals==\"replicationData\",error . \" \" . postEquals . \" \" . postEquals2,error . \" \" . type)\n| stats count, max(_time) AS mostRecent by host, error\n| search `comment(\"Allow exclusion via macro\")` `splunkadmins_s2sfilereceiver`\n| eval mostRecent=strftime(mostRecent, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-3h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. One or more indexing peers are having issues with receiving file replications and may require investigation",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "36 */3 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Search Failures",
    "search": "`comment(\"Attempt to detect search failures of interest, such as Search Factory: Unknown search command 'base64' so they can be fixed before it becomes an issue for multiple users. The search is generic to attempt to detect any new errors. Note that if you are running a modern Splunk version you may wish to use \\\"SearchHeadLevel - Search Messages admins only\\\" and \\\"SearchHeadLevel - Search Messages user level\\\" instead as they detect the issues at SH level\")`\nindex=_internal `indexerhosts` sourcetype=splunkd_remote_searches \"ERROR\" OR \"WARN\" NOT \"remote_metrics.json does not exist before reading\" NOT \", Broken pipe\" NOT \", Connection closed by peer\" NOT \", Connection reset by peer\" NOT \"is already running\" NOT \"Local side shutting down\" NOT (\"ERROR StreamedSearch\" \"Success\") \n| regex \"\\+0000 (ERROR|WARN)\" \n| search `comment(\"Allow exclusions via macro...\")` `splunkadmins_searchfailures`\n| cluster field=sid showcount=true \n| table host, cluster_count, _raw \n| eval indexer_cluster=`indexer_cluster_name(host)`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. One or more search jobs are failing to run for some reason, this may require investigation. The only issue so far has been around search factory/unknown search command but this search is generic just in case a new issue appears. Note that if you are running a modern Splunk version you may wish to use \"SearchHeadLevel - Search Messages admins only\" and \"SearchHeadLevel - Search Messages user level\" instead as they will detect issues at SH level",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "48 6 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Slow peer from remote searches",
    "search": "`comment(\"This warning when occurring repetitively tends to indicate some kind of issue that will require the file to be manually removed. For example a zero sized metadata file that cannot be reaped by the dispatch reaper\")` \nindex=_internal `indexerhosts` source=*remote_searches.log terminated \n| regex search!=\"^(pretypeahead|copybuckets)\" \n| rex \"(?s) elapsedTime=(?P<elapsedTime>[0-9\\.]+), search='(?P<search>.*?)(', savedsearch_name|\\\", drop_count=\\d+)\" \n| rex \"terminated: search_id=(?P<search_id>[^,]+)\" \n| regex search=\"^(litsearch|mcatalog|mstats|mlitsearch|litmstats|tstats|presummarize)\" \n| regex search_id=\"^remote\" \n| stats last(_time) AS _time, avg(elapsedTime) AS avgelapsedtime, max(elapsedTime) AS maxelapsedtime by search_id, host \n| eventstats max(maxelapsedtime) AS slowest, avg(avgelapsedtime) AS average by search_id \n| eval slow=average+`splunkadmins_slowpeer_time`, comment=\"Tested stddev() but what if the search is smaller than normal and some indexers take 5X longer, if the search was 3 seconds who cares\" \n| where maxelapsedtime>slow AND maxelapsedtime==slowest \n| bin _time span=5m \n| stats count by host, _time \n| where count>`splunkadmins_slowpeer_threshold`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-30m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. This alert is an example of how to find if a single (or a few) search/indexing peers are returning results more slowly than other peers resulting in slow searches",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/30 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - SmartStore - Bucket cache errors audit logs",
    "search": "index=_audit \"info=completed\" search_id!=\"'SummaryDirector_*\" search_id!=\"'rsa_*\" invocations_command_search_index_bucketcache_error>0 OR invocations_command_search_rawdata_bucketcache_error>0 \n| eval invocations_command_search_hit=invocations_command_search_index_bucketcache_hit + invocations_command_search_rawdata_bucketcache_hit, invocations_command_search_miss = invocations_command_search_index_bucketcache_miss + invocations_command_search_rawdata_bucketcache_miss \n| table _time, invocations_command_search_index_bucketcache_error, invocations_command_search_rawdata_bucketcache_error, total_run_time, user, has_error_msg, invocations_command_search_hit, invocations_command_search_miss, duration_command_*_miss, search_id",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. The audit logs from the search tier are advising of 1 or more bucket cache errors",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "18 */4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - SmartStore cache misses - remote_searches",
    "search": "index=_internal `indexerhosts` sourcetype=splunkd_remote_searches StreamedSearch Streamed search connection terminated search_id=* `indexerhosts` rawdata_bucketcache_miss>0 OR index_bucketcache_miss>0 `comment(\"based on a search from Richard Morgan's dashboard, https://github.com/silkyrich/cluster_health_tools/blob/master/default/data/ui/views/debug_cache_manager_misses.xml\")` \n| rex field=_raw \"search_rawdata_bucketcache_error=(?<rawdata_bucketcache_error>[\\d.]+)\" \n| rex field=_raw \"search_rawdata_bucketcache_miss=(?<rawdata_bucketcache_miss>[\\d.]+)\" \n| rex field=_raw \"search_index_bucketcache_error=(?<index_bucketcache_error>[\\d.]+)\" \n| rex field=_raw \"search_index_bucketcache_hit=(?<index_bucketcache_hit>[\\d.]+)\" \n| rex field=_raw \"search_index_bucketcache_miss=(?<index_bucketcache_miss>[\\d.]+)\" \n| rex field=_raw \"search_rawdata_bucketcache_hit=(?<rawdata_bucketcache_hit>[\\d.]+)\" \n| rex field=_raw \"search_rawdata_bucketcache_miss_wait=(?<rawdata_bucketcache_miss_wait>[\\d.]+)\" \n| rex field=_raw \"search_index_bucketcache_miss_wait=(?<index_bucketcache_miss_wait>[\\d.]+)\" \n| rex field=_raw \"drop_count=(?<drop_count>[\\d.]+)\" \n| rex field=_raw \"scan_count=(?<scan_count>[\\d.]+)\" \n| rex field=_raw \"eliminated_buckets=(?<eliminated_buckets>[\\d.]+)\" \n| rex field=_raw \"considered_events=(?<considered_events>[\\d.]+)\" \n| rex field=_raw \"decompressed_slices=(?<decompressed_slices>[\\d.]+)\" \n| rex field=_raw \"events_count=(?<events_count>[\\d.]+)\" \n| rex field=_raw \"total_slices=(?<total_slices>[\\d.]+)\" \n| rex field=_raw \"considered_buckets=(?<considered_buckets>[\\d.]+)\" \n| stats \n    sum(rawdata_bucketcache_error) as search_rawdata_bucketcache_error_sum\n    sum(rawdata_bucketcache_miss) as search_rawdata_bucketcache_miss_sum\n    sum(index_bucketcache_error) as search_index_bucketcache_error_sum\n    sum(index_bucketcache_hit) as search_index_bucketcache_hit_sum\n    sum(index_bucketcache_miss) as search_index_bucketcache_miss_sum\n    sum(rawdata_bucketcache_hit) as search_rawdata_bucketcache_hit_sum\n    sum(rawdata_bucketcache_miss_wait) as search_rawdata_bucketcache_miss_wait_sum.\n    sum(index_bucketcache_miss_wait) as search_index_bucketcache_miss_wait_sum.\n    min(_time) as time_min \n    max(_time) as time_max\n    sum(drop_count) as drop_count_sum\n    sum(scan_count) as scan_count_sum14746\n    sum(eliminated_buckets) as eliminated_buckets_sum\n    sum(considered_events) as considered_events_sum\n    sum(decompressed_slices) as decompressed_slices_sum\n    sum(events_count) as events_count_sum14746\n    sum(total_slices) as total_slices_sum\n    sum(considered_buckets) as considered_buckets_sum, values(search) AS search\n    by search_id server \n| search search_id=remote_* \n| eval cache_misses=search_rawdata_bucketcache_miss_sum + search_index_bucketcache_miss_sum \n| sort - search_rawdata_bucketcache_miss_sum \n| table search_id cache_misses * \n| sort 0 - cache_misses",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is designed to find the number of cache misses at the indexing tier, based on a search from Richard Morgan's dashboard, https://github.com/silkyrich/cluster_health_tools/blob/master/default/data/ui/views/debug_cache_manager_misses.xml",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "33 */4 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - These Indexes Are Approaching The warmDBCount limit",
    "search": "| dbinspect index=* state=warm\n| search `comment(\"Once the warmdb bucket count is reached then the buckets are moved to cold, this may be an issue if incorrectly configured, this alert warns in advance if we get close to the limit\")`\n`comment(\"This might be a bug in 6.5.2 but the buckets are printed twice by dbinspect in some cases...\")` `splunkadmins_warmdbcount`\n| dedup bucketId, splunk_server\n| stats count AS theCount by index, splunk_server\n| stats avg(theCount) AS averageCount, max(theCount) AS maxCount, min(theCount) AS minCount, values(splunk_server) by index\n| eval averageCount = round(averageCount)\n| join index [| rest /services/data/indexes \n              | dedup title \n              | rename title AS index \n              | table index, maxWarmDBCount]\n| eval percUsed = (100/maxWarmDBCount)*averageCount\n| where percUsed > `splunkadmins_warmdbcount_perc`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-90d@d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Buckets are either now rolling or will roll to cold due to the bucket count limit in warm been reached, this may need ajustment",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "48 6 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Time format has changed multiple log types in one sourcetype",
    "search": "`comment(\"This search detects when the time format has changed within the files 1 or more times, the time format per sourcetype should be consistent\")`\nindex=_internal \"DateParserVerbose - Accepted time format has changed\" sourcetype=splunkd (`splunkadmins_splunkd_source`) (`indexerhosts`) OR (`heavyforwarderhosts`) `splunkadmins_timeformat_change`\n| rex \"source(?:=|::)(?<source>[^\\|]+)\\|host(?:=|::)(?<host>[^\\|]+)\\|(?<sourcetype>[^\\|]+)\"\n| eval message=coalesce(message,event_message)\n| stats count, min(_time) AS firstSeen, max(_time) AS lastSeen by host, source, sourcetype, message\n| eval invesMaxTime=if(firstSeen=lastSeen,lastSeen+1,lastSeen)\n| eval invesDataSource = replace(source, \"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\")\n| eval potentialInvestigationQuery=\"`comment(\\\"If no results are found, prepend the earliest=/latest= with _index_ (eg _index_earliest=...) and expand the timeframe searched over, as the parsed timestamps from the data does not have to exactly match the time the warnings appeared...\\\")` sourcetype=\\\"\" . sourcetype . \"\\\" source=\\\"\" . invesDataSource . \"\\\" host=\" . host . \" earliest=\" . firstSeen . \" latest=\" . invesMaxTime . \" | eval start=substr(_raw, 0, 30) | cluster field=start\"\n| eval firstSeen=strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\")\n| fields - invesMaxTime, invesDataSource\n| sort - count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. A changing time format is likely due to multiple log types using the same sourcetype or a date time parsing issue",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 2 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Timestamp parsing issues combined alert",
    "search": "`comment(\"As found on https://runals.blogspot.com/2014/05/splunk-dateparserverbose-logs-part-2.html with minor modifications. Further queries available in the app https://splunkbase.splunk.com/app/1848/\")`\nindex=_internal DateParserVerbose `heavyforwarderhosts` OR `indexerhosts` sourcetype=splunkd source=*splunkd.log\n| rex \"source(?:=|::)(?<Source>[^\\|]+)\\|host(?:=|::)(?<Host>[^\\|]+)\\|(?<Sourcetype>[^\\|]+)\"\n| rex \"(?<msgs_suppressed>\\d+) similar messages suppressed.\"\n| eval Issue = case(like(_raw, \"%too far away from the previous event's time%\"), \"Variability in date/event timestamp\", like(_raw, \"%suspiciously far away from the previous event's time%\"), \"Variability in date/event timestamp\", like(_raw, \"%outside of the acceptable time window%\"), \"Timestamp is too far outside acceptable time window\", like(_raw, \"%Failed to parse timestamp%\"), \"Reverting to last known good timestamp\", like(_raw, \"%Accepted time format has changed%\"), \"Attempting to learn new timestamp format\", like(_raw, \"%The same timestamp has been used%\"), \"More than 100k+ events have the same timestamp\", 1=1, \"fixme\")\n| stats count sum(msgs_suppressed) as \"Duplicate Messages Suppressed\" by Sourcetype Issue Host Source\n| stats sum(count) as count dc(Host) as Host_count, dc(Source) as Sources, sum(\"Duplicate Messages Suppressed\") as \"Duplicate Messages Suppressed\", values(Host) AS Hosts by Sourcetype Issue \n| eval \"Total Count\"='Duplicate Messages Suppressed' + count\n| stats sum(\"Total Count\") as \"Total Count\", list(Issue) as Issues, values(Hosts) as Hosts, list(Host_count) AS Host_count list(Sources) as Sources, list(\"Duplicate Messages Suppressed\") as \"Duplicate Messages Suppressed\" by Sourcetype \n| sort - \"Total Count\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Find timestamp parsing issues and provide a report on the issues. Also refer to Mark Runal's blog for the original query or his app via https://splunkbase.splunk.com/app/1848/",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "44 4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Too many events with the same timestamp",
    "search": "`comment(\"Too many events with the same timestamp have been found. This may be a sign of poor quality data, or a problematic log file\")`\nindex=_internal \"Too many events\" (`indexerhosts`) OR (`heavyforwarderhosts`) `splunkadmins_splunkd_source` `splunkadmins_toomany_sametimestamp`\n| cluster showcount=true \n| rex \"Too many events \\((?P<number>[0-9]+.)\"\n| rename data_host AS host, data_sourcetype AS sourcetype, data_source AS source\n| eval invesDataSource = replace(source, \"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\"), invesStartTime=floor(_time)\n| eval investigationQuery=\"`comment(\\\"You will need to set the time settings manually as the log does not provide the parsed time, only the indexed time the issue occurred at...\\\")` index=* host=\" . host . \" sourcetype=\\\"\" . sourcetype . \"\\\" source=\\\"\" . invesDataSource . \"\\\" _index_earliest=\" . invesStartTime\n| eval message=coalesce(message,event_message)\n| table host, sourcetype, source, number, cluster_count, message, _time, investigationQuery",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Find excessive numbers of events with the same timestamp so they can be reviewed to see if the data is valid or not",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "44 4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Unclean Shutdown - Fsck",
    "search": "`comment(\"Attempt to detect if an indexer crash resulted in corrupt buckets, if so alert the admin so they are aware...\")`\n`comment(\"The indexer is likely going to print the line \\\"WARN  IndexerService - Indexer was started dirty: splunkd startup may take longer than usual; searches may not be accurate until background fsck completes.\\\", however we also want to know if buckets were corrupted. In a clustered environment the corrupt buckets should be added to the cluster master fixup list and repaired online, if a non-clustered environment refer to the Splunk fsck documentation. Note that I'm unable to find a log message to advise when the OnlineFsck completes in splunkd.log. You may also wish to refer to alert IndexerLevel - Corrupt buckets via DBInspect\")`\n`comment(\"FYI fixup lines in the splunkd log file may look like \\\"06-12-2018 07:31:47.160 +0000 INFO  ProcessTracker - (child_407__Fsck)  Fsck - (entire bucket) Rebuild for bucket='/opt/splunk/var/lib/splunk/indexname/db/db_1528466340_1520517600_38_A25ECA32-B33E-4469-8C76-22190FDCC8CB' took 86.26 seconds.\\\"\")`\nindex=_internal sourcetype=splunkd `splunkadmins_splunkd_source` `indexerhosts` \"At restart after an unclean shutdown found bucket path\" OR (\"finished moving hot to warm\" caller=init_roll) OR \"OnlineFsck - Scheduled repair fsck* kind='entire bucket'\"\n| rex field=path \"(?P<pathWithoutHot>.*)(/|\\\\\\\\)\\S+\"\n| join type=outer pathWithoutHot \n    [| rest /services/data/indexes `splunkindexerhostsvalue` \n    | fields homePath_expanded, title \n    | rename homePath_expanded AS pathWithoutHot, title AS idxFromREST]\n| eventstats max(_time) AS mostRecent by idx, host\n| bin _time span=5m\n| eval idx=coalesce(idxFromREST, idx)\n| stats count(eval(searchmatch(\"unclean shutdown\"))) AS uncleanCount, count(eval(searchmatch(\"Scheduled repair fsck\"))) AS scheduledRepairCount, count(eval(searchmatch(\"finished moving hot to warm\"))) AS hotToWarmCount, max(mostRecent) AS mostRecent by idx, host, _time\n| where uncleanCount>0\n| append\n    [makeresults 1\n    | eval idx=\"#The message \\\"At restart after an unclean shutdown found bucket path...\\\" results in buckets being rolled/repaired. Users may see errors when running searches, such as \\\"Failed to read size=2 event(s) from rawdata in bucket=...Rawdata may be corrupt, see search.log/splunk_search_messages sourcetype. Results may be incomplete!\\\" (OR) \\\"idx=_internal Could not read event: cd=(n/a). Results may be incomplete ! (logging only the first such error; enable DEBUG to see the rest).\\\" This appears to resolve itself in Splunk 7+ when the fsck's complete. I have not determined how to find a completion time...\"]\n| fields - _time, uncleanCount\n| sort idx\n| eval mostRecent=strftime(mostRecent, \"%+\")\n| addcoltotals labelfield=host label=\"Total Count\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. One or more indexes are mentioned as corrupt in the log files, this should auto-repair but it may cause errors in the search interface until the repair is complete, you may also wish to try IndexerLevel - Corrupt buckets via DBInspect",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "4,19,34,49 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Uneven Indexed Data Across The Indexers",
    "search": "| tstats summariesonly=t count WHERE index=\"*\" by splunk_server _time span=10m\n| search `comment(\"If the balance of data between indexer cluster members becomes very unbalanced then the searches tend to spend more CPU on a particular indexer / search peer and this eventually creates issues\")`\n| sort _time \n| eventstats sum(count) AS totalCountForTime, dc(splunk_server) AS indexers by _time \n| eval perc=round((count/totalCountForTime)*100,2) \n| eval expectedShare = 100 / indexers\n| eval perc = 100 - (expectedShare / perc)*100\n| where perc>`splunkadmins_uneven_indexed_perc`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. The data is not been spread across the indexers correctly during this last 4 hour block",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "@h",
    "cron_schedule": "56 1,5,9,13,17,21 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Valid Timestamp Invalid Parsed Time",
    "search": "`comment(\"The timestamp parsing did run but the timestamp found did not match previous events so the time parsing may need a review\")`\nindex=_internal sourcetype=splunkd (`splunkadmins_splunkd_source`) (`indexerhosts`) OR (`heavyforwarderhosts`) \n\"outside of the acceptable time window. If this timestamp is correct, consider adjusting\" \nOR \"is too far away from the previous event's time\" \nOR \"is suspiciously far away from the previous event's time\" `splunkadmins_valid_timestamp_invalidparsed`\n| rex \"source::(?P<source>[^|]+)\\|host::(?P<host>[^|]+)\\|(?P<sourcetype>[^|]+)\"\n| search `comment(\"The goal of this part of the search was to obtain the messages that are relating to this particular host/source/sourcetype, however since the message includes a time we cannot uses values(message) without getting a huge number of values, therefore we use cluster to obtain the unique values. Since we want the original start/end times we use labelonly=true\")`\n| cluster labelonly=true \n| eval message=coalesce(message,event_message)\n| stats count, min(_time) AS firstSeen, max(_time) AS lastSeen, first(message) AS message by host, source, sourcetype, cluster_label\n| search `comment(\"While 'A possible timestamp match (...) is outside of the acceptable time window' and 'Time parsed (...) is too far away from the previous event's time', result in the current indexing time been used, the 'Accepted time (...) is suspiciously far away from the previous event's time' is accepted and therefore we need to expand the investigation query time to include this time range as well!\")` \n| rex field=message \"Accepted time \\((?P<acceptedTime>[^\\)]+)\"\n| eval acceptedTime=strptime(acceptedTime, \"%a %b %d %H:%M:%S %Y\")\n| eval firstSeen=if(acceptedTime<firstSeen,acceptedTime,firstSeen)\n| search `comment(\"Now that we have the first message for each labelled cluster, we now take all relevant message per host/source/sourcetype\")`\n| stats values(acceptedTime) AS acceptedTime, sum(count) AS count, min(firstSeen) AS firstSeen, max(lastSeen) AS lastSeen, values(message) AS message by host, source, sourcetype\n| eval invesEnd=if(lastSeen=firstSeen,round(lastSeen+1),round(lastSeen)), invesStart=floor(firstSeen)\n| eval invesDataSource = replace(source, \"\\\\\\\\\", \"\\\\\\\\\\\\\\\\\")\n| eval investigationQuery=\"`comment(\\\"Please note that this query may need to be narrowed down further before running it, this is an example only...\\\")` index=* host=\" . host . \" sourcetype=\\\"\" . sourcetype . \"\\\" source=\\\"\" . invesDataSource . \"\\\" earliest=\" . invesStart . \" latest=\" . invesEnd . \" | eval indextime=strftime(_indextime, \\\"%+\\\")\"\n| eval firstSeen=strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\")\n| table host, source, sourcetype count, firstSeen, lastSeen, message, investigationQuery\n| sort - count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. The timestamp was parsed but an error was thrown to advise that the timestamp does not appear to be correct",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 2 * * 3",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Volume (Cold) Has Been Exceeded",
    "search": "`comment(\"The cold volume is causing indexes to be trimmed, this may or may not be an issue...\")` \nindex=_internal `indexerhosts` sourcetype=splunkd (`splunkadmins_splunkd_source`) \"Size exceeds max, will have to trim \" volume!=\"hot\"\n| fields host, _raw",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-5h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. The non-hot volume has been exceeded therefore we are deleting data before the time limit is hit...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "54 1,5,9,13,17,21 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Weekly Broken Events Report",
    "search": "`comment(\"The event that came in was greater than the maximum number of lines that were configured, therefore it was broken into multiple events...\")` \n`comment(\"If running Splunk 7 or newer than refer to the monitoring console Indexing -> Inputs -> Data Quality\")`\nindex=_internal \"AggregatorMiningProcessor - Breaking event because limit of\" sourcetype=splunkd (`splunkadmins_splunkd_source`) `splunkadmins_weekly_brokenevents`\n| rex \"Breaking event because limit of (?P<curlimit>\\d+)\" \n| stats max(_time) AS mostRecent, min(_time) AS firstSeen, count by data_sourcetype, data_host, curlimit\n| eval longerThan=curlimit-1\n| eval invesLatest = if(mostRecent==firstSeen,mostRecent+1,mostRecent)\n| rename data_sourcetype AS sourcetype, data_host AS host\n| eval investigationQuery=\"`comment(\\\"If no results are found prepend the earliest=/latest= with _index_ (eg _index_earliest=...) and expand the timeframe searched over, as the parsed timestamps from the data does not have to exactly match the time the warnings appeared...\\\")` index=* host=\" . host . \" sourcetype=\\\"\" . sourcetype . \"\\\" linecount>\" . longerThan . \" earliest=\" . firstSeen . \" latest=\" . invesLatest\n| fields - firstSeen, longerThan, invesLatest\n| eval mostRecent=strftime(mostRecent, \"%+\")\n| sort - count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. These events are been broken due to reaching the maximum number of lines limit...in Splunk 7 and above the Monitoring Console, Indexing -> Inputs -> Data Quality will help here...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 5 * * 4",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - Weekly Truncated Logs Report",
    "search": "`comment(\"The line was truncated due to length, the TRUNCATE setting may need tweaking (or it may be just bad data coming in)\")`\n`comment(\"If running Splunk 7 or newer than refer to the Monitoring Console, Indexing -> Inputs -> Data Quality\")`\n`comment(\"If you are in a (very) performance sensitive environment you might want to remove the rex/eval lines for the data_host field and let the admin update the inves query manually\")`\nindex=_internal \"Truncating line because limit of\" sourcetype=splunkd (`splunkadmins_splunkd_source`) (`heavyforwarderhosts`) OR (`indexerhosts`) `splunkadmins_weekly_truncated`\n| rex \"Truncating line because limit of (?P<curlimit>\\d+) bytes.*with a line length >= (?P<approxlinelength>\\S+)\" \n| rex field=data_host \"(?P<data_host>[^\\.]+)\"\n| eval data_host=data_host . \"*\"\n| stats min(_time) AS firstSeen, max(_time) AS lastSeen, count, avg(approxlinelength) AS avgApproxLineLength, max(approxlinelength) AS maxApproxLineLength, values(data_host) AS hosts by data_sourcetype, curlimit\n| rename data_sourcetype AS sourcetype\n| eval hostList=if(mvcount(hosts)>1,mvjoin(hosts,\" OR host=\"),hosts)\n| eval hostList=\"host=\" . hostList\n| eval avgApproxLineLength = round(avgApproxLineLength)\n| eval invesLastSeen=if(firstSeen==lastSeen,lastSeen+1,lastSeen)\n| eval firstSeen=firstSeen-10\n| eval invesLastSeen=invesLastSeen+10\n| eval investigationQuery=\"`comment(\\\"Find examples where the truncation limit has been reached\\\")` `comment(\\\"The earliest/latest time is based on the warning messages in the Splunk logs, they may need customisation!\\\")` index=* sourcetype=\" . sourcetype . \" \" . hostList . \" earliest=\" . firstSeen . \" latest=\" . invesLastSeen . \" | where len(_raw)=\" . curlimit\n| sort - count\n| eval lastSeen=strftime(lastSeen, \"%+\")\n| table sourcetype, curlimit, count, avgApproxLineLength, maxApproxLineLength, lastSeen, investigationQuery\n| where count>`splunkadmins_weekly_truncated_count`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. These events are been truncated due to hitting the truncation limit, in Splunk 7 and above the Monitoring Console, Indexing -> Inputs -> Data Quality will help here...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 5 * * 2",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - platform_stats.counters hosts",
    "search": "| tstats dc(host) AS unique_hosts where index=* earliest=-24h, latest=+5y _index_earliest=-1h\n| eval indexer_cluster=`indexer_cluster_name`\n| eval prefix=\"platform_stats.counters.\"\n| addinfo \n| rename info_max_time AS _time \n| fields - info_* \n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix indexer_cluster\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-5m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to provide a count of number of unique hostnames sending data to Splunk (note realtime_schedule = 0)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "45 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - platform_stats.counters hosts 24hour",
    "search": "| tstats dc(host) AS unique_hosts_24hr where index=*, earliest=-48h, latest=+5y _index_earliest=-24h\n| eval indexer_cluster=`indexer_cluster_name`\n| eval prefix=\"platform_stats.counters.\"\n| addinfo \n| rename info_max_time AS _time \n| fields - info_* \n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix indexer_cluster\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to provide a daily count of number of unique hostnames sending data to Splunk over a 24 hour period (note realtime_schedule = 0)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 1 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - platform_stats.indexers stddev incoming measurement",
    "search": "index=_internal `indexerhosts` sourcetype=splunkd source=*metrics.log TERM(group=tcpin_connections) \n| eval name=`forwarder_name(hostname)` \n| eval indexer_cluster=`indexer_cluster_name(host)` \n| bin _time span=1m \n| stats sum(kb) AS kb by host, indexer_cluster, name, _time \n| eval kb=kb/60 \n| stats stdev(kb) AS platform_stats.indexers.deviation_incoming by _time, indexer_cluster, name \n`comment(\"| eval prefix=\\\"platform_stats.indexers.\" | mcollect index=a_metrics_index split=true prefix_field=prefix indexer_cluster, name\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to measure the variance between indexer peers, (note realtime_schedule = 0), from an incoming forwarder point of view by forwarder group",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "*/10 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - platform_stats.indexers stddev measurement",
    "search": "index=_internal `indexerhosts` sourcetype=splunkd source=*metrics.log TERM(group=thruput) TERM(name=thruput) \n| bin span=62sec _time \n| eval indexer_cluster=`indexer_cluster_name(host)` \n| stats sum(instantaneous_kbps) as instantaneous_kbps by host _time indexer_cluster \n| stats stdev(instantaneous_kbps) AS stdev_kbps by indexer_cluster, _time \n| eval prefix=\"platform_stats.indexers.\" \n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix indexer_cluster\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to measure the variance between indexer peers, (note realtime_schedule = 0), span based on https://github.com/silkyrich/cluster_health_tools/tree/master/default/data/ui/views/indexer_performance.xml",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "*/10 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - platform_stats.indexers totalgb measurement",
    "search": "index=_internal `licensemasterhost` type=usage sourcetype=splunkd source=*license_usage.log\n`comment(\"Alternative query IndexerLevel - platform_stats.indexers totalgb_thruput measurement if you want the _internal indexes among others. This query uses license usage instead as we are measuring what non-internal data we are indexing\")`\n| stats sum(b) AS totalbytes by i \n| append \n    [| rest /services/search/distributed/peers \n    | fields guid peerName | rename guid AS i ] \n| eval totalgb=totalbytes/1024/1024/1024\n| eventstats values(peerName) AS indexer by i\n| where totalgb>0\n| stats sum(totalgb) AS totalgb by indexer\n| eval prefix=\"platform_stats.indexers.\"\n| eval indexer_cluster=`indexer_cluster_name` \n| addinfo \n| rename info_max_time AS _time \n| fields - info_* \n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix indexer indexer_cluster\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to measure the amount of data going through the platform, (note realtime_schedule = 0)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "*/10 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - platform_stats.indexers totalgb_thruput measurement",
    "search": "index=_internal `indexerhosts` TERM(group=thruput) TERM(name=index_thruput) source=*metrics.log sourcetype=splunkd \n| stats sum(kb) as totalkb by host \n| eval totalgb_thruput = totalkb/1024/1024 \n| where totalgb_thruput>0 \n| eval prefix=\"platform_stats.indexers.\" \n| eval indexer_cluster=`indexer_cluster_name` \n| addinfo \n| rename info_max_time AS _time \n| fields - info_*, totalkb \n| rename host AS indexer \n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix indexer indexer_cluster\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to measure the amount of data going through the platform, (note realtime_schedule = 0)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "*/10 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "IndexerLevel - strings_metadata triggering bucket rolling",
    "search": "`comment(\"The caller=strings_metadata relates to maxMetaEntries in the indexes.conf.spec file, at the time of writing it is the maximum number of unique lines in .data files in a bucket, once exceeded it is rolled so this may cause premature bucket rolling\")` \nindex=_internal `indexerhosts` sourcetype=splunkd `splunkadmins_splunkd_source` caller=strings_metadata \n| cluster showcount=true \n| stats sum(entries) AS count, values(host) AS hosts, values(event_message) AS event_messages by idx",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. This relates to premature bucket rolling so it may or may not be a high priority issue...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "47 4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_All_Assets",
    "search": "| union `kom_all_assets`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-10m@m",
    "eai:acl.sharing": "app",
    "description": "Details of all knowledge objects",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "@m",
    "cron_schedule": "*/30 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Apps_All",
    "search": "| rest `kom_all_apps`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h@h",
    "eai:acl.sharing": "app",
    "description": "Get All Apps for audit reporting lookup",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "now",
    "cron_schedule": "17 */4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Audit_Collect_Missing_Events",
    "search": "`kom_audit_indexes` status=2*   user=*\n(sourcetype=splunkd_ui_access (method IN (POST DELETE) servicesNS ((data/ui/views) OR (saved/searches) OR (datamodel/model)) OR (alerts/alert_actions)) OR (method IN (GET POST) uri_path=\"*/app/*/*/*\" AND (converttohtml OR edit)) OR (method IN (POST) \"*/manager/*/data/ui/views/*\")) OR (sourcetype=splunkd_access method IN (POST, DELETE) (/servicesNS/* OR /services/datamodel/*)\n((data (lookup* OR props OR transforms OR workflow)) OR (saved/ntags) OR (saved/fvtags) OR (macros) OR (saved/eventtypes) OR (saved/searches) OR (data/ui/views) OR (datamodel/model) OR (admin/tags)))\n| eval uri_path=urldecode(uri_path)\n| rex field=uri_path \"\\/(__raw\\/servicesNS\\/[a-zA-Z0-9-_.]+|manager)\\/(?<app_name>[a-zA-Z0-9-_]+)\\/data\\/ui\\/(?<ko_type>views)(\\/)?(?<ko_name>[a-zA-Z0-9-_]+)?(\\/)?(?<edit_type1>[^\\/]+)?\"\n| rex field=uri_path \"\\/app\\/(?<app_name>[a-zA-Z0-9-_]+)\\/(?<ko_name>[a-zA-Z0-9-_]+)(\\/)?(?<edit_type1>[^\\/]+)?\"\n| rex field=uri_path \"__raw\\/servicesNS\\/[a-zA-Z0-9-_.]+\\/(?<app_name>[a-zA-Z0-9_-]+)\\/saved\\/(?<ko_type>searches)?(\\/)?(?<ko_name>[a-zA-Z0-9:\\s._-]+)?(\\/)?(?<edit_type1>[^\\/]+)?\"\n| rex field=uri_path \"__raw\\/servicesNS\\/[a-zA-Z0-9-_.]+\\/(?<app_name>[a-zA-Z0-9_-]+)\\/datamodel\\/(?<ko_type>model)?(\\/)?(?<ko_name>[a-zA-Z0-9:\\s._-]+)?(\\/)?(?<edit_type1>[^\\/]+)?\"\n| rex field=uri_path \"__raw\\/servicesNS\\/[a-zA-Z0-9-_.]+\\/(?<app_name>[a-zA-Z0-9_-]+)\\/alerts\\/(?<ko_type>alert_actions)?(\\/)?(?<ko_name>[a-zA-Z0-9:\\s._-]+)(\\/)?(?<edit_type1>[^\\/]+)?\"\n| rex field=uri_path \"^\\/servicesNS\\/[a-zA-Z0-9-_]+\\/(?<app_name>[a-zA-Z0-9-_]+)\\/data\\/(?<after_data>[a-zA-Z0-9-_]+)(\\/)?(?<ko_type>[a-zA-Z0-9-_.]+)?(\\/)?(?<ko_name>[^\\/]*)?\"\n| rex field=uri_path \"^\\/servicesNS\\/[a-zA-Z0-9-_]+\\/(?<app_name>[a-zA-Z0-9-_]+)\\/(saved|admin|datamodel)\\/(?<ko_type>[a-zA-Z0-9-_.]+)(\\/)?(?<ko_name>[^\\/]+)?\"\n| rex field=uri_path \"^\\/servicesNS\\/[a-zA-Z0-9-_]+\\/(?<app_name>[a-zA-Z0-9-_]+)\\/[a-zA-Z0-9-_]+\\/(?<ko_type>macros)(\\/)?(?<ko_name>[^\\/]+)?\"\n| rex field=uri_path \"^\\/servicesNS\\/[a-zA-Z0-9-_]+\\/(?<app_name>[a-zA-Z0-9-_]+)\\/data\\/(?<ko_type>lookup-table-files)(\\/)?(?<ko_name>[a-zA-Z0-9-_.]+)?\"\n| rex field=uri_path \"^\\/services\\/datamodel\\/(?<ko_type>model)(\\/)?(?<ko_name>[a-zA-Z0-9:\\s._-]+)?(\\/)?(?<edit_type1>[^\\/]+)?\"\n| eval ko_type = case(ko_type==\"lookups\" AND after_data==\"props\",\"lookup-props\",ko_type==\"lookups\" AND after_data==\"transforms\",\"lookup-transforms\",1=1,ko_type)\n|rename file AS edit_type\n|fillnull edit_type value=\"none\" \n|fillnull value=NULL |eval ko_name = if(ko_name==\"\", \"NULL\", ko_name)\n|eval ko_name=urldecode(ko_name)\n|eval action=case(method == \"POST\",\"EDIT\",method == \"GET\",\"CREATE\",method == \"DELETE\",\"DELETE\",1=1,\"UNKNOWN\") \n|eval edit_type = urldecode(edit_type) |eval edit_type = if((edit_type == ko_name OR ko_name==\"NULL\"),\"no_edit_type\",edit_type) |eval action = if((ko_name==\"NULL\" OR ko_name==\"\"),\"NEW\",action) |eval ko_name = urldecode(ko_name) \n|eval edit_type=`set_edit_type(edit_type)`\n`ko_edittype_filter`\n|rex field=other \"\\s(?<event_duration>\\d+)ms\"\n|eval search_title=\"RMD5\" . substr(md5(ko_name), 13)\n|eval ko_type = `set_ko_type(ko_type)`\n|rename host AS origin_splunk_server\n|eval app_name=urldecode(app_name)\n|eval audit_event_hash = md5(_time.origin_splunk_server.method.uri_path.event_duration)\n|stats count AS event_count by _time origin_splunk_server app_name ko_type ko_name action edit_type user uri_path search_title after_data audit_event_hash\n|fields - event_count\n| lookup searchheads_lookup _key AS origin_splunk_server OUTPUTNEW searchhead domain_url shc_label label AS sh_label \n| eval origin_splunk_server = if(isnotnull(shc_label) AND shc_label!=\"\",shc_label,origin_splunk_server) \n| transaction ko_name action edit_type ko_type user app_name origin_splunk_server maxspan=2s mvlist=t \n|eval time_stamp = _time\n|eval url = `build_url(ko_type,after_data)`\n|convert timeformat=\"%H:%M:%S %d-%b-%Y\" ctime(time_stamp)\n| join type=left user [|inputlookup kom_splunk_users |fields username realname |rename username AS user]\n| join type=left ko_type ko_name [|inputlookup kom_asset_all_kos |fields title ko_type label |rename title AS ko_name, label AS ko_label]\n| join type=left app_name [|inputlookup kom_splunk_apps |fields title label |rename title AS app_name, label AS app_label ]\n`first_mv(ko_name)` `first_mv(ko_label)` `first_mv(action)` `first_mv(edit_type)` `first_mv(ko_type)` `first_mv(ko_type)` `first_mv(user)` `first_mv(realname)`\n`first_mv(app_name)` `first_mv(app_label)` `first_mv(origin_splunk_server)` `first_mv(url)` `first_mv(uri_path)` `first_mv(audit_event_hash)` `first_mv(search_title)`\n| fields time_stamp ko_name ko_label action edit_type ko_type user realname app_name app_label origin_splunk_server url uri_path audit_event_hash search_title |fields - _raw\n| join type=left audit_event_hash\n    [|search `kom_summary_indexes` source=KOM_Change_Audit_Summary | eval is_audited = 1 |fields audit_event_hash is_audited]\n| where isnull(is_audited)\n| addinfo\n| collect `kom_summary_indexes` marker=\"search_name=KOM_Audit_Collect_Missing_Events\" source=KOM_Change_Audit_Summary addtime=true",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-7d",
    "eai:acl.sharing": "app",
    "description": "Collect missing events and write to summary index",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "-1h",
    "cron_schedule": "48 */4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Change_Audit_Summary",
    "search": "`kom_audit_indexes` status=2*   user=* \n(sourcetype=splunkd_ui_access (method IN (POST DELETE) servicesNS ((data/ui/views) OR (saved/searches) OR (datamodel/model)) OR (alerts/alert_actions)) OR (method IN (GET POST) uri_path=\"*/app/*/*/*\" AND (converttohtml OR edit)) OR (method IN (POST) \"*/manager/*/data/ui/views/*\")) OR (sourcetype=splunkd_access method IN (POST, DELETE) (/servicesNS/* OR /services/datamodel/*) \n((data (lookup* OR props OR transforms OR workflow)) OR (saved/ntags) OR (saved/fvtags) OR (macros) OR (saved/eventtypes) OR (saved/searches) OR (data/ui/views) OR (datamodel/model) OR (admin/tags))) \n| eval uri_path=urldecode(uri_path) \n| rex field=uri_path \"\\/(__raw\\/servicesNS\\/[a-zA-Z0-9-_.]+|manager)\\/(?<app_name>[a-zA-Z0-9-_]+)\\/data\\/ui\\/(?<ko_type>views)(\\/)?(?<ko_name>[a-zA-Z0-9-_]+)?(\\/)?(?<edit_type1>[^\\/]+)?\" \n| rex field=uri_path \"\\/app\\/(?<app_name>[a-zA-Z0-9-_]+)\\/(?<ko_name>[a-zA-Z0-9-_]+)(\\/)?(?<edit_type1>[^\\/]+)?\" \n| rex field=uri_path \"__raw\\/servicesNS\\/[a-zA-Z0-9-_.]+\\/(?<app_name>[a-zA-Z0-9_-]+)\\/saved\\/(?<ko_type>searches)?(\\/)?(?<ko_name>[a-zA-Z0-9:\\s._-]+)?(\\/)?(?<edit_type1>[^\\/]+)?\" \n| rex field=uri_path \"__raw\\/servicesNS\\/[a-zA-Z0-9-_.]+\\/(?<app_name>[a-zA-Z0-9_-]+)\\/datamodel\\/(?<ko_type>model)?(\\/)?(?<ko_name>[a-zA-Z0-9:\\s._-]+)?(\\/)?(?<edit_type1>[^\\/]+)?\" \n| rex field=uri_path \"__raw\\/servicesNS\\/[a-zA-Z0-9-_.]+\\/(?<app_name>[a-zA-Z0-9_-]+)\\/alerts\\/(?<ko_type>alert_actions)?(\\/)?(?<ko_name>[a-zA-Z0-9:\\s._-]+)(\\/)?(?<edit_type1>[^\\/]+)?\" \n| rex field=uri_path \"^\\/servicesNS\\/[a-zA-Z0-9-_]+\\/(?<app_name>[a-zA-Z0-9-_]+)\\/data\\/(?<after_data>[a-zA-Z0-9-_]+)(\\/)?(?<ko_type>[a-zA-Z0-9-_.]+)?(\\/)?(?<ko_name>[^\\/]*)?\" \n| rex field=uri_path \"^\\/servicesNS\\/[a-zA-Z0-9-_]+\\/(?<app_name>[a-zA-Z0-9-_]+)\\/(saved|admin|datamodel)\\/(?<ko_type>[a-zA-Z0-9-_.]+)(\\/)?(?<ko_name>[^\\/]+)?\" \n| rex field=uri_path \"^\\/servicesNS\\/[a-zA-Z0-9-_]+\\/(?<app_name>[a-zA-Z0-9-_]+)\\/[a-zA-Z0-9-_]+\\/(?<ko_type>macros)(\\/)?(?<ko_name>[^\\/]+)?\" \n| rex field=uri_path \"^\\/servicesNS\\/[a-zA-Z0-9-_]+\\/(?<app_name>[a-zA-Z0-9-_]+)\\/data\\/(?<ko_type>lookup-table-files)(\\/)?(?<ko_name>[a-zA-Z0-9-_.]+)?\" \n| rex field=uri_path \"^\\/services\\/datamodel\\/(?<ko_type>model)(\\/)?(?<ko_name>[a-zA-Z0-9:\\s._-]+)?(\\/)?(?<edit_type1>[^\\/]+)?\" \n| eval ko_type = case(ko_type==\"lookups\" AND after_data==\"props\",\"lookup-props\",ko_type==\"lookups\" AND after_data==\"transforms\",\"lookup-transforms\",1=1,ko_type) \n|rename file AS edit_type \n|fillnull edit_type value=\"none\" \n|fillnull value=NULL |eval ko_name = if(ko_name==\"\", \"NULL\", ko_name)\n|eval ko_name=urldecode(ko_name) \n|eval action=case(method == \"POST\",\"EDIT\",method == \"GET\",\"CREATE\",method == \"DELETE\",\"DELETE\",1=1,\"UNKNOWN\") \n|eval edit_type = urldecode(edit_type) |eval edit_type = if((edit_type == ko_name OR ko_name==\"NULL\"),\"no_edit_type\",edit_type) |eval action = if((ko_name==\"NULL\" OR ko_name==\"\"),\"NEW\",action) |eval ko_name = urldecode(ko_name) \n|eval edit_type=`set_edit_type(edit_type)` \n`ko_edittype_filter` \n|rex field=other \"\\s(?<event_duration>\\d+)ms\" \n|eval search_title=\"RMD5\" . substr(md5(ko_name), 13) \n|eval ko_type = `set_ko_type(ko_type)` \n|rename host AS origin_splunk_server \n|eval app_name=urldecode(app_name) \n|eval audit_event_hash = md5(_time.origin_splunk_server.method.uri_path.event_duration) \n|stats count AS event_count by _time origin_splunk_server app_name ko_type ko_name action edit_type user uri_path search_title after_data audit_event_hash \n|fields - event_count _raw\n| join type=left origin_splunk_server [| inputlookup searchheads_lookup | eval searchhead=_key |fields searchhead domain_url shc_label label |rename searchhead AS origin_splunk_server]\n| eval origin_splunk_server_label = if(isnotnull(shc_label) AND shc_label!=\"\",shc_label,label)\n| eval origin_splunk_server_label = if(isnull(origin_splunk_server_label) OR origin_splunk_server_label==\"\", origin_splunk_server, origin_splunk_server_label)\n| transaction ko_name action edit_type ko_type user app_name origin_splunk_server maxspan=2s mvlist=t \n|eval time_stamp = _time |fields - _time \n|eval url = `build_url(ko_type,after_data)` \n|convert timeformat=\"%H:%M:%S %d-%b-%Y\" ctime(time_stamp) \n| join type=left user [|inputlookup kom_splunk_users |fields username realname |rename username AS user] \n| join type=left ko_type ko_name [|inputlookup kom_asset_all_kos |fields title ko_type label |rename title AS ko_name, label AS ko_label] \n| join type=left app_name [|inputlookup kom_splunk_apps |fields title label |rename title AS app_name, label AS app_label ] \n`first_mv(ko_name)` `first_mv(ko_label)` `first_mv(action)` `first_mv(edit_type)` `first_mv(ko_type)` `first_mv(ko_type)` `first_mv(user)` `first_mv(realname)`\n`first_mv(app_name)` `first_mv(app_label)` `first_mv(origin_splunk_server)` `first_mv(url)` `first_mv(uri_path)` `first_mv(audit_event_hash)` `first_mv(search_title)`\n`first_mv(origin_splunk_server_label)` `first_mv(domain_url)`\n| fields time_stamp ko_name ko_label action edit_type ko_type user realname app_name app_label origin_splunk_server url uri_path audit_event_hash search_title origin_splunk_server_label domain_url |fields - _raw",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-20m@m",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "-1m@m",
    "cron_schedule": "*/19 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Custom_Command_List",
    "search": "| rest `run_map_for_rest(\"/servicesNS/nobody/-/data/commands\")` \n| fields app_name title streaming filename splunk_server shc_name \n| rename title AS command",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-10m@m",
    "eai:acl.sharing": "app",
    "description": "Generate a list of all custom SPL commands",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "@m",
    "cron_schedule": "53 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Dashboard_All_Assets",
    "search": "| rest `kom_all_dashboards`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "",
    "cron_schedule": "43 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Dashboard_All_Searches_Statistics",
    "search": "`kom_summary_indexes` source=\"KOM_Dashboard_Searches*\" dash_epoch=*\n| lookup kom_dashboard_searches sid_user sid_app dash_epoch shc_member OUTPUTNEW DashboardName AS dashboard_name_lookup app_name AS app_name_lookup\n| eval decoded_sid_app = if(isnotnull(sid_app),sid_app,\"\")\n| kombase64 action=decode decoded_sid_app\n| eval app_name = if(app_name == sid_app,decoded_sid_app,app_name)\n| eval DashboardName = if(isnull(DashboardName) OR DashboardName==\"\",dashboard_name_lookup,DashboardName)\n| eval kom_search_execution_stats=if(kom_summary_type=\"dashboard_searches_utilisation\",0,1)\n| eval kom_search_resource_stats=if(kom_summary_type=\"dashboard_searches_utilisation\",1,0)\n| stats values(DashboardName) AS dashboard_name values(sid_app) AS sid_app dc(search_spl) AS dc_spl avg(avg_idx_cpu) AS avg_idx_cpu avg(avg_sh_cpu) AS avg_sh_cpu avg(total_run_time_sec) AS avg_total_run_time_sec avg(avg_idx_read_mb) AS avg_idx_read_mb avg(avg_idx_written_mb) AS avg_idx_written_mb avg(avg_idx_pct_mem) AS avg_idx_pct_mem avg(avg_sh_read_mb) AS avg_sh_read_mb avg(avg_sh_written_mb) AS avg_sh_written_mb avg(avg_sh_pct_mem) AS avg_sh_pct_mem max(max_idx_cpu) AS max_idx_cpu max(max_idx_read_mb) AS max_idx_read_mb max(max_idx_written_mb) AS max_idx_written_mb max(max_idx_pct_mem) AS max_idx_pct_mem max(max_sh_cpu) AS max_sh_cpu max(max_sh_read_mb) AS max_sh_read_mb max(max_sh_written_mb) AS max_sh_written_mb max(max_sh_pct_mem) AS max_sh_pct_mem avg(ms_spent_extracting_rawdata) AS ms_spent_extracting_rawdata avg(ms_spent_examing_tsidx) AS ms_spent_examing_tsidx avg(dropped_events) AS dropped_events avg(scanned_events) AS scanned_events avg(event_count) AS event_count avg(search_startup_time) AS search_startup_time avg(searched_buckets) AS searched_buckets avg(bloomfilter_eliminated_bkts) AS bloomfilter_eliminated_bkts avg(events_in_matching_bkts) AS events_in_matching_bkts avg(slices_in_matching_bkts) AS slices_in_matching_bkts avg(slices_decompressed) AS slices_decompressed avg(search_span_sec) AS search_span_sec sum(kom_search_execution_stats) AS kom_search_execution_stats sum(kom_search_resource_stats) AS kom_search_resource_stats \n  by sid_search_name dash_epoch app_name",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-80m@m",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "-20m@m",
    "cron_schedule": "52 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Dashboard_Identified_Searches",
    "search": "index=summary source=\"KOM_Dashboard_Searches_*\" search_type=dashboard\n| eval decoded_sid_app = if(isnotnull(sid_app),sid_app,\"\")\n| kombase64 action=decode decoded_sid_app\n| eval app_name = if(app_name == sid_app,decoded_sid_app,app_name)\n| stats values(app_name) AS app_name values(DashboardName) AS DashboardName values(search_props_mode) AS search_props_mode values(search_props_type) AS search_props_type values(search_id) AS search_id values(search_name) AS summary_search_name values(search_type) AS search_type values(sid_search_name) AS sid_search_name values(sid_seqno) AS sid_seqno values(orig_host) AS orig_host\n    count(search_name) AS count_summary_search_name\nby sid_app dash_epoch sid_user shc_member\n| where isnotnull(DashboardName)\n| fields app_name count_summary_search_name DashboardName sid_user orig_host shc_member dash_epoch savedsearch_name sid_search_name sid_seqno *",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-2h",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "now",
    "cron_schedule": "37 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Dashboard_Requests_Summary",
    "search": "index=_internal sourcetype=splunkd_ui_access method=GET NOT \"__raw\" NOT user=\"-\" status=200\n| rex field=uri_path \"\\/[-\\w]+\\/app\\/(?<app_name>[-\\w]+)\\/(?<dashboard>[-\\w]+)$\"\n| eval app_name=if(app_name=\"-\" OR app_name=\"launcher\",null(),app_name)\n| eval dashboard=if((dashboard=\"report\" OR dashboard=\"reports\" OR dashboard=\"dashboards\" OR dashboard=\"search\"),null(),dashboard)\n| eval dashboard_title=if(match(dashboard, \"[-\\s_]\"), \"RMD5\" . substr(md5(dashboard), 13), dashboard) \n| eval file=dashboard\n| rename host AS orig_splunk_server \n| stats latest(_time) AS _time count AS dashboard_views dc(user) AS distinct_user values(user) AS users by orig_splunk_server app_name file dashboard dashboard_title uri_path \n| where isnotnull(app_name) AND isnotnull(dashboard)\n| convert ctime(_time) AS view_datetime\n| eval sid_app = app_name\n| kombase64 action=encode sid_app",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-21m@m",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "-1m@m",
    "cron_schedule": "*/20 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Dashboard_Searches_Execution_Statistics_Summary",
    "search": "index=_audit TERM(action=search)  NOT scheduler savedsearch_name!=\"\" \n(TERM(info=granted) TERM(search_id=*)) OR (TERM(info=completed) OR TERM(info=canceled))\n| eval search_id=trim(search_id, \"'\")\n| eval search=trim(search, \"'\")\n| rex \"user=(?<user>[^,]+)\"\n| rex field=search_id \"^(?<adhoc_epoch>\\d+)\\.(?<sid_seqno>\\d+)(?:_(?<shc_member>[-\\w]+))?$\"\n| rex field=search_id \"^SummaryDirector_(?<summary_epoch>\\d+)\\.(?<sid_seqno>\\d+)(?:_(?<shc_member>[-\\w]+))?$\"\n| rex field=search_id \"^RemoteStorageRetrieveBuckets_(?<s2_epoch>\\d+)\\.(?<sid_seqno>\\d+)$\"\n| rex field=search_id \"^md_(?<metadata_epoch>\\d+)\\.(?<sid_seqno>\\d+)$\"\n| rex field=search_id \"^(?<sid_user>\\w+)__?(?<sid_owner>\\w+)_(?<sid_app>[^_]+)__?(?<sid_search_name>[^_]+)_(?<dash_epoch>\\d+)\\.(?<sid_seqno>\\d+)(?:_(?<shc_member>[-\\w]+))?$\"\n| rex field=search_id \"^(?:(?<rt_marker>rt_)|(?<replica_marker>rsa_))?(?<sid_user>\\w+)_?(?<sid_owner>\\w+)__?(?<sid_app>[^_]+_)_?(?<sid_search_name>[^_]+)_at_(?<scheduler_epoch>\\d+)_(?<sid_seqno>\\d+)(?:_(?<shc_member>[-\\w]+))?$\"\n| rex field=search_id \"^subsearch_(?<parent_sid>(?<adhoc_epoch>\\d+)\\.(?<sid_seqno>\\d+))(?:_(?<shc_member>[-\\w]+))?_(?<sub_epoch>\\d+)\\.(?<sub_seqno>\\d+)$\"\n| rex field=search_id \"^subsearch_(?<parent_sid>(?<sid_user>\\w+?)__?(?<sid_owner>\\w+?)_(?<sid_app>[^_]+?)__(?<sid_search_name>[^_]+)_(?<dash_epoch>\\d+)\\.(?<sid_seqno>\\d+))(?:_(?<shc_member>[-\\w]+))?_(?<sub_epoch>\\d+)\\.(?<sub_seqno>\\d+)$$\"\n| eval search_type=case(isnotnull(adhoc_epoch), \"adhoc\", isnotnull(summary_epoch), \"acceleration\", isnotnull(metadata_epoch), \"metadata\", isnotnull(s2_epoch), \"S2\", isnotnull(dash_epoch), \"dashboard\", isnotnull(scheduler_epoch), \"scheduled\") \n| eval shc_member = if(isnull(shc_member),\"standalone\",shc_member)\n| where search_type=\"dashboard\"\n| stats earliest(user) AS user\n    earliest(host) AS host\n    earliest(savedsearch_name) AS savedsearch_name\n    earliest(search) AS search_string\n    latest(eval(if(info=\"completed\", _time, null()))) AS completed_time\n    latest(total_run_time) AS total_run_time\n    latest(duration_command_search_rawdata_bucketcache_miss) AS rawdata_cache_miss\n    latest(duration_command_search_index_bucketcache_miss) AS index_cache_miss\n    latest(duration_command_search_rawdata) AS rawdata_time\n    latest(duration_command_search_index) AS index_time\n    latest(drop_count) AS drop_count\n    latest(scan_count) AS scan_count\n    latest(event_count) AS event_count\n    latest(search_startup_time) AS search_startup_time\n    latest(searched_buckets) AS searched_buckets\n    latest(eliminated_buckets) AS eliminated_buckets\n    latest(considered_events) AS considered_events\n    latest(total_slices) AS total_slices\n    latest(decompressed_slices) AS decompressed_slices\n    latest(search_et) AS search_et\n    latest(search_lt) AS search_lt\n    by search_id search_type sid_user sid_owner sid_app sid_search_name dash_epoch sid_seqno shc_member\n| eval sid_user=trim(sid_user, \"_\") \n| eval completed_time = completed_time + 1, span=round(search_lt - search_et, 0)\n| eval span_pretty=tostring(round(span, 0), \"duration\")\n| fields - search_et search_lt\n| rename user AS search_user\n    search_string AS search_spl, total_run_time AS total_run_time_sec,\n    scan_count AS scanned_events, event_count AS matching_events,\n    drop_count AS dropped_events,\n    searched_buckets AS buckets_in_time_range,\n    eliminated_buckets AS bloomfilter_eliminated_bkts,\n    considered_events AS events_in_matching_bkts,\n    total_slices AS slices_in_matching_bkts ,\n    rawdata_time AS ms_spent_extracting_rawdata,\n    index_time AS ms_spent_examing_tsidx,\n    span AS search_span_sec,\n    span_pretty AS search_span,\n    decompressed_slices AS slices_decompressed,\n| eval app_name = sid_app\n| eval app_name=if((len(app_name)<24),app_name.\"========================\",app_name)\n| fields completed_time app_name dash_epoch sid_user sid_search_name sid_app search_type *",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-31m@m",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "-1m@m",
    "cron_schedule": "*/30 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Dashboard_Searches_Resource_Utilisation_Summary",
    "search": "index=_introspection sourcetype=splunk_resource_usage component::PerProcess data.process_type::search data.search_props.provenance=UI:Dashboard* \n| eval search_id=replace('data.search_props.sid', \"^remote_[^_]+_(.*)$\", \"\\1\") \n| stats earliest(data.search_props.provenance) AS provenance, \n    earliest(data.search_props.label) AS savedsearch_name, \n    avg(eval(if('data.search_props.role' == \"peer\", 'data.pct_cpu', null()))) AS avg_idx_cpu, \n    avg(eval(if('data.search_props.role' == \"peer\", 'data.read_mb', null()))) AS avg_idx_read_mb, \n    avg(eval(if('data.search_props.role' == \"peer\", 'data.written_mb', null()))) AS avg_idx_written_mb, \n    avg(eval(if('data.search_props.role' == \"peer\", 'data.pct_memory', null()))) AS avg_idx_pct_mem, \n    avg(eval(if('data.search_props.role' == \"head\", 'data.pct_cpu', null()))) AS avg_sh_cpu, \n    avg(eval(if('data.search_props.role' == \"head\", 'data.read_mb', null()))) AS avg_sh_read_mb, \n    avg(eval(if('data.search_props.role' == \"head\", 'data.written_mb', null()))) AS avg_sh_written_mb, \n    avg(eval(if('data.search_props.role' == \"head\", 'data.pct_memory', null()))) AS avg_sh_pct_mem, \n    max(eval(if('data.search_props.role' == \"peer\", 'data.pct_cpu', null()))) AS max_idx_cpu, \n    max(eval(if('data.search_props.role' == \"peer\", 'data.read_mb', null()))) AS max_idx_read_mb, \n    max(eval(if('data.search_props.role' == \"peer\", 'data.written_mb', null()))) AS max_idx_written_mb, \n    max(eval(if('data.search_props.role' == \"peer\", 'data.pct_memory', null()))) AS max_idx_pct_mem, \n    max(eval(if('data.search_props.role' == \"head\", 'data.pct_cpu', null()))) AS max_sh_cpu, \n    max(eval(if('data.search_props.role' == \"head\", 'data.read_mb', null()))) AS max_sh_read_mb, \n    max(eval(if('data.search_props.role' == \"head\", 'data.written_mb', null()))) AS max_sh_written_mb, \n    max(eval(if('data.search_props.role' == \"head\", 'data.pct_memory', null()))) AS max_sh_pct_mem, \n    BY _time data.search_props.app data.search_props.type, data.search_props.mode, search_id data.search_props.role\n| fillnull value=0 avg_idx_cpu avg_sh_cpu avg_idx_pct_mem avg_sh_pct_mem avg_idx_read_mb avg_sh_read_mb avg_idx_written_mb avg_sh_written_mb max_sh_read_mb max_idx_read_mb max_sh_written_mb max_idx_written_mb max_idx_cpu max_idx_pct_mem max_sh_cpu max_sh_pct_mem\n| rename data.search_props.type AS search_props_type, \n    data.search_props.mode AS search_props_mode, \n    data.search_props.app AS app_name\n    data.search_props.role AS splunk_search_role\n| rex field=search_id \"^(?<sid_user>\\w+)__?(?<sid_owner>\\w+)_(?<sid_app>[^_]+)__?(?<sid_search_name>[^_]+)_(?<dash_epoch>\\d+)\\.(?<sid_seqno>\\d+)(?:_(?<shc_member>[-\\w]+))?$\" \n| rex field=search_id \"^subsearch_(?<parent_sid>(?<sid_user>\\w+)__?(?<sid_owner>\\w+)_(?<sid_app>[^_]+_?)_(?<sid_search_name>[^_]+)_(?<dash_epoch>\\d+)\\.(?<sid_seqno>\\d+))(?:_(?<shc_member>[-\\w]+))?_(?<sub_epoch>\\d+)\\.(?<sub_seqno>\\d+)$$\" \n| eval search_type=case(isnotnull(adhoc_epoch), \"adhoc\", isnotnull(summary_epoch), \"acceleration\", isnotnull(metadata_epoch), \"metadata\", isnotnull(s2_epoch), \"S2\", isnotnull(dash_epoch), \"dashboard\", isnotnull(scheduler_epoch), \"scheduled\") \n| eval sid_user=trim(sid_user, \"_\") \n| rex field=provenance \":Dashboard:(?<DashboardName>\\w+)\" \n| eval introspection_time = _time \n| fields introspection_time app_name DashboardName dash_epoch sid_user sid_search_name sid_app search_type *",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-31m@m",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "-1m@m",
    "cron_schedule": "*/30 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Report_All_Assets",
    "search": "| rest `kom_all_reports`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "",
    "cron_schedule": "41 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Report_Requests_Summary",
    "search": "index=_internal sourcetype=splunkd_ui_access saved searches method=GET status=200 NOT history \n|eval uri_path=urldecode(uri_path)  \n|rex field=uri_path \"\\/(?<request_type>[\\w%-.]+)\\/(?<app_name>[\\w.-]+)[\\/]+saved\\/\" \n|rex field=uri_path \"saved\\/searches\\/(?<report_name>[a-zA-Z0-9:%\\s._-]+)\" \n|eval dashboard_ref = if(request_type!=\"nobody\",1,0) \n|eval user_view = if(request_type==\"nobody\",1,0) \n|eval report_name=urldecode(report_name) \n|eval app_name=urldecode(app_name) \n|eval app_name = if(isnull(app_name),\"Error - unknown\",app_name), report_name = if(isnull(report_name),\"Error - unknown\",report_name) \n|eval search_title=if(match(report_name, \"[-\\s_]\"), \"RMD5\" . substr(md5(report_name), 13), report_name) \n|where report_name!=\"Error - unknown\" AND report_name!=\"_new\" \n|rename host AS orig_splunk_server \n|stats latest(_time) AS _time count AS event_count dc(user) AS distinct_user values(user) AS users sum(dashboard_ref) AS dashboard_refs sum(user_view) AS user_views by orig_splunk_server app_name report_name search_title \n|eval dashboard_refs = round(dashboard_refs/2,0) \n|eval report_views = dashboard_refs+user_views \n|convert ctime(_time) AS view_datetime \n|eval sid_app = app_name \n|kombase64 action=encode sid_app",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-21m@m",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "-1m@m",
    "cron_schedule": "*/20 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Reports_Searches_Execution_Statistics_Summary",
    "search": "index=_audit sourcetype=audittrail \n(TERM(info=granted) TERM(search_id=*)) OR (TERM(info=completed) OR TERM(info=canceled))\n| eval user=if((user == \"n/a\"),null(),user)\n| eval search_id=trim(search_id, \"'\") \n| rex field=search_id \"^(?<sid_caller>\\w+)((_|__)(?<sid_owner>\\w+)(_|__)(?<sid_app>\\w+)__(?<sid_search_name>\\w+))_at\"\n| eval sid_caller = rtrim(sid_caller,\"_\")\n| eval owner = rtrim(owner,\"_\")\n| eval search_type=case(match(sid_caller,\"^(SummaryDirector_|summarize_)\"),\"summarization\",match(sid_caller,\"^((rsa_|rt_|summarize_)?scheduler|alertsmanager)\"),\"scheduled\", match(search_id,\"^_[a-zA-Z0-9]+__\"), \"scheduled\", match(sid_caller,\"^subsearch_\"),\"subsearch\", (isnotnull(sid_caller) AND isnotnull(sid_owner) AND isnotnull(sid_search_name)),\"report\", true(),\"other\")\n| rename sid_caller AS sid_user\n| where search_type=\"report\"\n| stats earliest(user) AS user\n    earliest(host) AS orig_splunk_server\n    earliest(savedsearch_name) AS savedsearch_name\n    earliest(search) AS search_string\n    latest(eval(if(info=\"completed\", _time, null()))) AS completed_time\n    latest(eval(if(info=\"granted\", _time, null()))) AS granted_time    \n    latest(total_run_time) AS total_run_time\n    latest(duration_command_search_rawdata_bucketcache_miss) AS rawdata_cache_miss\n    latest(duration_command_search_index_bucketcache_miss) AS index_cache_miss\n    latest(duration_command_search_rawdata) AS rawdata_time\n    latest(duration_command_search_index) AS index_time\n    latest(drop_count) AS drop_count\n    latest(scan_count) AS scan_count\n    latest(event_count) AS event_count\n    latest(search_startup_time) AS search_startup_time\n    latest(searched_buckets) AS searched_buckets\n    latest(eliminated_buckets) AS eliminated_buckets\n    latest(considered_events) AS considered_events\n    latest(total_slices) AS total_slices\n    latest(decompressed_slices) AS decompressed_slices\n    latest(search_et) AS search_et\n    latest(search_lt) AS search_lt\n    by search_id search_type sid_user sid_owner sid_app sid_search_name\n| eval sid_user=trim(sid_user, \"_\") \n| eval completed_time = completed_time + 1, granted_time = granted_time + 1, span=round(search_lt - search_et, 0)\n| eval search_time = round(completed_time - granted_time,2)\n| eval run_vs_duration_time = round(search_time - total_run_time,2)\n| eval span_pretty=tostring(round(span, 0), \"duration\")\n| fields - search_et search_lt\n| rename user AS search_user\n    search_string AS search_spl, total_run_time AS total_run_time_sec,\n    scan_count AS scanned_events, event_count AS matching_events,\n    drop_count AS dropped_events,\n    searched_buckets AS buckets_in_time_range,\n    eliminated_buckets AS bloomfilter_eliminated_bkts,\n    considered_events AS events_in_matching_bkts,\n    total_slices AS slices_in_matching_bkts ,\n    rawdata_time AS ms_spent_extracting_rawdata,\n    index_time AS ms_spent_examing_tsidx,\n    span AS search_span_sec,\n    span_pretty AS search_span,\n    decompressed_slices AS slices_decompressed,\n| eval app_name = sid_app\n| fields run_vs_duration_time search_time total_run_time_sec granted_time completed_time app_name sid_user sid_search_name sid_app search_type *\n| localop\n| kombase64 action=decode app_name",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-31m@m",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "-1m@m",
    "cron_schedule": "*/30 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Reports_Searches_Resource_Utilisation_Summary",
    "search": "index=_introspection  source=\"*resource_usage.log*\" component::PerProcess data.process_type::search data.search_props.provenance=UI:Report\n| eval search_id=replace('data.search_props.sid', \"^remote_[^_]+_(.*)$\", \"\\1\") \n| stats earliest(data.search_props.provenance) AS provenance, \n    earliest(data.search_props.label) AS savedsearch_name, \n    earliest(host) AS orig_splunk_server    \n    avg(eval(if('data.search_props.role' == \"peer\", 'data.pct_cpu', null()))) AS avg_idx_cpu, \n    avg(eval(if('data.search_props.role' == \"peer\", 'data.read_mb', null()))) AS avg_idx_read_mb, \n    avg(eval(if('data.search_props.role' == \"peer\", 'data.written_mb', null()))) AS avg_idx_written_mb, \n    avg(eval(if('data.search_props.role' == \"peer\", 'data.pct_memory', null()))) AS avg_idx_pct_mem, \n    avg(eval(if('data.search_props.role' == \"head\", 'data.pct_cpu', null()))) AS avg_sh_cpu, \n    avg(eval(if('data.search_props.role' == \"head\", 'data.read_mb', null()))) AS avg_sh_read_mb, \n    avg(eval(if('data.search_props.role' == \"head\", 'data.written_mb', null()))) AS avg_sh_written_mb, \n    avg(eval(if('data.search_props.role' == \"head\", 'data.pct_memory', null()))) AS avg_sh_pct_mem, \n    max(eval(if('data.search_props.role' == \"peer\", 'data.pct_cpu', null()))) AS max_idx_cpu, \n    max(eval(if('data.search_props.role' == \"peer\", 'data.read_mb', null()))) AS max_idx_read_mb, \n    max(eval(if('data.search_props.role' == \"peer\", 'data.written_mb', null()))) AS max_idx_written_mb, \n    max(eval(if('data.search_props.role' == \"peer\", 'data.pct_memory', null()))) AS max_idx_pct_mem, \n    max(eval(if('data.search_props.role' == \"head\", 'data.pct_cpu', null()))) AS max_sh_cpu, \n    max(eval(if('data.search_props.role' == \"head\", 'data.read_mb', null()))) AS max_sh_read_mb, \n    max(eval(if('data.search_props.role' == \"head\", 'data.written_mb', null()))) AS max_sh_written_mb, \n    max(eval(if('data.search_props.role' == \"head\", 'data.pct_memory', null()))) AS max_sh_pct_mem, \n    BY _time data.search_props.app data.search_props.type, data.search_props.mode, search_id data.search_props.role\n| rex field=search_id \"^(?<sid_caller>\\w+)((_|__)(?<sid_owner>\\w+)(_|__)(?<sid_app>\\w+)__(?<sid_search_name>\\w+))_at\"\n| eval sid_caller = rtrim(sid_caller,\"_\")\n| eval sid_owner = rtrim(sid_owner,\"_\")\n| eval search_type=case(match(sid_caller,\"^(SummaryDirector_|summarize_)\"),\"summarization\",match(sid_caller,\"^((rsa_|rt_|summarize_)?scheduler|alertsmanager)\"),\"scheduled\", match(search_id,\"^_[a-zA-Z0-9]+__\"),\"scheduled\", match(sid_caller,\"^subsearch_\"),\"subsearch\", (isnotnull(sid_caller) AND isnotnull(sid_owner) AND isnotnull(sid_search_name)),\"report\", true(),\"other\")\n| fillnull value=0 avg_idx_cpu avg_sh_cpu avg_idx_pct_mem avg_sh_pct_mem avg_idx_read_mb avg_sh_read_mb avg_idx_written_mb avg_sh_written_mb max_sh_read_mb max_idx_read_mb max_sh_written_mb max_idx_written_mb max_idx_cpu max_idx_pct_mem max_sh_cpu max_sh_pct_mem\n| rename data.search_props.type AS search_props_type, \n    data.search_props.mode AS search_props_mode, \n    data.search_props.app AS app_name\n    data.search_props.role AS splunk_search_role\n    sid_caller AS sid_user\n| eval introspection_time = _time \n| fields introspection_time app_name sid_user sid_search_name sid_app search_type *",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-31m@m",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "-1m@m",
    "cron_schedule": "*/30 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Search_Runtime_By_Type",
    "search": "index=_audit TERM(info=completed) TERM(action=search) \n| fields + search_id, total_run_time, savedsearch_name\n| where savedsearch_name!=\"Bucket Copy Trigger\"\n| eval search_id=trim(search_id,\"'\") \n| rex field=search_id max_match=2 \"(?<search_family>scheduler|subsearch)_\" \n| eval search_family=mvappend(search_family,case(match(search_id,\"^\\\\d{10}\\\\.\\\\d+\"),\"adhoc\",match(search_id,\"^SummaryDirector\"),\"summary\",match(search_id,\"(?<dash_search_name>\\\\w+)_\\\\d{10}\\\\.\\\\d+(?:_[-A-F0-9]+)?$\"),\"dashboard\",true(),search_family)) \n| where search_family!=\"subsearch\"\n| bin span=1d _time\n| stats max(total_run_time) AS total_run_time by search_id search_family _time\n| stats sum(total_run_time) AS total_run_time by search_family _time |eval total_run_time=round(total_run_time,0)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-7d@h",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "now",
    "cron_schedule": "23 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Unused_Lookups",
    "search": "| rest `run_map_for_rest(\"/servicesNS/-/-/saved/searches\")` \n| rex field=search max_match=0 \"\\|\\s*(inputlookup|lookup|inputlookup\\s+append=true)\\s+(?<lookupfilename>\\w+)[\\s,\\.csv]\" \n| fields splunk_server app_name title lookupfilename owner \n| where lookupfilename!=\"\" \n| eval type = \"in_reports\" \n| mvexpand lookupfilename \n| append [| rest `run_map_for_rest(\"/servicesNS/-/-/data/ui/views\")` \n    | xpath field=eai:data outfield=all_the_things \"//search\" \n    | eval all_the_things=mvfilter(NOT match(all_the_things, \"^\\s$\")) \n    | spath input=eai:data output=inputs path=form.fieldset.input \n    | eval inputs=mvfilter(match(inputs, \"<search\")) \n    | fields - eai:data eai:acl.perms eai:acl.perms.* eai:acl.removable eai:acl.can_write eai:acl.modifiable eai:acl.can_share_* eai:acl.can_list eai:acl.can_change_perms eai:type eai:userName \n    | spath input=inputs output=input_searches path=search.query \n    | fields - inputs \n    | makemv tokenizer=\"(?ms)(<search.*?</search>)\" all_the_things \n    | mvexpand all_the_things \n    | spath input=all_the_things output=main_search path=search.query \n    | spath input=all_the_things output=base_name path=search{@id} \n    | rex field=main_search max_match=0 \"\\|\\s*(inputlookup|lookup|inputlookup\\s+append=true)\\s+(?<lookupfilename>\\w+)[\\s,\\.csv]\" \n    | fields splunk_server app_name title lookupfilename owner \n    | where lookupfilename!=\"\" \n    | eval type = \"in_dashboards\" \n    | mvexpand lookupfilename] \n| append [| rest `run_map_for_rest(\"/servicesNS/-/-/data/ui/views\")` \n    | rename eai:type AS view_type, eai:data AS main_search_html \n    | fields - eai:data eai:acl.perms eai:acl.perms.* eai:acl.removable eai:acl.can_write eai:acl.modifiable eai:acl.can_share_* eai:acl.can_list eai:acl.can_change_perms eai:type eai:userName \n    | where view_type==\"html\" \n    | rex field=main_search_html max_match=0 \"\\|\\s*(inputlookup|lookup|inputlookup\\s+append=true)\\s+(?<lookupfilename>\\w+)[\\s,\\.csv]\" \n    | fields splunk_server app_name title lookupfilename owner \n    | where lookupfilename!=\"\" \n    | eval type = \"in_html_dashboards\" \n    | mvexpand lookupfilename] \n| append [| rest `run_map_for_rest(\"/servicesNS/-/-/data/lookup-table-files\")` \n    | rename title AS filename \n    | fields splunk_server app_name filename \n    | append [| rest `run_map_for_rest(\"/servicesNS/-/-/data/props/lookups\")` \n        | fields splunk_server app_name transform stanza owner \n        | join splunk_server app_name trasnform \n            [| rest `run_map_for_rest(\"/servicesNS/-/-/data/transforms/lookups\")` \n            | where type=\"file\" \n            | rename title AS transform \n            | fields splunk_server app_name transform filename owner]] \n    | stats values(stanza) AS stanzas values(transform) AS transform values(owner) AS owner by splunk_server app_name filename \n    | eval auto_lookup = if(isnotnull(stanzas),1,0) \n    | where auto_lookup=1 | rename filename AS lookupfilename, transform AS title \n    | eval type = \"auto_lookup\" \n    | fields splunk_server app_name title lookupfilename type owner] \n| append [| rest `run_map_for_rest(\"/servicesNS/-/-/datamodel/model\")` \n    | rex field=description max_match=0 \"(inputlookup|lookup|inputlookup\\s+append=true)\\s+(?<lookupfilename>\\w+)[\\s,\\.csv]\" \n    | fields splunk_server app_name title lookupfilename \n    | where lookupfilename!=\"\" \n    | eval type = \"in_datamodel\" \n    | mvexpand lookupfilename] \n| append [| rest `run_map_for_rest(\"/servicesNS/-/-/admin/macros\")` \n    | rex field=definition max_match=0 \"(inputlookup|lookup|inputlookup\\s+append=true)\\s+(?<lookupfilename>\\w+)[\\s,\\.csv]\" \n    | fields splunk_server app_name title lookupfilename owner \n    | where lookupfilename!=\"\" \n    | eval type = \"in_macros\" \n    | mvexpand lookupfilename] \n| stats dc(type) AS dc_inuse_type values(owner) AS owner by splunk_server app_name lookupfilename \n| append [| rest `run_map_for_rest(\"/servicesNS/-/-/data/lookup-table-files\")` \n    | rename title AS lookupfilename, eai:acl.sharing AS sharing \n    | eval type = \"all\" \n    | eval dc_inuse_type = 0 \n    | fields splunk_server app_name lookupfilename dc_inuse_type type owner sharing] \n| rex field=lookupfilename \"(?<extracted_lookupfilename>.*)\\.csv\" \n| eval lookupfilename=if(isnotnull(extracted_lookupfilename),extracted_lookupfilename,lookupfilename) \n| eval splunk_default_app = if(((app_name==\"launcher\" AND dc_inuse_type>0) OR app_name==\"splunk_archiver\" OR app_name==\"splunk_monitoring_console\"),1,0) \n| lookup searchheads_lookup _key AS splunk_server OUTPUTNEW searchhead domain_url shc_label label AS sh_label \n| eval splunk_server = if(isnotnull(shc_label) AND shc_label!=\"\",shc_label,splunk_server) \n| where splunk_default_app=0 AND isnotnull(sh_label) \n| stats max(dc_inuse_type) AS dc_inuse_type values(owner) AS owner values(sharing) AS sharing values(type) AS type by splunk_server app_name lookupfilename \n| where dc_inuse_type < 1 \n| rename lookupfilename AS title |eval ko_type=\"lookup\" \n| table app_name title owner splunk_server sharing ko_type \n| collect `kom_summary_indexes` marker=\"search_name=KOM_Unused_Lookups\" source=KOM_Unused_Lookups_Summary addtime=true",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m@m",
    "eai:acl.sharing": "app",
    "description": "List of the lookups which are no longer used.",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "now",
    "cron_schedule": "9 1 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Unused_Macros",
    "search": "| rest `run_map_for_rest(\"/servicesNS/-/-/saved/searches\")` \n| rex field=search max_match=0 \"`(?<macro_name>[\\w,-]+)(\\(|`)\" \n| fields splunk_server app_name title macro_name owner \n| where macro_name!=\"\" \n| eval type = \"in_reports\" \n| mvexpand macro_name \n| append [| rest `run_map_for_rest(\"/servicesNS/-/-/data/ui/views\")` \n    | xpath field=eai:data outfield=all_the_things \"//search\" \n    | eval all_the_things=mvfilter(NOT match(all_the_things, \"^\\s$\")) \n    | spath input=eai:data output=inputs path=form.fieldset.input \n    | eval inputs=mvfilter(match(inputs, \"<search\")) \n    | fields - eai:data eai:acl.perms eai:acl.perms.* eai:acl.removable eai:acl.can_write eai:acl.modifiable eai:acl.can_share_* eai:acl.can_list eai:acl.can_change_perms eai:type eai:userName \n    | spath input=inputs output=input_searches path=search.query \n    | fields - inputs \n    | makemv tokenizer=\"(?ms)(<search.*?</search>)\" all_the_things \n    | mvexpand all_the_things \n    | spath input=all_the_things output=main_search path=search.query \n    | spath input=all_the_things output=base_name path=search{@id} \n    | rex field=main_search max_match=0 \"`(?<macro_name>[\\w,-]+)(\\(|`)\" \n    | fields splunk_server app_name title macro_name macro_name1 owner \n    | where macro_name!=\"\" \n    | eval type = \"in_dashboards\" \n    | mvexpand macro_name] \n| append [| rest `run_map_for_rest(\"/servicesNS/-/-/data/ui/views\")` \n    | rename eai:type AS view_type, eai:data AS main_search_html \n    | where view_type==\"html\" \n    | rex field=main_search_html max_match=0 \"`(?<macro_name>[\\w,-]+)(\\(|`)\" \n    | fields splunk_server app_name title macro_name owner \n    | where macro_name!=\"\" \n    | eval type = \"in_html_dashboards\" \n    | mvexpand macro_name] \n| append [| rest `run_map_for_rest(\"/servicesNS/-/-/admin/macros\")` \n    | rex field=definition max_match=0 \"`(?<macro_name>[\\w,-]+)(\\(|`)\" \n    | fields splunk_server app_name title macro_name owner \n    | where macro_name!=\"\" \n    | eval type = \"in_macros\" \n    | mvexpand macro_name] \n| append [| rest `run_map_for_rest(\"/servicesNS/-/-/datamodel/model\")` \n    | rex field=description max_match=0 \"`(?<macro_name>[\\w,-]+)(\\(|`)\" \n    | fields splunk_server app_name title macro_name \n    | where macro_name!=\"\" \n    | eval type = \"in_datamodel\" \n    | mvexpand macro_name] \n| stats dc(type) AS dc_inuse_type by splunk_server app_name macro_name \n| append [| rest `run_map_for_rest(\"/servicesNS/-/-/admin/macros\")` \n    | rename title AS macro_name, eai:acl.sharing AS sharing \n    | rex field=macro_name \"(?<macro_name>[\\w,-]+)(\\(|$)\" \n    | eval type = \"all\" \n    | eval dc_inuse_type = 0 \n    | fields splunk_server app_name macro_name dc_inuse_type type owner sharing] \n| eval splunk_default_app = if((app_name==\"splunk_archiver\" OR app_name==\"splunk_monitoring_console\"),1,0) \n| lookup searchheads_lookup _key AS splunk_server OUTPUTNEW searchhead domain_url shc_label label AS sh_label \n| eval splunk_server = if(isnotnull(shc_label) AND shc_label!=\"\",shc_label,splunk_server) \n| where splunk_default_app=0 AND isnotnull(sh_label) \n| stats max(dc_inuse_type) AS dc_inuse_type values(owner) AS owner values(sharing) AS sharing by splunk_server app_name macro_name \n| where dc_inuse_type < 1 \n| rename macro_name AS title |eval ko_type=\"macro\" \n| table app_name title owner splunk_server sharing ko_type \n| collect `kom_summary_indexes` marker=\"search_name=KOM_Unused_Macros\" source=KOM_Unused_Macros_Summary addtime=true",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m",
    "eai:acl.sharing": "app",
    "description": "List of the macros which are no longer used.",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "now",
    "cron_schedule": "7 1 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KOM_Users_All",
    "search": "| rest `kom_users_all`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h@h",
    "eai:acl.sharing": "app",
    "description": "Get All Users for audit reporting lookup",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "now",
    "cron_schedule": "*/27 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "KO_Inactive_Scheduled_Searches_Performance_Metrics",
    "search": "|inputlookup kom_asset_map_report |rename eai:acl.sharing AS sharing eai:acl.owner AS owner |fields app_name title splunk_server search is_scheduled is_alert has_report_action user sharing owner\n| eval is_collect=if(match(search,\"((\\|\\s|\\|)collect\\s|(\\|\\s|\\|)mcollect\\s)\"),1,0)\n| eval is_outputlookup=if(match(search,\"((\\|\\s|\\|)outputlookup\\s|(\\|\\s|\\|)outputcsv\\s)\"),1,0)\n| eval sharing=if(isnull(sharing),null(),sharing)\n| eval owner=if(isnull(owner),null(),owner)\n| eval private = if(sharing=\"user\",1,0)\n| eval splunk_default_app = if((app_name==\"splunk_archiver\" OR app_name==\"splunk_monitoring_console\" OR app_name=\"splunk_instrumentation\"),1,0)\n| where splunk_default_app=0 \n| lookup kom_asset_map_dashboard ref_report_name AS title OUTPUTNEW app_name AS dashboard_app_name title AS dashboard_name splunk_server AS dashboard_splunk_server\n| eval is_ref_in_dashboard = if(isnotnull(dashboard_name),1,0)\n| eval active=if(is_outputlookup=1 OR is_collect=1 OR is_alert!=0 OR has_report_action!=0 OR is_ref_in_dashboard=1,1,0)\n| where is_scheduled=1 AND active=0\n| rex field=search max_match=0 \"`(?<macro_name>[\\w,-]+)(\\(|`)\" |fillnull value=\"\" macro_name\n| mvexpand macro_name\n| join type=left macro_name splunk_server [| rest `run_map_for_rest(\"/servicesNS/-/-/admin/macros\")` \n    | eval has_output_cmd = if(match(definition,\"((\\|\\s|\\|)outputlookup\\s|(\\|\\s|\\|)collect\\s|(\\|\\s|\\|)mcollect\\s)|(\\|\\s|\\|)outputcsv\\s\"),1,0)\n    | fields splunk_server app_name title has_output_cmd |rename title AS macro_title\n    | where has_output_cmd==1\n    | rex field=macro_title \"(?<macro_name>[\\w,-]+)(\\(|$)\"\n    | eval has_output_in_macro = 1\n    | fields macro_title macro_name splunk_server has_output_in_macro]\n| eval has_output_in_macro = if(isnull(has_output_in_macro),0,1)\n| join type=left app_name title splunk_server [|search `kom_summary_indexes` source=KOM_Report_Requests_Summary app_name=\"*\" orig_splunk_server IN (*)\n| rename orig_splunk_server AS splunk_server\n| stats max(report_views) AS report_views by app_name search_title report_name splunk_server\n| rename report_name AS title]\n| eval report_views = if(isnull(report_views),0,report_views)\n| stats values(has_output_in_macro) AS has_output_in_macro  max(report_views) AS report_views by splunk_server, title, app_name \n| eval is_viewed = if(report_views>0,1,0)\n| where is_viewed==0 AND has_output_in_macro==0 |rename title AS savedsearch_name\n| join type=left app_name savedsearch_name [ |search (index=_internal host IN (*) sourcetype=scheduler status=*) NOT \"_ACCELERATE*\" run_time=*\n| rename app AS app_name\n| stats dc(sid) AS executions avg(run_time) as avg_runtime sum(run_time) as total_runtime by app_name, savedsearch_name\n| eval avg_runtime=round(avg_runtime,0)\n| fields savedsearch_name, app_name, avg_runtime, total_runtime, executions]\n| fields app_name savedsearch_name total_runtime avg_runtime executions\n| fillnull value=0",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-7d@h",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_kom",
    "dispatch.latest_time": "now",
    "cron_schedule": "53 00 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "License Usage Data Cube",
    "search": "index=_internal source=*license_usage.log* type=\"Usage\" | eval h=if(len(h)=0 OR isnull(h),\"(SQUASHED)\",h) | eval s=if(len(s)=0 OR isnull(s),\"(SQUASHED)\",s) | eval idx=if(len(idx)=0 OR isnull(idx),\"(UNKNOWN)\",idx) | bin _time span=1d | stats sum(b) as b by _time, pool, s, st, h, idx",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-31d",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "search",
    "dispatch.latest_time": "-0d",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "LicenseMaster - Duplicated License Situation",
    "search": "`comment(\"This warning will appear in the messages console of the license master, considering you have 72 hours to fix the issue you might want to be alerted about this so it can be fixed promptly! The scenario is likely to occur when a heavy forwarder is sending data into the indexers without using the forwarder license or talking to the cluster master.\")`\nindex=_internal `licensemasterhost` \"Duplicated License situation happen\" (`splunkadmins_splunkd_source`) \n| cluster showcount=true \n| fields _time, host, source, _raw, cluster_count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. A duplicated licensing situation will normally require intervention to fix, this scenario can happen when a forwarder is not using the forwarder license or license master but is sending data into the indexers...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "27 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Messages by minute last 3 hours",
    "search": "index=_internal source=\"*metrics.log\" eps \"group=per_source_thruput\" NOT filetracker | eval events=eps*kb/kbps | timechart fixedrange=t span=1m limit=5 sum(events) by series",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-3h",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "search",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "MonitoringConsole - Core dumps have appeared on the filesystem",
    "search": "| rest /services/admin/file-explorer/%2Fopt%2Fsplunk count=0 f=name f=lastModifiedTime splunk_server=* \n| search name=*core* \n| eval recent=now()-(7200) \n| where lastModifiedTime>recent \n| sort - lastModifiedTime \n| eval lastModifiedTime=strftime(lastModifiedTime, \"%+\") \n| eval indexer_cluster=`indexer_cluster(splunk_server)` \n| eval search_head=splunk_server \n| eval search_head_cluster=`search_head_cluster` \n| eval env=if(indexer_cluster==splunk_server,search_head_cluster,indexer_cluster) \n| table env, splunk_server, name, title, lastModifiedTime",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. core dumps are normally an issue that should be investigated with Splunk support if they are not a known issue, or dmesg or the journal shows no OS level issues at the time...Note that this can run from any search head, but the monitoring console may make more sense as it has connectivity to all instances",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 2,6,10,14,18,22 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "MonitoringConsole - Crash logs have appeared on the filesystem",
    "search": "index=_internal source=*crash.log \n| stats count by source, host, sourcetype \n| eval indexer_cluster=`indexer_cluster(host)` \n| eval search_head=host \n| eval search_head_cluster=`search_head_cluster` \n| eval env=if(indexer_cluster==host,search_head_cluster,indexer_cluster) \n| table source, host, sourcetype, env",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. cCash logs are normally an issue that should be investigated with Splunk support if they are not a known issue, or dmesg or the journal shows no OS level issues at the time...Note that this can run from any search head, but the monitoring console may make more sense as it has connectivity to all instances",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 2,6,10,14,18,22 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Orphaned scheduled searches",
    "search": "| rest timeout=600 splunk_server=local /servicesNS/-/-/saved/searches add_orphan_field=yes count=0 \n| search orphan=1 disabled=0 is_scheduled=1 \n| eval status = if(disabled = 0, \"enabled\", \"disabled\") \n| fields title eai:acl.owner eai:acl.app eai:acl.sharing orphan status is_scheduled cron_schedule next_scheduled_time next_scheduled_time actions \n| rename title AS \"search name\" eai:acl.owner AS owner eai:acl.app AS app eai:acl.sharing AS sharing",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "search",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Products and the Content Mapped to Them",
    "search": "| inputlookup data_inventory_products_lookup \n| search stage!=step-sourcetype stage!=step-cim \n| rex field=basesearch \"^\\s*(?<basesearch>.*?)\\s*$\"\n| eval basesearch=if(like(basesearch, \"(%\") AND like(basesearch, \"%)\"), basesearch, \"(\" . basesearch . \")\") \n| makemv delim=\"|\" eventtypeId \n| mvexpand eventtypeId \n| rename eventtypeId as data_source_categories \n| join data_source_categories \n    [| sseanalytics \n    | search search_title!=\"\" \n    | stats count count(eval(bookmark_status=\"successfullyImplemented\")) as count_successfullyImplemented values(search_title) as search_title by data_source_categories \n    | eval search_title=mvjoin(search_title, \"|\")] \n| eval Product = vendorName . \" \" . productName\n| makemv delim=\"|\" data_source_categories | mvexpand data_source_categories\n| rex field=data_source_categories \"^(?<ds>[^-]*)\" | sseidenrichment type=datasourceid field=ds | sseidenrichment type=dscid field=data_source_categories | eval data_source_categories = data_source . \" > \" . data_source_category| stats values(*) as * by Product | search Product!=\" \" \n| table Product basesearch data_source_categories metadata_json search_title count \n| spath input=metadata_json | fields - metadata_json\n| table Product basesearch data_source_categories search_title * count \n| rename count as \"Total Mapped Content for This Product\" basesearch as \"Dataset That Provides Visibility\" search_title as \"Saved Search Name\" data_source_categories as \"Data Source Category\" description as Description",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "This expects that you have completed the Data Inventory configuration, and mapped your active content on the Manage Bookmarks page. You will then get a complete view from product to the content that it enables.",
    "eai:acl.app": "Splunk_Security_Essentials",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "RapidDiag Telemetry: CLI access statistics",
    "search": "index=_internal sourcetype=\"splunk_rapid_diag\" splunk_server=\"local\" component=cli_internal token_auth=False mode=* action=* result=* \n| table mode, action , result \n| stats count AS data.count by mode, action, result \n| rename mode as data.mode, action as data.action , result AS data.result \n| makejson data.* output=event",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_rapid_diag",
    "dispatch.latest_time": "",
    "cron_schedule": "10 3 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "RapidDiag Telemetry: Execution statistics",
    "search": "index=_internal sourcetype=\"splunk_rapid_diag\" splunk_server=\"local\" (component=task OR (component=collector internal=False)) name=* status=* \n| table component, name, status \n| eval name=substr(sha256(name),0,12) \n| stats count AS data.count by component, name, status \n| rename status as data.status, name as data.metricName, component AS data.type \n| makejson data.* output=event",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_rapid_diag",
    "dispatch.latest_time": "",
    "cron_schedule": "0 3 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "RapidDiag Telemetry: UI access statistics",
    "search": "index=_internal sourcetype=\"splunkd_ui_access\" app/splunk_rapid_diag/task_manager OR app/splunk_rapid_diag/task_template_wizard OR app/splunk_rapid_diag/data_collection OR app/splunk_rapid_diag/reference_guide method=GET splunk_server=\"local\" \n| table user, uri_path, status \n| stats count AS data.count by user, uri_path, status \n| eval [| rest splunk_server=local /servicesNS/nobody/splunk_instrumentation/telemetry \n| table telemetrySalt \n| format \n| rex field=search mode=sed \"s/[()]//g\"] \n| eval data.user=substr(sha256(telemetrySalt + user),0,12) \n| rename uri_path as data.uri_path, status as data.status \n| makejson data.* output=event",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_rapid_diag",
    "dispatch.latest_time": "",
    "cron_schedule": "5 3 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Saved Searches and the Data that Enables Them",
    "search": "| inputlookup data_inventory_products_lookup \n| search stage!=step-sourcetype stage!=step-cim \n| rex field=basesearch \"^\\s*(?<basesearch>.*?)\\s*$\" \n| eval basesearch=if(like(basesearch, \"(%\") AND like(basesearch, \"%)\"), basesearch, \"(\" . basesearch . \")\") \n| makemv delim=\"|\" eventtypeId \n| mvexpand eventtypeId \n| rename eventtypeId as data_source_categories \n| join data_source_categories \n    [| sseanalytics \n    | search search_title!=\"\" \n    | stats count count(eval(bookmark_status=\"successfullyImplemented\")) as count_successfullyImplemented values(search_title) as search_title by bookmark_status data_source_categories \n    | eval search_title=mvjoin(search_title, \"|\")] \n| eval Product = vendorName . \" \" . productName \n| makemv search_title delim=\"|\" \n| mvexpand search_title\n| stats values(basesearch) as basesearch values(Product) as Product by search_title\n| eval basesearch = \"(\" . mvjoin(basesearch, \" OR \") . \")\", \"Products That Provide Visibility\"=mvjoin(mvfilter(Product!=\" \"), \", \")\n| rename basesearch as \"Dataset That Provides Visibility\" search_title as \"Saved Search Name\"\n| table \"Saved Search Name\" \"Dataset That Provides Visibility\" \"Products That Provide Visibility\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "This expects that you have completed the Data Inventory configuration, and mapped your active content on the Manage Bookmarks page. You will then be get a list oriented to the saved searches, showing each saved search and the data set and products that enable it",
    "eai:acl.app": "Splunk_Security_Essentials",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Accelerated DataModels with All Time Searching Enabled",
    "search": "| rest /servicesNS/-/-/data/models `splunkadmins_restmacro` search=acceleration=1 search=acceleration.earliest_time=0 search=disabled=0 f=eai:data f=eai:acl* \n| fields eai:acl.app, eai:acl.owner, eai:acl.sharing, title, updated \n| rename eai:acl.app AS app, eai:acl.owner AS owner, eai:acl.sharing AS sharing",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Having an accelerated data model running searches every 5 minutes over all time can cause serious issues. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 7 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Accelerated DataModels with wildcard or no index specified",
    "search": "| rest /servicesNS/-/-/datamodel/model splunk_server=local search=eai:type=datamodel f=description f=acceleration f=eai:acl* \n| spath input=description path=objects{}.objectSearch output=objectSearch \n| spath input=acceleration path=enabled output=accelerationEnabled \n| where accelerationEnabled!=\"false\" \n| rename eai:acl.app AS app, eai:acl.sharing AS sharing \n| table splunk_server, title, app, sharing, objectSearch, accelerationEnabled, updated \n| mvexpand objectSearch \n| rex field=objectSearch mode=sed \"s/\\(index=\\* OR index=_\\*\\)/indexwildcard/g\" \n| rex field=objectSearch \"(?P<index>index(\\s*=\\s*\\S+|\\s+IN\\s+\\([^\\)]+))\" \n| where isnull(index) OR match(index, \"\\*\") \n| stats values(objectSearch) AS objectSearch by splunk_server, title, index, app, sharing, accelerationEnabled, updated",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. An accelerated data model searching over all indexes or many indexes can be a a minor issue for the Splunk indexing tier, or a major issue if using smartstore on the indexers in combination with a large number of indexes...the receipt files are stored per-index and each indexer will query for them on deletion (a request flood). Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 7 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Alerts that have not fired an action in X days",
    "search": "`comment(\"Attempt to find alerts that are scheduled but not firing any actions, the alerts may need further review or may no longer be required. The app regex is in here because of some creative alert naming, X:app=Y is a real alert name in my environment!\")`\nindex=_internal source=\"*scheduler.log\" sourcetype=scheduler `searchheadhosts` alert_actions!=\"\" \n| rex \", app=\\\"(?P<app>[^\\\"]+)\\\",\"\n| stats count by savedsearch_name, app \n| append \n    [| rest splunk_server=local /servicesNS/-/-/saved/searches \n    | search actions!=\"summary_index\" actions!=\"\" next_scheduled_time!=\"\" search!=\"| noop\" \n    | table eai:acl.app, title \n    | eval fromRESTQuery=\"\"\n    | rename title as savedsearch_name, eai:acl.app as app ]\n| eventstats count(eval(isnotnull(fromRESTQuery))) AS restCount, count by savedsearch_name, app\n| where restCount=1 AND count=1\n| table savedsearch_name, app",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-30d@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report can be run to determine which alerts have not sent an alert based on the time period / amount of internal logs available...Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Audit log search example only",
    "search": "`comment(\"Query the audit logs for information about earliest/latest time and the search used. However the issue appears to be that apiStart/EndTime can be overriden by earliest/latest keywords *and* the auto-extracted search field isn't accurate and savedsearch_name of search<number> is actually a dashboards (which can be seen through introspection but not through audit searches!\")`\nindex=_audit `searchheadhosts` action=search info=granted search=* NOT \"search='typeahead prefix\"\n| rex \"(?m)search='(?P<thesearch>[\\S\\s]+)',\\s+autojoin=\"\n| table _time, ttl, user, apiStartTime, apiEndTime, earliest, latest, savedsearch_name, thesearch\n| sort - _time",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This is just an example for querying the audit logs provided for reference only",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Captain Switchover Occurring",
    "search": "index=_internal `searchheadhosts` sourcetype=splunkd `splunkadmins_splunkd_source` \"SHCRaftConsensus\" NOT \"failed appendEntriesRequest err\" NOT \"SHCRaftConsensus - NOT_LEADER\" `splunkadmins_captain_switchover`\n| search `comment(\"Exclude the search head shutdown times\")` NOT [`splunkadmins_shutdown_time(searchheadhosts,20,120)`] `comment(\"Exclude manual transfer\")` NOT [`splunkadmins_transfer_captain_times(searchheadhosts,20,120)`]\n| cluster showcount=true\n| fields _time, cluster_count, _raw\n| sort - _time",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-180m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. If the captain has been changed then scheduled searches and alerts will be paused during the switchover, if this is not part of a restart then something is likely wrong...ironically this alert will not run if the issue is actually occurring so that's why the time range window is thee times the runtime of the alert in case it is missed once or twice...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "37 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Dashboard load times",
    "search": "`comment(\"Determine dashboard load times by using the introspection index, note that changes in the timepicker or similar will change your dashboard load times so this is provided as an example query only\")`\nindex=_introspection `searchheadhosts` sourcetype=splunk_resource_usage data.search_props.sid::* \"UI:Dashboard\"\n| eval app = 'data.search_props.app'\n| eval elapsed = 'data.elapsed'\n| eval label = 'data.search_props.label'\n| eval type = 'data.search_props.type'\n| eval mode = 'data.search_props.mode'\n| eval user = 'data.search_props.user'\n| eval provenance='data.search_props.provenance'\n| eval label=coalesce(label, provenance)\n| eval search_head = 'data.search_props.search_head'\n| stats max(elapsed) as runtime earliest(_time) as Started by type, mode, app, user, label, data.pid, search_head, host\n| stats max(runtime) AS totalRuntime by Started, app, user, label\n| sort - Started\n| eval Started=strftime(Started,\"%+\")\n| eval duration = tostring(totalRuntime, \"duration\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report was created to roughly find dashboard load times, if the timepicker for the dashboard is changed then this report will produce different results so this should either be run on a dashboard with consistent time periods or used to roughly measure load times...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Dashboard refresh intervals",
    "search": "| rest splunk_server=local servicesNS/-/-/data/ui/views timeout=600 \n| regex eai:data=\"(<earliest>rt-\\d+[^\\<]+|<refresh>\\d+|refresh=\\\"[^\\\"]+)\" \n| rex field=\"eai:data\" \"(?s)<refresh>(?<refresh_time>\\d+[^\\<]+)<\\/refresh>\" max_match=30 \n| rex field=\"eai:data\" \"(?s)refresh=\\\"(?<refresh_time>[^\\\"]+)\\\"\" max_match=30 \n| rex field=\"eai:data\" \"(?s)<earliest>rt-(?<refresh_time>\\d+[^\\<]+)\" max_match=30 \n| stats values(refresh_type) AS refresh_type by eai:appName, eai:acl.app, eai:acl.sharing, label, title, refresh_time \n| rex field=refresh_time \"^\\d+(?P<refresh_unit>.*)\" \n| eval refresh_type=case(isnotnull(refresh_unit) AND match('eai:data',\"<earliest>rt-\\d+[^\\<]+\"),\"RealTime\",isnotnull(refresh_unit),\"Search\",1=1,\"Form\") \n| addinfo \n| eval refresh_time_seconds=if(isnotnull(refresh_unit),relative_time(info_search_time, \"-\" . refresh_time),refresh_time) \n| eval refresh_time_seconds=if(isnotnull(refresh_unit),floor((refresh_time_seconds-info_search_time)*-1),refresh_time) \n| fields - info_*, refresh_unit",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Credit to Niket Nilay (@niketnilay) with modifications, report on dashboard refresh intervals/realtime refresh intervals",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Dashboards invalid character in splunkd",
    "search": "index=_internal `searchheadhosts` sourcetype=splunkd `splunkadmins_splunkd_source` \"AdminManager\" \"invalid\"\n| rex \"<label>(?P<dashboard_label>[^<]+)\"\n| rex \"(?s)^(\\S+\\s+){3}(?P<error>.*)\"\n| stats count, latest(_time) AS mostrecent, earliest(_time) AS firstseen, values(host) AS hosts, values(dashboard_label) AS dashboard_label by error\n| eval mostrecent=strftime(mostrecent, \"%+\"), firstseen=strftime(firstseen, \"%+\")\n| table count, dashboard_label, error, mostrecent, firstseen",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. One or more invalid character messages appeared in the Splunkd logs. This may require additional investigation.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "11 4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Dashboards using depends and running searches in the background",
    "search": "| rest /servicesNS/-/-/data/ui/views splunk_server=local\n| search eai:acl.app!=splunk_monitoring_console `dashboard_depends_filter1`\n| regex eai:data=\"depends\\s*=\\\"\" \n| fields eai:data, label, title, eai:acl.app \n| rex field=eai:data max_match=20 \"(?s)<row\\s*(depends\\s*=\\\"(?P<rowtokens>[^\\\"]+)|[^>]+depends\\s*=\\\"(?P<rowtokens2>[^\\\"]+))(?P<row>.*?)</row>\" \n| rex field=eai:data max_match=20 \"(?s)<panel\\s*(depends\\s*=\\\"(?P<paneltokens>[^\\\"]+)|[^>]+depends\\s*=\\\"(?P<paneltokens2>[^\\\"]+))(?P<panel>.*?)</panel>\" \n| rex field=eai:data max_match=20 \"(?s)<chart\\s*(depends\\s*=\\\"(?P<charttokens>[^\\\"]+)|[^>]+depends\\s*=\\\"(?P<charttokens2>[^\\\"]+))(?P<chart>.*?)</chart>\" \n| rex field=eai:data max_match=20 \"(?s)<event\\s*(depends\\s*=\\\"(?P<eventtokens>[^\\\"]+)|[^>]+depends\\s*=\\\"(?P<eventtokens2>[^\\\"]+))(?P<event>.*?)</event>\" \n| rex field=eai:data max_match=20 \"(?s)<map\\s*(depends\\s*=\\\"(?P<maptokens>[^\\\"]+)|[^>]+depends\\s*=\\\"(?P<maptokens2>[^\\\"]+))(?P<map>.*?)</map>\" \n| rex field=eai:data max_match=20 \"(?s)<single\\s*(depends\\s*=\\\"(?P<singletokens>[^\\\"]+)|[^>]+depends\\s*=\\\"(?P<singletokens2>[^\\\"]+))(?P<single>.*?)</single>\" \n| rex field=eai:data max_match=20 \"(?s)<table\\s*(depends\\s*=\\\"(?P<tabletokens>[^\\\"]+)|[^>]+depends\\s*=\\\"(?P<tabletokens2>[^\\\"]+))(?P<table>.*?)</table>\" \n| rex field=eai:data max_match=20 \"(?s)<viz\\s*(depends\\s*=\\\"(?P<viztokens>[^\\\"]+)|[^>]+depends\\s*=\\\"(?P<viztokens2>[^\\\"]+))(?P<viz>.*?)</viz>\" \n| rex field=row \"(?s)(?P<rowsearch><search[^>]*>.*?</search>)\" \n| rex field=panel \"(?s)(?P<panelsearch><search[^>]*>.*?</search>)\" \n| rex field=chart \"(?s)(?P<chartsearch><search[^>]*>.*?</search>)\" \n| rex field=event \"(?s)(?P<eventsearch><search[^>]*>.*?</search>)\" \n| rex field=map \"(?s)(?P<mapsearch><search[^>]*>.*?</search>)\" \n| rex field=single \"(?s)(?P<singlesearch><search[^>]*>.*?</search>)\" \n| rex field=table \"(?s)(?P<tablesearch><search[^>]*>.*?</search>)\" \n| rex field=viz \"(?s)(?P<vizsearch><search[^>]*>.*?</search>)\" \n| eval rowsearchfiltered=mvfilter(match(rowsearch,\"<search.*?\\s+depends\\s*=\")) \n| eval panelsearchfiltered=mvfilter(match(panelsearch,\"<search.*?\\s+depends\\s*=\")) \n| eval chartsearchfiltered=mvfilter(match(chartsearch,\"<search.*?\\s+depends\\s*=\")) \n| eval eventsearchfiltered=mvfilter(match(eventsearch,\"<search.*?\\s+depends\\s*=\")) \n| eval mapsearchfiltered=mvfilter(match(mapsearch,\"<search.*?\\s+depends\\s*=\")) \n| eval singlesearchfiltered=mvfilter(match(singlesearch,\"<search.*?\\s+depends\\s*=\")) \n| eval tablesearchfiltered=mvfilter(match(tablesearch,\"<search.*?\\s+depends\\s*=\")) \n| eval vizsearchfiltered=mvfilter(match(vizsearch,\"<search.*?\\s+depends\\s*=\")) \n| where (isnotnull(rowsearch) AND (isnotnull(rowsearchfiltered) AND mvcount(rowsearch)!=mvcount(rowsearchfiltered)) OR (isnull(rowsearchfiltered))) \n    OR (isnotnull(panelsearch) AND (isnotnull(panelsearchfiltered) AND mvcount(panelsearch)!=mvcount(panelsearchfiltered)) OR (isnull(panelsearchfiltered)))\n    OR (isnotnull(chartsearch) AND (isnotnull(chartsearchfiltered) AND mvcount(chartsearch)!=mvcount(chartsearchfiltered)) OR (isnull(chartsearchfiltered)))\n    OR (isnotnull(eventsearch) AND (isnotnull(eventsearchfiltered) AND mvcount(eventsearch)!=mvcount(eventsearchfiltered)) OR (isnull(eventsearchfiltered))) \n    OR (isnotnull(mapsearch) AND (isnotnull(mapsearchfiltered) AND mvcount(mapsearch)!=mvcount(mapsearchfiltered)) OR (isnull(mapsearchfiltered)))\n    OR (isnotnull(singlesearch) AND (isnotnull(singlesearchfiltered) AND mvcount(singlesearch)!=mvcount(singlesearchfiltered)) OR (isnull(singlesearchfiltered)))\n    OR (isnotnull(tablesearch) AND (isnotnull(tablesearchfiltered) AND mvcount(tablesearch)!=mvcount(tablesearchfiltered)) OR (isnull(tablesearchfiltered)))\n    OR (isnotnull(vizsearch) AND (isnotnull(vizsearchfiltered) AND mvcount(vizsearch)!=mvcount(vizsearchfiltered)) OR (isnull(vizsearchfiltered))) \n| eval rowtokens=coalesce(rowtokens,rowtokens2)\n| nomv rowtokens\n| eval rowtokens=replace(rowtokens,\"\\$\",\"\\\\$\")\n| makemv tokenizer=(\\S+) rowtokens\n| eval rowtokens=split(rowtokens,\",\") \n| eval paneltokens=coalesce(paneltokens,paneltokens2) \n| nomv paneltokens\n| eval paneltokens=replace(paneltokens,\"\\$\",\"\\\\$\")\n| makemv tokenizer=(\\S+) paneltokens\n| eval charttokens=coalesce(charttokens,charttokens2)\n| nomv charttokens\n| eval charttokens=replace(charttokens,\"\\$\",\"\\\\$\")\n| makemv tokenizer=(\\S+) charttokens\n| eval charttokens=split(charttokens,\",\") \n| eval eventtokens=coalesce(eventtokens,eventtokens2)\n| nomv eventtokens\n| eval eventtokens=replace(eventtokens,\"\\$\",\"\\\\$\")\n| makemv tokenizer=(\\S+) eventtokens\n| eval eventtokens=split(eventtokens,\",\") \n| eval maptokens=coalesce(maptokens,maptokens2)\n| nomv maptokens\n| eval maptokens=replace(maptokens,\"\\$\",\"\\\\$\")\n| makemv tokenizer=(\\S+) maptokens\n| eval maptokens=split(maptokens,\",\") \n| eval singletokens=coalesce(single,single2)\n| nomv singletokens\n| eval singletokens=replace(singletokens,\"\\$\",\"\\\\$\")\n| makemv tokenizer=(\\S+) singletokens\n| eval singletokens=split(singletokens,\",\") \n| eval tabletokens=coalesce(tabletokens,tabletokens2)\n| nomv tabletokens\n| eval tabletokens=replace(tabletokens,\"\\$\",\"\\\\$\")\n| makemv tokenizer=(\\S+) tabletokens\n| eval tabletokens=split(tabletokens,\",\") \n| eval viztokens=coalesce(viztokens,viztokens2)\n| nomv viztokens\n| eval viztokens=replace(viztokens,\"\\$\",\"\\\\$\")\n| makemv tokenizer=(\\S+) viztokens\n| eval viztokens=split(viztokens,\",\") \n| search `comment(\"This would be more accurate as such but make the search really, really slow...so combining into 1 large value...\n| streamfilter fieldname=rowtoken_matches pattern=rowtokens rowsearch\n| streamfilter fieldname=paneltoken_matches pattern=paneltokens panelsearch\n| streamfilter fieldname=chartoken_matches pattern=charttokens chartsearch\n| streamfilter fieldname=eventtoken_matches pattern=eventtokens eventsearch\n| streamfilter fieldname=maptoken_matches pattern=maptokens mapsearch\n| streamfilter fieldname=singletoken_matches pattern=singletokens singlesearch\n| streamfilter fieldname=tabletoken_matches pattern=tabletokens tablesearch\n| streamfilter fieldname=viztoken_matches pattern=viztokens vizsearch\n\")`\n| eval combined=mvappend(rowsearch, panelsearch, chartsearch, eventsearch, mapsearch, singlesearch, tablesearch, vizsearch)\n| eval combinedtokens=mvappend(rowtokens, paneltokens, charttokens, eventtokens, maptokens, singletokens, tabletokens, viztokens)\n| eval counter=mvcount(combinedtokens) \n| where counter>0 \n| `dashboard_depends_filter2` \n| streamfilter fieldname=combinedtoken_matches pattern=combinedtokens combined\n| makemv tokenizer=(\\S+) combinedtoken_matches\n| eval counter2=mvcount(combinedtoken_matches)\n| where (isnotnull(combinedtoken_matches) AND mvcount(combinedtokens)!=mvcount(combinedtoken_matches)) OR isnull(combinedtoken_matches)\n| rename eai:acl.app AS app\n| `dashboard_depends_filter3` \n| table title, label, app, eai:data, combinedtokens, combinedtoken_matches",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. For any depends= attributes in a dashboard, check if the searches below this level also depend on the token (in case they are background loading/searching even when not visible). Note that this search utilises the streamfilterwildcard custom search command included in the TA-Alerts for SplunkAdmins application on SplunkBase (or github)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Dashboards using special characters",
    "search": "| rest /servicesNS/-/-/data/ui/views splunk_server=local f=eai:data f=label f=title f=eai:* timeout=600 \n| fields eai:data, label, title, eai:acl.app, eai:acl.owner, eai:acl.sharing \n| search NOT (eai:acl.app=trackme AND title=TrackMe) NOT (title=available_icons AND eai:acl.app=network-diagram-viz)\n| rex field=eai:data max_match=20 \"(?s)<row[^>]*>(?P<row>.*?)</row>\" \n| rex field=eai:data max_match=20 \"(?s)<panel[^>]*>(?P<panel>.*?)</panel>\" \n| rex field=eai:data max_match=20 \"(?s)<chart[^>]*>(?P<chart>.*?)</chart>\" \n| rex field=eai:data max_match=20 \"(?s)<event[^>]*>(?P<event>.*?)</event>\" \n| rex field=eai:data max_match=20 \"(?s)<map[^>]*>(?P<map>.*?)</map>\" \n| rex field=eai:data max_match=20 \"(?s)<single[^>]*>(?P<single>.*?)</single>\" \n| rex field=eai:data max_match=20 \"(?s)<table[^>]*>(?P<table>.*?)</table>\" \n| rex field=eai:data max_match=20 \"(?s)<viz[^>]*>(?P<table>.*?)</viz>\" \n| rex field=row \"(?s)(?P<rowsearch><search[^>]*>.*?</search>)\" \n| rex field=panel \"(?s)(?P<panelsearch><search[^>]*>.*?</search>)\" \n| rex field=chart \"(?s)(?P<chartsearch><search[^>]*>.*?</search>)\" \n| rex field=event \"(?s)(?P<eventsearch><search[^>]*>.*?</search>)\" \n| rex field=map \"(?s)(?P<mapsearch><search[^>]*>.*?</search>)\" \n| rex field=single \"(?s)(?P<singlesearch><search[^>]*>.*?</search>)\" \n| rex field=table \"(?s)(?P<tablesearch><search[^>]*>.*?</search>)\" \n| rex field=viz \"(?s)(?P<vizsearch><search[^>]*>.*?</search>)\" \n| eval combined=mvappend(rowsearch, panelsearch, chartsearch, eventsearch, mapsearch, singlesearch, tablesearch, vizsearch) \n| nomv combined \n| where isnotnull(combined) \n| regex combined!=\"(?s)^[\\\\\\d\\s\\w\\|`\\\"\\*\\(\\)\\[\\]\\+=\\-;:!,\\./%\\?<>{}^#$@'&~]+$\" \n| rex field=combined \"(?s)(?P<before_special_character>^[\\\\\\d\\s\\w\\|`\\\"\\*\\(\\)\\[\\]\\+=\\-;:!,\\./%\\?<>{}^#$@'&~]*)(?P<special_character>.)\" \n| rename eai:acl.app AS app, eai:acl.sharing AS sharing, eai:acl.owner AS owner \n| table title, label, app, sharing, owner, special_character, before_special_character",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Find special characters in dashboard searches, they are often copy & pasted in from another application and break searches",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Dashboards with all time searches set",
    "search": "| rest splunk_server=local /servicesNS/-/-/data/ui/views timeout=600 \n| search `comment(\"While it will be more accurate to look at the audit logs to see who is using all time in the earliest/latest fields, this is an attempt to identify dashboards that do not have a <earliest> field *or* an earliest= within the search query. There is likely room for improvement in this query but it appears to work so far...\")` \n| search NOT ((eai:acl.app=\"splunk_simple_xml_examples\" OR eai:acl.app=splunk_app_windows_infrastructure) AND eai:acl.owner=\"nobody\") \n| rex field=eai:data \"(?s)<input\\s*(?P<time_input>[^>]*type\\s*=\\s*\\\"time[^>]+)>\" \n| eval has_global_time_picker=if(match(time_input, \"token\\s*=\"),null(),if(isnotnull(time_input),true(),null())) \n| where isnull(has_global_time_picker) \n| rex field=eai:data max_match=500 \"(?s)<searc(?P<base>h[^>]*)>(?P<search>.*?)</search>\" \n| eval combined = mvzip(base, search, \"%%%%%%%%%%\") \n| search `comment(\"From the data, find tokens, if the token includes an earliest= value, find it and store it into token_name2\")` \n| multireport \n    [| xpath field=eai:data \"//input\" outfield=input \n    | eval input=mvfilter(match(input,\"token\\s*=\\s*\")) \n    | xpath field=input \"//@token\" outfield=token \n    | xpath field=input \"//input\" outfield=tokenremainder \n    | makemv token tokenizer=(\\S+) \n    | eval token_combined=mvzip(token, tokenremainder, \"%%%%%%%%%%\") \n    | eval token_combined=mvfilter(match(token_combined,\"earliest\\s*=\\s*\"))\n    | eval token_name2=mvindex(split(token_combined, \"%%%%%%%%%%\"),0)\n    | stats count, values(token_name2) AS time_tokens by eai:acl.app, eai:acl.sharing, eai:appName, combined, label, title, eai:acl.owner, updated ] \n    [| stats count, values(token_name2) AS time_tokens by eai:acl.app, eai:acl.sharing, eai:appName, combined, label, title, eai:acl.owner, updated ] \n| stats count, values(time_tokens) AS time_tokens by eai:acl.app, eai:acl.sharing, eai:appName, combined, label, title, eai:acl.owner, updated\n| eventstats values(time_tokens) AS time_tokens by eai:acl.app, eai:acl.sharing, eai:appName, label, title, eai:acl.owner, updated\n| eval split=split(combined,\"%%%%%%%%%%\") \n| eval base=mvindex(split,0) \n| eval search=mvindex(split,1) \n| fields eai:acl.app, eai:acl.sharing, eai:appName, search, base, label, title, updated, eai:acl.owner, time_tokens\n| where NOT match(base,\"(base=|ref=)\") AND match(search, \"<query>\") \n| eval splunk_server = `splunkadmins_splunk_server_name`\n| `splunkadmins_audit_logs_macro_sub_v8` \n| `splunkadmins_audit_logs_macro_sub_v8` \n| rex field=search \"(?s)<query>(?P<query>.*?)</query>\" \n| rex field=query \"earliest\\s*=\\s*(?P<earliest>\\s*\\S+\\s)\" \n| where isnull(earliest)\n| eval hassearch=if(match(query, \"(?s)^\\s*\\|\\s*search\\s+\"),1,0) \n| where hassearch==0 AND NOT match(query, \"(?s)^\\s*\\||^\\s*<!\\[CDATA\\[\\s*\\|\") \n| regex search!=\"(?s)<earliest>.*?</earliest>\"\n| rex field=query \"\\$(?P<token>[^\\$]+)\\$\" max_match=50\n| nomv time_tokens\n| search `comment(\"If you run pre-Splunk 8.0.x then you will need to mvexpand at this point instead, then perhaps a stats values(*) AS * by eai:acl.app, eai:acl.sharing, label, title, updated, eai:acl.owner or similar...\")` \n| eval matches=mvmap(token,if(match(time_tokens,\"(^|\\s+)\" . token . \"(\\s+|$)\"),\"true\",null()))\n| where isnull(matches)\n| stats count, values(search) AS search_examples by eai:acl.app, eai:acl.sharing, label, title, updated, eai:acl.owner\n| rename eai:acl.app AS app, eai:acl.sharing AS sharing, eai:acl.owner AS owner, title AS label\n| table label, app, sharing, updated, owner, search_examples",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. This alert is designed to highlight dashboards that have no earliest/latest within a <search> element, and no global time picker defined, therefore it is likely that using this dashboard would result in all time searches running. The more accurate search is \"SearchHeadLevel - audit logs showing all time searches\". For macro substitution to work the splunkadmins_macros lookup file needs to exist. Note this is likely to generate some false alarms, I have attempted to cater for earliest= within tokens",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "7 3 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Data Model Acceleration Completion Status",
    "search": "| rest /services/admin/summarization by_tstats=t splunk_server=local count=0 \n| search `comment(\"Found on https://answers.splunk.com/answers/555005/how-to-check-the-percent-of-the-dm-acceleration-co.html \")`\n| eval datamodel=replace('summary.id',\"DM_\".'eai:acl.app'.\"_\",\"\") \n| join type=left datamodel \n  [| rest /services/data/models splunk_server=local count=0 \n  | table title acceleration.cron_schedule eai:digest \n  | rename title as datamodel \n  | rename acceleration.cron_schedule AS cron] \n| table datamodel eai:acl.app summary.access_time summary.is_inprogress summary.size summary.latest_time summary.complete summary.buckets_size summary.buckets cron summary.last_error summary.time_range summary.id summary.mod_time eai:digest summary.earliest_time summary.last_sid summary.access_count \n| rename summary.id AS summary_id, summary.time_range AS retention, summary.earliest_time as earliest, summary.latest_time as latest, eai:digest as digest \n| rename summary.* AS *, eai:acl.* AS * \n| sort datamodel",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. The % complete of the data model which is stored on the indexer level but run from the search head level...refer to the data model dashboards for more detailed information. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - DataModel Fields",
    "search": "| rest /servicesNS/-/-/data/models splunk_server=local | table title \n| map maxsearches=30 search=\" \n|datamodel \\\"$title$\\\" \n| spath output=model_name path=modelName \n| spath output=foo path=objects{} \n| mvexpand foo \n| spath input=foo output=object_name path=objectName \n| spath input=foo output=bar path=calculations{} \n| spath input=foo output=foo path=fields{} \n| table model_name,object_name,bar,foo \n| eval foobar = mvappend(foo,bar) \n| table model_name, object_name,foobar \n| mvexpand foobar \n| spath input=foobar output=field_name path=fieldName \n| spath input=foobar output=field_type path=type \n| spath input=foobar output=calc_type path=outputFields{}.type \n| spath input=foobar output=calc_name path=outputFields{}.fieldName \n| spath input=foobar output=field_desc path=comment{}.description \n| spath input=foobar output=calc_desc path=outputFields{}.comment{}.description \n| spath input=foobar output=field_recommended path=comment{}.recommended \n| spath input=foobar output=calc_recommended path=outputFields{}.comment{}.recommended \n| eval method=if(isnotnull(field_name),\\\"field\\\",\\\"eval\\\"), name=coalesce(field_name,calc_name), type=coalesce(field_type,calc_type), desc=coalesce(field_desc, calc_desc), recommended=coalesce(field_recommended, calc_recommended) \n| fillnull value=\"false\" recommended \n| fields model_name, object_name, name, recommended, method, type, desc\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report prints out datamodel fields, found on slack thanks to Dave Shpritz (@automine)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - DataModels report",
    "search": "| rest /servicesNS/-/-/datamodel/model splunk_server=local search=eai:type=datamodel \n| table title, description, eai:acl.sharing, eai:acl.app \n| spath input=description path=objects{}.objectSearch output=objectSearch \n| fields - description \n| eval objectSearch=mvindex(objectSearch,0) \n| rex field=objectSearch \"^\\s*\\|?(?P<definition>[^\\|]+)\" \n| rename title AS datamodel, eai:acl.sharing AS sharing, eai:acl.app AS app \n| table datamodel, sharing, app, definition \n| eval splunk_server=\"default\" \n| outputlookup splunkadmins_datamodels",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is required to support the audit log summary searches. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "50 5 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Detect Excessive Search Use - Dashboard - Automated",
    "search": "`comment(\"Based on contents of Detect Excessive Search Use, with the addition of narrowing down to dashboard loads with more than 1 user. From there, attempt to auto-run the introspection index query to determine which dashboards may be involved and which apps. Finally this finishes with a sendresults email to the customer to advise them to consider changing their dashboard to use scheduled saved searches\")`\nindex=_audit info=granted \"search='\" NOT \"savedsearch_name=\\\"Threat - Correlation Searches - Lookup Gen\\\"\" NOT \"savedsearch_name=\\\"Bucket Copy Trigger\\\"\" NOT \"search='| copybuckets\" NOT \"search='search index=_telemetry sourcetype=splunk_telemetry | spath\" NOT \"savedsearch_name=\\\"_ACCELERATE_*\"\n| rex \"(?s), search='(?P<search>.*)\\]$\" \n| rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n| regex search!=\"\\|\\s+(rest|inputlookup|makeresults|tstats count AS \\\"Count of [^\\\"]+\\\"\\s+ from sid=)\"\n| rex \"apiEndTime='[^,]+, savedsearch_name=\\\"(?P<savedsearch_name>[^\\\"]+)\"\n| eval apiEndTime=strptime(apiEndTime, \"'%a %B %d %H:%M:%S %Y'\"), apiStartTime=strptime(apiStartTime, \"'%a %B %d %H:%M:%S %Y'\")\n| eval timePeriod=apiEndTime-apiStartTime\n| bin _time span=10m\n| eval search_id=substr(search_id,2,len(search_id)-2)\n| stats count, values(host) AS hostList, values(savedsearch_name) AS savedSearchName, values(ttl) AS ttl by search, user, _time, timePeriod\n| eval frequency = ceil((10*60)/timePeriod)\n| fillnull frequency\n| where count>4 AND count>frequency\n| eval timePeriod=tostring(timePeriod,\"duration\")\n| stats sum(count) AS count, max(count) AS \"maxCountPerSpan\", values(user) AS userList, values(hostList) AS hostList, values(savedSearchName) AS savedSearchName, earliest(_time) AS firstSeen, latest(_time) AS mostRecent by search\n| where mvcount(userList) > 2 AND match(savedSearchName, \"^search\\d+\")\n| stats values(sids) AS sids, max(count) AS count, max(maxCountPerSpan) AS maxCountPerSpan by firstSeen, mostRecent, userList, hostList\n| stats values(userList) AS userList, values(hostList) AS hostList by firstSeen, mostRecent, count, maxCountPerSpan\n| addinfo\n| eval searchDuration = tostring(info_max_time - info_min_time, \"duration\")\n| sort - count\n| head 20\n| append [ | makeresults | eval userList=\"nonexistentuser\", loadCount=0, searchDuration=\"10\", count=\"0\" | fields - _time ]\n| map\n    [ search index=_introspection `indexerhosts` sourcetype=splunk_resource_usage\n        [| makeresults\n        | eval data.search_props.user=$userList$\n        | makemv data.search_props.user\n        | mvexpand data.search_props.user\n        | return 20 data.search_props.user ]\n    | eval users=$userList$, loadCount=$count$, searchDuration=$searchDuration$\n    | stats count, values(users) AS userList, values(loadCount) AS loadCount, values(searchDuration) AS searchDuration by data.search_props.provenance, data.search_props.app\n    | search data.search_props.provenance=UI:Dashboard*\n    | rename data.search_props.provenance AS provenance, data.search_props.app AS app ] maxsearches=20\n| search `comment(\"Exclusions lists apply at this point as we have app/dashboard context\")` NOT [ | inputlookup dashboard_automated_app_exclusion.csv ]  NOT [ | inputlookup dashboard_automated_app_history.csv | makemv searchInfo tokenizer=(\\S+) | mvexpand searchInfo | rename searchInfo AS provenance | fields - currtime ]\n| makemv userList\n| eval userCount = mvcount(userList)\n| mvexpand userList\n| search `comment(\"Beyond this point this search will likely need customisation to work in a particular environment, remove the comments and \\ symbols to make this work...\n| ldapfilter search=\\\"(&(CN=$userList$)(objectClass=organizationalPerson))\\\" attrs=\\\"mail\\\"\n| where isnotnull(mail)\n\")`\n| stats values(provenance) AS searchInfo, values(searchDuration) AS searchDuration, values(loadCount) AS loadCount, values(mail) AS email_to, max(userCount) AS maxUsersPerDashboard by app\n| nomv email_to\n| rex mode=sed field=email_to \"s/ /;/g\"\n| search `comment(\"\n| sendresults subject=\\\"Shared dashboards not using saved searches\\\" msgstyle=\\\"table {font-family:Arial;font-size:12px;border: 1px solid black;padding:3px}th {background-color:#AAAAAA;color:#fff;border-left: solid 1px #e9e9e9} td {border:solid 1px #e9e9e9}\\\" showemail=f showsubj=f\n| fields app, searchInfo\n| eval currtime=now()\n| outputlookup dashboard_automated_app_history.csv append=true\n\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Based on the Detect Excessive Search Use dashboard, attempt to automate detection of dashboards loaded by multiple users",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Detect MongoDB errors",
    "search": "`comment(\"The main goal of this alert errors which might not appear in splunkd.log but are critical to keeping the kvstore running on the search heads. Please check the mongod.log file for further information, the additional count field is simply determining that mongo is still logging...\")`\n`comment(\"Attempt to find errors in the mongod log and make sure the errors do not relate to shutdown events in the search head cluster. Since this does will ignore any events when either cluster shutsdown it might not be sensitive enough for some use cases...\")`\nindex=_internal `searchheadhosts` `splunkadmins_mongo_source` (\" E \" OR \" F \" OR \" W \") `comment(\"https://jira.mongodb.org/browse/SERVER-42078 advises this is harmless\")` NOT \"update of non-mod failed\" `splunkadmins_mongodb_errors`\n| regex _raw=\"^\\s+?\\S+\\s+[EF]\" \n| search `comment(\"Exclude time periods where shutdowns were occurring\")` AND NOT [`splunkadmins_shutdown_time(searchheadhosts,60,60)`]\n| eventstats max(_time) AS mostRecent, min(_time) AS firstSeen by host\n| bin _time span=10m \n| stats values(_raw) AS logMessages, max(mostRecent) AS mostRecent, min(firstSeen) AS firstSeen by _time, host \n| search `comment(\"One final symptom that appears when mongodb is dead is the logging just stops, zero data, however this proved to be tricky in Splunk so the below query uses a few tricks to ensure the data will show zero values even if the server stops reporting. timechart was recommended by splunkanswers as it creates a timebucket with null values if no data is found...\")`\n| append \n    [ | tstats count where index=_internal `searchheadhosts` `splunkadmins_mongo_source` by host, _time span=5m \n    | search `searchheadhosts`\n    | timechart limit=0 partial=f span=5m sum(count) AS count by host \n    | fillnull \n    | untable _time, host, count \n    | stats max(_time) AS mostRecent, min(_time) AS firstSeen, last(count) AS lastCount by host \n    | where lastCount=0 \n    | eval logMessages=\"Zero log entries found at this time, mongod might not be running, please investigate\" \n    | fields - lastCount] \n| eval mostRecent = strftime(mostRecent, \"%+\"), firstSeen=strftime(firstSeen, \"%+\")\n| fields _time, host, firstSeen, mostRecent, logMessages\n| search `comment(\"Just in case...\")` `splunkadmins_mongodb_errors2`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. If there are errors in the mongo log files on a search head cluster (unrelated to restarts) then this might indicate a kvstore issue which needs attention",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "53 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Detect changes to knowledge objects",
    "search": "`comment(\"Attempt to determine what changes to knowledge or creation of new knowledge objects on a per-application/type basis. Using splunkd*access logs due to lack of information in other logs in non-clustered search heads. Everything works fine unless the /services/ endpoint is used in which case we have no idea which app owned the updated item and we have to just assume it could be any app. Also the query is super-complicated because Splunk often provides 3 endpoints to edit the same config, it also allows // or similar in the URL. Also refer to the alternative queries for SearchHeadLevel - Detect changes to knowledge objects directory, and SearchHeadLevel - Detect changes to knowledge objects non-directory\")`\nindex=_internal sourcetype=splunkd_access OR sourcetype=splunkd_ui_access method=POST status=200 OR status=201 NOT \"/manager/\" NOT \"/dispatch HTTP\"\n| rex field=uri \"/servicesNS/[^/]+/(?P<app>[^/]+)\" \n| eval type=case(match(uri,\"/data/+props/+calcfields($|/)\"),\"calcfields\",match(uri,\"/saved/+searches($|/)\"),\"savedsearch\",match(uri,\"/admin/+savedsearch($|/)\"),\"savedsearch\",match(uri,\"/configs/+conf-savedsearches\"),\"savedsearch\",match(uri,\"/data/+ui/+views($|/)\"),\"dashboards\",match(uri,\"/+admin/+views($|/)\"),\"dashboards\",match(uri,\"/data/+props/+fieldaliases($|/)\"),\"fieldaliases\",match(uri,\"/+admin/+fieldaliases(/|$)\"),\"fieldaliases\",match(uri,\"data/+props/+extractions\"),\"fieldextractions\",match(uri,\"/+admin/+props-extract($|/)\"),\"fieldextractions\",match(uri,\"/data/+transforms/+extractions($|/)\"),\"fieldtransformations\",match(uri,\"/+admin/+transforms-extract($|/)\"),\"fieldtransformations\",match(uri,\"data/+ui/+workflow-actions($|/)\"),\"workflow-actions\",match(uri,\"/+admin/+workflow-actions($|/)\"),\"workflow-actions\",match(uri,\"/+configs/+conf-workflow_actions($|/)\"),\"workflow-actions\",match(uri,\"/+configs/+conf-props($|/)\"),\"props*\",match(uri,\"/+configs/+conf-transforms($|/)\"),\"transforms*\",match(uri,\"/+data/+props/+sourcetype-rename($|/)\"),\"sourcetype-renaming\",match(uri,\"/+admin/+sourcetype-rename($|/)\"),\"sourcetype-renaming\",match(uri,\"/+admin/+tags($|/)\"),\"tags\",match(uri,\"/+saved/+(n|fv)tags($|/)\"),\"tags\",match(uri,\"/+configs/+conf-tags($|/)\"),\"tags\",match(uri,\"/+saved/+eventtypes($|/)\"),\"eventtypes\",match(uri,\"/+admin/+eventtypes($|/)\"),\"eventtypes\",match(uri,\"/+configs/+conf-eventtypes($|/)\"),\"eventtypes_conf\",match(uri,\"/+data/+ui/+nav($|/)\"),\"navMenu\",match(uri,\"/+admin/*nav($|/)\"),\"navMenu\",match(uri,\"/+datamodel/+model($|/)\"),\"datamodel\",match(uri,\"/+configs/+conf-datamodels($|/)\"),\"datamodel\",match(uri,\"/+admin/+datamodel-files($|/)\"),\"datamodel\",match(uri,\"/+admin/+datamodeledit($|/)\"),\"datamodel\",match(uri,\"/+storage/+collections/+config($|/)\"),\"kvstore\",match(uri,\"/+configs/+conf-collections($|/)\"),\"kvstore\",match(uri,\"/+admin/+collections-conf($|/)\"),\"kvstore\",match(uri,\"/+data/+ui/+times($|/)\"),\"times\",match(uri,\"/+configs/+conf-times($|/)\"),\"times\",match(uri,\"/+admin/+conf-times($|/)\"),\"times\",match(uri,\"/+data/+ui/+panels($|/)\"),\"panels\",match(uri,\"/+configs/+conf-panels($|/)\"),\"panels\",match(uri,\"/+data/+props/+lookups($|/)\"),\"lookup-definition\",match(uri,\"/+admin/+props-lookup($|/)\"),\"lookup-definition\",match(uri,\"/+data/+transforms/+lookups($|/)\"),\"automaticlookup\",match(uri,\"/+admin/+transforms-lookup($|/)\"),\"automaticlookup\",match(uri,\"/+admin/+macros($|/)\"),\"macros\",match(uri,\"/+configs/+conf-macros($|/)\"),\"macros\",match(uri,\"/+data/+macros($|/)\"),\"macros\",1==1,\"unknown\")\n| where type!=\"unknown\"\n| eval app=if(isnull(app),\"*\",app)\n| eval type2 = if(type==\"eventtypes\",\"tags\",null())\n| search `comment(\"note that eventtypes can have tags created so assume eventtype == tag creation, eventtypes_conf doesn't appear to work\")`\n| stats values(_time) AS times, values(user) AS userList, values(type2) AS type2 by type, app",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-7d@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Attempt to determine what times knowledge object config was changed by using splunk access logs (information not available in audit logs at the time of testing). Also refer to SearchHeadLevel - Detect changes to knowledge objects directory/non-directory",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Detect changes to knowledge objects directory",
    "search": "| rest \"/services/directory\" count=0 `splunkadmins_restmacro`\n| table updated, eai:type, eai:acl.app, eai:location\n| eval updatedEpoch=strptime(updated,\"%Y-%m-%dT%H:%M:%S%:z\")\n| rename eai:type AS type, eai:acl.app AS app, eai:location AS location\n| stats count by type, app\n| fields - count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-7d@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Attempt to determine what times knowledge object config was changed by using the directory REST API endpoint, note this does not cover all knowledge objects, also refer to SearchHeadLevel - Detect changes to knowledge objects non-directory",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Detect changes to knowledge objects non-directory",
    "search": "| rest splunk_server=local /servicesNS/-/-/storage/collections/config count=0 f=updated f=eai:appName\n| eval type=\"kvstore\"\n| append [ | rest splunk_server=local /servicesNS/-/-/datamodel/model count=0 f=updated f=eai:appName | eval type=\"datamodel\" ]\n| append [ | rest splunk_server=local /servicesNS/-/-/data/ui/panels count=0 f=updated f=eai:appName | eval type=\"panels\" ]\n| append [ | rest splunk_server=local /servicesNS/-/-/data/props/calcfields count=0 | table updated, eai:acl.app | rename eai:acl.app AS eai:appName | eval type=\"calcfields\" ]\n| eval updatedEpoch=strptime(updated,\"%Y-%m-%dT%H:%M:%S%:z\")\n| rename eai:appName AS app\n| stats count by type, app",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-7d@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Attempt to determine what times knowledge object config was changed by using the directory REST API endpoint, note this does not cover all knowledge objects, also refer to SearchHeadLevel - Detect changes to knowledge objects directory",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Detect searches hitting corrupt buckets",
    "search": "`comment(\"Note this requires further testing due to switch to splunk_search_messages\")`\n`comment(\"Attempt to find corrupt buckets appearing in the search heads dispatch/info.csv files, this will show that a user is seeing the \\\"data may be corrupt\\\" messages\")`\n`comment(\"In a clustered environment this should auto-repair via the cluster master fixup list, messages such as \\\"06-12-2018 07:31:47.160 +0000 INFO  ProcessTracker - (child_407__Fsck)  Fsck - (entire bucket) Rebuild for bucket='/opt/splunk/var/lib/splunk/indexname/db/db_1528466340_1520517600_38_A25ECA32-B33E-4469-8C76-22190FDCC8CB' took 86.26 seconds.\\\" should appear in the splunkd logs. In a non-clustered environment refer to the Splunk fsck documentation. You may also wish to try IndexerLevel - Corrupt buckets via DBInspect\")`\nindex=_internal sourcetype=splunk_search_messages \"corrupt\" OR \"corrupted\" OR \"Consider running fsck\" `searchheadhosts` \n| rex \",\\\"(\\[[^\\]]+\\]\\[[^\\]]+\\]: )?\\[(?P<peer>[^\\]]+)\"\n| rex \"message=\\[(?P<peer>[^\\]]+)\" \n| rex \"path='(?P<diskloc>[^']+)\" \n| rex \"files in '(?P<diskloc>[^']+)\" \n| rex field=diskloc \".*/(?P<bucket>[^/]+)$\" \n| fillnull value=\"Unknown\" diskloc peer bucket \n| stats values(host) AS reportingHost, max(_time) AS mostRecent, first(_raw) AS raw by bucket, diskloc, peer\n| sort - mostRecent\n| eval mostRecent=strftime(mostRecent, \"%+\")\n| table mostRecent, diskloc, peer, reportingHost, bucket, raw",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This query checks for searches that have found a corrupt bucket in the environment, this does require the limits.conf setting log_search_messages=true",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Determine query scan density",
    "search": "`comment(\"Determine the query scan density of queries per-index. Excludes replicated search jobs (rsa)\")` index=_audit `searchheadhosts` action=search sourcetype=audittrail search_id!=\"rsa_*\"\n| eval sname=if(isnull(savedsearch_name) OR savedsearch_name==\"\", search, savedsearch_name)\n| stats list(search_type) as search_type, list(api_et) as api_et, list(api_lt) as api_lt, list(apiStartTime) as apiStartTime, list(apiEndTime) as apiEndTime,list(search_et) as search_et, list(search_lt) as search_lt, list(info) as status, list(total_run_time) as total_run_time list(event_count) as event_count,list(considered_events) as considered_events, list(result_count) as result_count, list(scan_count) as scan_count, list(ttl) as ttl, list(is_realtime) as search_realtime_check, list(_time) as TimeAudited, list(sname) as sname by search_id\n| where isnotnull(scan_count) AND NOT event_count=\"N/A\" AND LIKE(sname, \"%index%=%\")\n| search sname!=\"'typeahead prefix=*\"\n| rex field=sname \"(?s)index(\\s*=\\s*|::)(?P<indexname>[^ \\t]+)\"\n| eval indexname=replace(indexname, \"'\", \"\"), indexname=replace(indexname, \"\\\"\", \"\")\n| stats avg(event_count) AS avgEventCount, avg(scan_count) AS avgScanCount, avg(result_count) AS avgResultCount by indexname\n| eval scanDensity=(avgResultCount/avgScanCount)*100",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This query measures the approx density of a search to determine if it's considered rare or dense.\nThis version provides an example query which can then be used to drill down into further details as required",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Disabled modular inputs are running",
    "search": "`comment(\"Attempt to detect when a Splunk scripted_input appears to be running at OS level even though disabled=1 is set\")`\n`comment(\"Splunk support have advised that the modular input scripts run by design, in fact disabled=1 means the inheriting input stanzas should be disabled, not that the modular input is disabled\")`\n`comment(\"As per the updated documentation on https://docs.splunk.com/Documentation/AddOns/released/Overview/Distributedinstall best practice is now to remove the inputs.conf and inputs.conf.spec on SH clusters...\")`\nindex=_introspection `localsearchheadhosts` sourcetype=splunk_resource_usage \n| spath component \n| search component=PerProcess data.process!=splunkd \n| spath data.args \n| rex field=data.args \"[/\\\\\\\\](?P<title>[^/\\\\\\\\\\.]+)\\.[^\\.]+\" \n| join title overwrite=false\n    [| rest splunk_server=local /servicesNS/-/-/configs/conf-inputs \n    | search title!=\"*://*\" disabled=1 \n    | table eai:acl.app, disabled, interval, title] \n| rename eai:acl.app AS app \n| stats max(_time) AS mostRecent, min(_time) AS firstSeen, values(host) AS hostList, values(data.status) AS status by app, interval, title, data.args, data.process, data.process_type \n| eval mostRecent=strftime(mostRecent, \"%+\"), firstSeen=strftime(firstSeen, \"%+\") \n| table title, app, firstSeen, mostRecent, hostList, interval, data.args, data.process, data.process_type, status",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. While the modular input is disabled it appears to be running according to the introspection logs. The solution is to remove the inputs.conf and inputs.conf.spec from the relevant app, or at least remove the non-used modular inputs. Note this alert is only relevant to the search head/cluster it is running on and this is *not* an issue as such from the Splunk support point of view, however the python scripts can impact server performance... Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "39 5 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - EventTypes report",
    "search": "| rest /servicesNS/-/-/saved/eventtypes splunk_server=local search=index search=disabled=0 \n| search eai:acl.sharing!=\"user\" \n| rename eai:acl.app AS app, eai:acl.sharing AS sharing \n| eval sharing=if(sharing==\"system\",\"global\",sharing) \n| table title, search, app, sharing \n| rename search as definition, title AS eventtype \n| eval splunk_server=\"default\" \n| outputlookup splunkadmins_eventtypes",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is required to support the audit log summary searches. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "50 5 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Excessive REST API usage",
    "search": "`comment(\"attempt to detect overuse of the REST API by non-system users\")` index=_internal `searchheadhosts` sourcetype=splunkd_access useragent!=\"Splunk/*\" useragent!=\"Splunkd/*\" user!=splunk-system-user user!=admin user!=- NOT \"/results_preview\" \"/search/jobs/\" clientip!=\"127.0.0.1\" `comment(\"this is the splunk internal httplib version proxying requests on behalf of clients this will likely change on upgrade, current as of 8.2.2.1\")` NOT (`splunkadmins_excessive_rest_api_httplib` \"isProxyRequest=true\")\n| regex uri!=\"/control$\" \n| bin _time span=2m \n| stats count by user, _time \n| where count>`splunkadmins_excessive_rest_api_threshold` \n| eval earliest=_time-120, latest=_time+120 \n| eval query=\"index=_internal `searchheadhosts` sourcetype=splunkd_access useragent!=\\\"Splunk/*\\\" useragent!=\\\"Splunkd/*\\\" user!=splunk-system-user user!=admin user!=- NOT \\\"/results_preview\\\" \\\"/search/jobs/\\\" clientip!=\\\"127.0.0.1\\\" NOT (\\\"Python-httplib2/0.13.1 (gzip)\\\" \\\"isProxyRequest=true\\\") user=\" . user . \" earliest=\" . earliest . \" latest=\" . latest . \" | regex uri!=\\\"/control$\\\" | rex field=uri \\\"/(?P<last_of_url>[^/]+$)\\\" | streamstats current=false last(_time) AS prev_time by last_of_url | eventstats count AS count_by_last_of_url by last_of_url | eval time_diff=if(isnull(prev_time),null(),prev_time-_time)\" \n| fields - earliest, latest\n| sort - count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Excessive usage of the REST API by, for example, querying the jobs endpoint continuously without sleeping can result in the Splunk search head crashing due to excessive thread usage",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "42 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Index access list by user",
    "search": "| rest /services/authorization/roles splunk_server=\"local\" \n| eval comment=\"This search aims to provide a giant list of users and what indexes they have access to (as in a list of index names, not a list of wildcards). Due to mvexpand hitting memory limits in the environment this alternative version runs many subsearches that do not hit the memory limits\" \n| table title, srchIndexesAllowed, srchIndexesDefault, imported_srchIndexesAllowed, imported_srchIndexesDefault \n| rename title as roles \n| makemv srchIndexesAllowed tokenizer=(\\S+) \n| makemv srchIndexesDefault tokenizer=(\\S+) \n| makemv imported_srchIndexesAllowed tokenizer=(\\S+) \n| makemv imported_srchIndexesDefault tokenizer=(\\S+) \n| eval srchIndexesAllowed = mvappend(srchIndexesAllowed, imported_srchIndexesAllowed) \n| eval srchIndexesDefault = mvappend(srchIndexesDefault, imported_srchIndexesDefault) \n| fillnull srchIndexesDefault, srchIndexesAllowed value=\"requiredformvexpand\" \n| mvexpand srchIndexesAllowed \n| eval srchIndexesAllowed=if(srchIndexesAllowed==\"requiredformvexpand\",null(),srchIndexesAllowed) \n| eval srchIndexesAllowed=lower(srchIndexesAllowed) \n| fields srchIndexesAllowed, srchIndexesDefault, roles \n| map \n    [| inputlookup splunkadmins_indexlist where index=\"$srchIndexesAllowed$\" AND index!=\"requiredformvexpand\"\n    | eval regex=\"^\" . \"$srchIndexesAllowed$\" . \"$\" \n    | eval regex=replace(regex,\"\\*\",\".*\") \n    | eval regex=if(substr(regex,1,3)==\"^.*\",\"^[^_].*\" . substr(regex,4),regex) \n    | where match(index,regex) \n    | eval srchIndexesAllowed=\"$srchIndexesAllowed$\", srchIndexesDefault=\"$srchIndexesDefault$\", roles=\"$roles$\" \n    | fields index, roles, srchIndexesAllowed, srchIndexesDefault ] maxsearches=5000 \n| stats values(index) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by roles \n| makemv srchIndexesDefault tokenizer=(\\S+) \n| mvexpand srchIndexesDefault \n| append [ | makeresults | eval srchIndexesAllowed=\"workaround for map errors\", srchIndexesDefault=\"to pass appinspect\", roles=\"N/A\" ]\n| map \n    [| inputlookup splunkadmins_indexlist where index=\"$srchIndexesDefault$\" \n    | eval regex=\"^\" . \"$srchIndexesDefault$\" . \"$\" \n    | eval regex=replace(lower(regex),\"\\*\",\".*\") \n    | eval regex=if(substr(regex,1,3)==\"^.*\",\"^[^_].*\" . substr(regex,4),regex) \n    | where match(index,regex) \n    | eval srchIndexesAllowed=\"$srchIndexesAllowed$\", srchIndexesDefault=\"$srchIndexesDefault$\", roles=\"$roles$\" \n    | fields index, roles, srchIndexesAllowed, srchIndexesDefault ] maxsearches=5000 \n| where srchIndexesAllowed!=\"workaround for map errors\"\n| stats values(srchIndexesAllowed) AS srchIndexesAllowed, values(index) AS srchIndexesDefault by roles \n| makemv srchIndexesAllowed tokenizer=(\\S+) \n| append \n    [| rest /services/admin/LDAP-groups splunk_server=local \n    | where isnotnull(roles) \n    | mvexpand users \n    | rex field=users \"CN=(?P<user>[^,]+)\" \n    | stats values(user) AS user by roles ] \n| append \n    [| rest /services/authentication/users splunk_server=local \n    | search type=Splunk \n    | table title, roles \n    | rename title AS user \n    | mvexpand roles ] \n| append \n    [| makeresults \n    | eval user=\"splunk-system-user\", roles=\"admin\" ]\n| eval srchIndexesDefault = if(srchIndexesDefault==\"requiredformvexpand\",null(),srchIndexesDefault)     \n| eventstats values(srchIndexesAllowed) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by roles \n| stats values(srchIndexesAllowed) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by user\n| outputlookup splunkadmins_userlist_indexinfo",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-5m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report outputs a list of indexes available on a per-user basis which is used by another report, \"SearchHeadLevel - Search Queries summary non-exact match\", requires report \"SearchHeadLevel - Index list report\"",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Index list by cluster report",
    "search": "| eventcount summarize=false index=* OR index=_* \n| eval indexer_cluster=`indexer_cluster_name(server)` \n| stats count by index, indexer_cluster \n| fields - count \n| outputlookup splunkadmins_indexlist_by_cluster",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-30d@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report outputs a list of indexes available per indexer cluster. Used by other reports such as IndexerLevel - RemoteSearches Indexes Stats Wilcard",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Index list report",
    "search": "| eventcount summarize=false index=* OR index=_* \n| fields index \n| dedup index\n| append [ | makeresults | eval index=\"requiredformvexpand\" ]\n| table index\n| outputlookup splunkadmins_indexlist",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-30d@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report outputs a list of indexes available and an additional \"requiredformvexpand\" value, which is used by the report \"SearchHeadLevel - Index access list by user\"",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Indexer Peer Connection Failures",
    "search": "`comment(\"Further testing required. Detect failures from the search.log advising that the peer was unable to send a response, for example This can be caused by the peer unexpectedly closing or resetting the connection. Search results might be incomplete!...This requires the search.log messages (see description of this alert) to obtain the splunk_search_messages sourcetype. The Unable to distribute to peer named status=Down scenario can also result from having many indexers and may require an increase to the timeouts in distsearch.conf\")`\n`comment(\"info.csv often reports failures as well but sometimes these are not in search.log and vice-versa. Unable to distribute to peer/Connection failed/Unable to determine response all appear to be some kind of failure. Attempting to use the [search] log_search_messages = true in the limits.conf file and then use the search_messages.log file to find what would normally appear in info.csv in the dispatch directory per-search...\")`\nindex=_internal sourcetype=spluink_server_messages source!=\"*rsa_scheduler_*\" `searchheadhosts` (\"error\" \"for peer\") OR \"Error connecting\" OR \"Got status\" `comment(\"Ignoring \\\"HTTP error status message from\\\" OR \\\"HTTP client error\\\" as they tend to appear when one of the previous examples is there...\")`\n| rex \"for peer (?P<peer>[^\\.]+)\"\n| rex \"ERROR\\s+\\S+\\s+-\\s+(sid:[^ ]+)?(?P<message>.*)\"\n| bin _time span=1m\n| eval msgpeer = host + source + peer + _time\n| rex field=host \"(?P<host>[^\\.]+)\"\n| stats dc(msgpeer) AS count, dc(eval(searchmatch(\"source=*scheduler_*\"))) AS schedulerCount, values(host) AS reportingHost, values(message) AS message by peer, _time\n| eval errorFrom=\"splunk_search_messages\"\n| append\n    [ search index=_internal sourcetype=splunk_search_messages orig_component=\"DispatchThread\" `searchheadhosts` \"Connection failed\" OR \"Unable to determine response\" OR \"Unable to distribute to peer\"\n    | rex \",\\\"(\\[[^\\]]+\\]\\[[^\\]]+\\]: )?\\[(?P<peer>[^\\.\\]]+).*?\\] (?P<message>[^\\\"]+)\"\n    | rex \"Unable to distribute to peer named .* (?P<message>because.*?)\\\",\"\n    | rex field=uri \"(?P<IP>[^:]+)\"\n    | lookup dnslookup clientip as IP OUTPUT clienthost AS peer\n    | rex field=peer \"(?P<peer>[^\\.]+)\"\n    | bin _time span=1m\n    | eval msgpeer = host + source + peer + _time\n    | rex field=host \"(?P<host>[^\\.]+)\"\n    | stats dc(msgpeer) AS count, dc(eval(searchmatch(\"source=*scheduler_*\"))) AS schedulerCount, values(host) AS reportingHost, values(message) AS message by peer, _time\n    | eval errorFrom=\"splunk_search_messages\"\n        ]\n| stats sum(count) AS count, sum(schedulerCount) AS schedulerCount, values(reportingHost) AS reportingHost, values(message) AS message, values(errorFrom) AS errorFrom by peer, _time\n| eval countAndSchedulerCount = count . \" / \" . schedulerCount\n| table _time, peer, reportingHost, countAndSchedulerCount, message, errorFrom\n| sort - _time",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. For some reason one or more peers are failing to respond to the search heads, which may impact search results. Any failure will be reporting an error either to the end user or to a scheduled search. Note this alert requires the splunk_search_messages sourcetype (or search.log) and the [search]\nlog_search_messages = true\nIn the limits.conf file and then use the search_messages.log file",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "53 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - IndexesPerRole Remote Report",
    "search": "search `comment(\"remote the comments once you have TA-webtools installed to use the curl command...<begin> | curl \")` method=get uri=\"$url$/services/authorization/roles?output_mode=json&count=0&f=srchIndexesAllowed&f=srchIndexesDefault&f=imported_srchIndexesAllowed&f=imported_srchIndexesDefault\" user=\"$user$\" pass=\"$pass$\" \n| rex field=curl_message max_match=10000 \"{\\\"name\\\":\\\"(?P<role>[^\\\"]+)\\\".*?\\\"imported_srchIndexesAllowed\\\":(?P<imported_srchIndexesAllowed>\\[[^\\]]*\\]),\\\"imported_srchIndexesDefault\\\":(?P<imported_srchIndexesDefault>\\[[^\\]]*\\]),\\\"srchIndexesAllowed\\\":(?P<srchIndexesAllowed>\\[[^\\]]*\\]),\\\"srchIndexesDefault\\\":(?P<srchIndexesDefault>\\[[^\\]]*\\])\" \n| fields - curl_* \n| eval srchIndexesAllowed=mvzip(srchIndexesAllowed,imported_srchIndexesAllowed) \n| eval srchIndexesDefault=mvzip(srchIndexesDefault,imported_srchIndexesDefault) \n| eval data=mvzip(role,mvzip(srchIndexesDefault,srchIndexesAllowed,\"%%%%\"),\"%%%%\") \n| fields data \n| mvexpand data \n| makemv delim=\"%%%%\" data \n| eval roles=mvindex(data,0), srchIndexesDefault=mvindex(data,1), srchIndexesAllowed=mvindex(data,2) \n| fields - data \n| eval srchIndexesDefault=replace(srchIndexesDefault,\"(\\[\\],|,\\[\\]|\\\"|\\[|\\])\",\"\") \n| eval srchIndexesAllowed=replace(srchIndexesAllowed,\"(\\[\\],|,\\[\\]|\\\"|\\[|\\])\",\"\") \n| makemv srchIndexesAllowed delim=\",\" \n| makemv srchIndexesDefault delim=\",\" \n| eval srchIndexesAllowed=if(mvcount(srchIndexesAllowed)==0 OR isnull(srchIndexesAllowed),\"requiredformvexpand\",srchIndexesAllowed), srchIndexesDefault=if(mvcount(srchIndexesDefault)==0 OR isnull(srchIndexesDefault),\"requiredformvexpand\",srchIndexesDefault) \n| mvexpand srchIndexesAllowed \n| eval srchIndexesAllowed=if(srchIndexesAllowed==\"requiredformvexpand\",null(),srchIndexesAllowed) \n| eval srchIndexesAllowed=lower(srchIndexesAllowed) \n| fields srchIndexesAllowed, srchIndexesDefault, roles \n| append [ | makeresults | eval srchIndexesAllowed=\"NA\", srchIndexesDefault=\"NA\", roles=\"novalidroles\", splunk_server=\"default\" | fields - _time ]\n| map \n    \"SearchHeadLevel - IndexesPerRole srchIndexesallowed Report\" maxsearches=15000 \n| stats values(index) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by roles, splunk_server \n| eval srchIndexesDefault=replace(srchIndexesDefault,\",\",\" \") \n| makemv srchIndexesDefault tokenizer=(\\S+) \n| mvexpand srchIndexesDefault \n| append [ | makeresults | eval srchIndexesAllowed=\"NA\", srchIndexesDefault=\"NA\", roles=\"novalidroles\", splunk_server=\"default\" | fields - _time ]\n| map \n    \"SearchHeadLevel - IndexesPerRole srchIndexesdefault Report\" maxsearches=15000 \n| stats values(srchIndexesAllowed) AS srchIndexesAllowed, values(index) AS srchIndexesDefault by roles \n| where roles!=\"novalidroles\"\n| makemv srchIndexesAllowed tokenizer=(\\S+) \n| eval srchIndexesDefault = if(srchIndexesDefault==\"requiredformvexpand\",null(),srchIndexesDefault)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-5m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Attempt to query a remote Splunk instance to find a list of accessible indexes per role within Splunk, relies on 2 other reports for the map commands, requires the \"SearchHeadLevel - Index list report\" report to be run to populate the lookup file splunkadmins_indexlist",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - IndexesPerRole Report",
    "search": "| rest /services/authorization/roles splunk_server=\"local\" \n| eval comment=\"This search aims to provide a giant list of users and what indexes they have access to (as in a list of index names, not a list of wildcards). Due to mvexpand hitting memory limits in the environment this alternative version runs many subsearches that do not hit the memory limits\" \n| table title, srchIndexesAllowed, srchIndexesDefault, imported_srchIndexesAllowed, imported_srchIndexesDefault \n| rename title as roles \n| makemv srchIndexesAllowed tokenizer=(\\S+) \n| makemv srchIndexesDefault tokenizer=(\\S+) \n| makemv imported_srchIndexesAllowed tokenizer=(\\S+) \n| makemv imported_srchIndexesDefault tokenizer=(\\S+) \n| eval srchIndexesAllowed = mvappend(srchIndexesAllowed, imported_srchIndexesAllowed) \n| eval srchIndexesDefault = mvappend(srchIndexesDefault, imported_srchIndexesDefault) \n| eval splunk_server=\"default\"\n| eval srchIndexesAllowed=if(mvcount(srchIndexesAllowed)==0 OR isnull(srchIndexesAllowed),\"requiredformvexpand\",srchIndexesAllowed), srchIndexesDefault=if(mvcount(srchIndexesDefault)==0 OR isnull(srchIndexesDefault),\"requiredformvexpand\",srchIndexesDefault)\n| mvexpand srchIndexesAllowed \n| eval srchIndexesAllowed=if(srchIndexesAllowed==\"requiredformvexpand\",null(),srchIndexesAllowed) \n| eval srchIndexesAllowed=lower(srchIndexesAllowed) \n| fields srchIndexesAllowed, srchIndexesDefault, roles, splunk_server\n| map \n    \"SearchHeadLevel - IndexesPerRole srchIndexesallowed Report\" maxsearches=15000 \n| stats values(index) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by roles, splunk_server\n| eval srchIndexesDefault=replace(srchIndexesDefault,\",\",\" \")\n| makemv srchIndexesDefault tokenizer=(\\S+)\n| mvexpand srchIndexesDefault \n| map \n    \"SearchHeadLevel - IndexesPerRole srchIndexesdefault Report\" maxsearches=15000 \n| stats values(srchIndexesAllowed) AS srchIndexesAllowed, values(index) AS srchIndexesDefault by roles, splunk_server \n| makemv srchIndexesAllowed tokenizer=(\\S+) \n| eval srchIndexesDefault = if(srchIndexesDefault==\"requiredformvexpand\",null(),srchIndexesDefault)\n| search `comment(\"You can add this in if you have mutiple search head clusters or search heads after removing the backspaces...| append [ savedsearch \\\"SearchHeadLevel - IndexesPerRole Remote Report\\\" url=\\\"...\\\" user=\\\"...\\\" pass=\\\"...\\\" | eval splunk_server=\"...\" ]\")`\n| outputlookup splunkadmins_indexes_per_role",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-5m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Query the local Splunk instance to determine the indexes available per role, requires the splunkadmins_indexlist lookup file",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - IndexesPerRole srchIndexesallowed Report",
    "search": "| inputlookup splunkadmins_indexlist where index=\"$srchIndexesAllowed$\" AND index!=\"requiredformvexpand\"\n    | eval regex=\"^\" . \"$srchIndexesAllowed$\" . \"$\" \n    | eval regex=replace(regex,\"\\*\",\".*\") \n    | eval regex=if(substr(regex,1,3)==\"^.*\",\"^[^_].*\" . substr(regex,4),regex) \n    | where match(index,regex) \n    | eval srchIndexesAllowed=\"$srchIndexesAllowed$\", srchIndexesDefault=\"$srchIndexesDefault$\", roles=\"$roles$\", splunk_server=\"$splunk_server$\"\n    | fields index, roles, srchIndexesAllowed, srchIndexesDefault, splunk_server",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-5m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Designed to be run by a map / 1 event, run a lookup against the srchIndexesAllowed and a regex match to see if the index name in the event matches the srchIndexesAllowed..., requires the splunkadmins_indexlist lookup file",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - IndexesPerRole srchIndexesdefault Report",
    "search": "| inputlookup splunkadmins_indexlist where index=\"$srchIndexesDefault$\"\n    | eval regex=\"^\" . \"$srchIndexesDefault$\" . \"$\" \n    | eval regex=replace(regex,\"\\*\",\".*\") \n    | eval regex=if(substr(regex,1,3)==\"^.*\",\"^[^_].*\" . substr(regex,4),regex) \n    | where match(index,regex) \n    | eval srchIndexesAllowed=\"$srchIndexesAllowed$\", srchIndexesDefault=\"$srchIndexesDefault$\", roles=\"$roles$\", splunk_server=\"$splunk_server$\"\n    | fields index, roles, srchIndexesAllowed, srchIndexesDefault, splunk_server",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-5m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Designed to be run by a map / 1 event, run a lookup against the srchIndexesDefault and a regex match to see if the index name in the event matches the srchIndexesDefault..., requires the splunkadmins_indexlist lookup file",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - IndexesPerUser Report",
    "search": "| rest /services/authorization/roles splunk_server=\"local\" \n| eval comment=\"This search aims to provide a giant list of users and what indexes they have access to (as in a list of index names, not a list of wildcards). Due to mvexpand hitting memory limits in the environment this alternative version runs many subsearches that do not hit the memory limits\" \n| table title, srchIndexesAllowed, srchIndexesDefault, imported_srchIndexesAllowed, imported_srchIndexesDefault \n| rename title as roles \n| makemv srchIndexesAllowed tokenizer=(\\S+) \n| makemv srchIndexesDefault tokenizer=(\\S+) \n| makemv imported_srchIndexesAllowed tokenizer=(\\S+) \n| makemv imported_srchIndexesDefault tokenizer=(\\S+) \n| eval srchIndexesAllowed = mvappend(srchIndexesAllowed, imported_srchIndexesAllowed) \n| eval srchIndexesDefault = mvappend(srchIndexesDefault, imported_srchIndexesDefault) \n| fillnull srchIndexesDefault, srchIndexesAllowed value=\"requiredformvexpand\" \n| mvexpand srchIndexesAllowed \n| eval srchIndexesAllowed=if(srchIndexesAllowed==\"requiredformvexpand\",null(),srchIndexesAllowed) \n| eval srchIndexesAllowed=lower(srchIndexesAllowed) \n| fields srchIndexesAllowed, srchIndexesDefault, roles \n| eval splunk_server=\"default\" \n| append [ | makeresults | eval srchIndexesAllowed=\"\", srchIndexesDefault=\"\", roles=\"novalidroles\", splunk_server=\"default\" | fields - _time ]\n| map \n    \"SearchHeadLevel - IndexesPerRole srchIndexesallowed Report\" maxsearches=5000 \n| stats values(index) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by roles, splunk_server \n| makemv srchIndexesDefault tokenizer=(\\S+) \n| mvexpand srchIndexesDefault \n| map \n    \"SearchHeadLevel - IndexesPerRole srchIndexesdefault Report\" maxsearches=5000 \n| stats values(srchIndexesAllowed) AS srchIndexesAllowed, values(index) AS srchIndexesDefault by roles, splunk_server \n| where roles!=\"novalidroles\" \n| makemv srchIndexesAllowed tokenizer=(\\S+) \n| append \n    [| rest /services/authentication/users f=type f=roles splunk_server=local \n    | table title, roles \n    | rename title AS user \n    | mvexpand roles ] \n| append \n    [| makeresults \n    | eval user=\"splunk-system-user\", roles=\"admin\" ]\n| eval srchIndexesDefault = if(srchIndexesDefault==\"requiredformvexpand\",null(),srchIndexesDefault)     \n| eventstats values(srchIndexesAllowed) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by roles \n| stats values(srchIndexesAllowed) AS srchIndexesAllowed, values(srchIndexesDefault) AS srchIndexesDefault by user",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-5m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report requires the splunkadmins_indexlist lookup, it lists indexes accessible per-user from a local server. Requires the \"SearchHeadLevel - Index list report\" report to be run to populate the lookup file splunkadmins_indexlist",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - KVStore Or Conf Replication Issues Are Occurring",
    "search": "`comment(\"Detect search head issues related to extended search head downtime, in particular ConfReplication issues or KV store replication issues\")`\n`comment(\"KVStore - http://docs.splunk.com/Documentation/Splunk/latest/Admin/ResyncKVstore , ConfReplication - http://docs.splunk.com/Documentation/Splunk/latest/DistSearch/HowconfrepoworksinSHC#Replication_synchronization_issues\")`\n`comment(\"The search head cluster captain is disconnected can relate to a SH cluster restart *or* if outside a rolling restart this may require a restart of the problematic search head...\")`\n`comment(\"In addition to this you could also look for \\\"Error pushing configurations to captain\\\" consecutiveErrors>1 , this would also hint at a potential issue although a small number of consecutive errors appears to be normal...\")`\n`comment(\"If you see the message \\\"Consider performing a destructive configuration resync on this search head cluster member\\\", then it's a real issue and often requires manual intervention...\")` \nindex=_internal `searchheadhosts` \"Local KV Store has replication issues\" OR (\"ConfReplicationThread\" \"captain\") OR (\"SHCMasterHTTPProxy\" \"Low Level http request\" \"socket_error\") sourcetype=splunkd (`splunkadmins_splunkd_source`) \n| regex \"\\S+\\s+\\S+\\s+\\S+\\s+(ERROR|WARN)\" \n| search `comment(\"Exclude time periods where shutdowns were occurring\")` AND NOT [`splunkadmins_shutdown_time(searchheadhosts,0,0)`]\n| cluster showcount=true t=0.93 labelonly=t \n| fillnull value=0 consecutiveErrors \n| stats min(_time) AS firstSeen, max(_time) AS mostRecent, values(_raw) AS _raw, max(cluster_count) AS cluster_count, max(consecutiveErrors) AS consecutiveErrors by host, cluster_label \n| eval search_head_cluster=`search_head_cluster`  \n| eval firstSeen=strftime(firstSeen, \"%+\"), mostRecent=strftime(mostRecent, \"%+\") \n| where (match(_raw, \"Error pushing configurations\") AND consecutiveErrors>4) OR (match(_raw, \"Error pulling configurations\") AND consecutiveErrors>2) OR NOT match(_raw, \"Error (pushing|pulling) configurations\") \n| fields - cluster_label, consecutiveErrors",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-10m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. If the KVStore is out of sync or the search head is out of sync it will likely require a manual resync/clean to get it working as expected\nIf it relates to a conf replication issue it is likely a problematic search head requiring a restart or it may require a force sync...(the logs will advise on this)\nTo remove false alarms this alert now checks if any shutdown messages appear, this may require tweaking in your environment as it checks for *any* search head shutdown...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "12,22,32,42,52,02 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - LDAP users have been disabled or left the company cleanup required",
    "search": "`comment(\"If we see a failed to get LDAP user 'username' from any configured servers then that is a sign the user is no longer in the company. However if there is also a message of Couldn't find matching groups for that same user it is more likely that they exist but just do not have access to Splunk\")`\n`comment(\"If you see this alert fire, than you probably need to cleanup the (for example) /opt/splunk/etc/users/... directory on each search head due to a user leaving/becoming disabled in LDAP. Alternatively they have a savedsearch/dashboard that you can find in the .meta files on the search head(s)\")`\nindex=_internal `searchheadhosts` \"Failed to get LDAP user=\\\"\" OR \"Couldn't find matching groups for user=\" OR \"HTTPAuthManager - SSO failed - User does not exist\" sourcetype=splunkd (`splunkadmins_splunkd_source`)\n| eval message=coalesce(message,event_message)\n| dedup message \n| rex \"SSO failed - User does not exist: (?P<user>\\S+)\"\n| stats count, values(message) AS messages, values(component), AS components values(log_level), max(_time) AS lastSeen by user, host\n| search `comment(\"count=1 eliminates users who are failing to login...if a user is active in LDAP but fails to login we should not not get a 'Couldn't find matching groups for user' line in the logs\")`\n`comment(\"If we are using a single sign on system and a user without any groups attempts sign on we should see the SSO failed - User does not exist: <username> message\")`\n| where user!=\"undefined\" AND user!=\"nobody\" AND like(messages,\"Failed to get LDAP user%\") AND NOT like(messages,\"SSO failed - User does not exist%\")\n| table user, messages, lastSeen, host\n| eval lastSeen=strftime(lastSeen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. These users have been disabled or left the company but their users files are on the filesystem and this is therefore triggering warning or errors in the Splunk logs, please cleanup the old user files for these users.\nA separate alert should exist for orphaned searches...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "33 11 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Long Running Searches Found",
    "search": "`comment(\"Find any search running longer than a period of time, this is useful for tracking down poorly built search queries or dashboards\")` \nindex=_audit sourcetype=audittrail info=completed  \n`splunkenterprisehosts` \n`comment(\"Exclude accelerated searches\")` \nsavedsearch_name!=_ACCELERATE*\ntotal_run_time>300\n`splunkadmins_longrunning_searches`\n`comment(\"At this point we have a list of searches minus the various exclusions, we now filter out the real time searches as they will always run for a long period of time...\")`\n| regex search_id!=\"rt.*\" | table savedsearch_name, search_id, total_run_time, search_et, search_lt, api_et, api_lt, scan_count, _time, user, info, host | eval search_et=strftime(search_et, \"%d/%m/%Y %H:%M\"), search_lt=strftime(search_lt, \"%d/%m/%Y %H:%M\"), api_et=strftime(api_et, \"%d/%m/%Y %H:%M\"), api_lt=strftime(api_lt, \"%d/%m/%Y %H:%M\"), _time=strftime(_time, \"%d/%m/%Y %H:%M\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. Extra long running searches have been found",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "38 0,4,8,12,14,16,20 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Long filenames may be causing issues",
    "search": "`comment(\"Detect the issue where someone has created a file longer than 100 characters and the cluster is having issues with replication. The 100 character issue was confirmed in Splunk 6.5.2 during a support case\")`\nindex=_internal `searchheadhosts` (\"ArchiveFile - Failed to write archive header for\" \"Pathname too long\") OR (\"ERROR Archiver - Unable to add entry\") (`splunkadmins_splunkd_source`) sourcetype=splunkd \n| cluster showcount=true\n| fields _time, _raw, cluster_count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. There are one or more dashboards or alerts with a filename long enough to cause errors in the archive processor, the exact implications are unknown but the alert/dashboard may need to be removed.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "56 5 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Lookup CSV size",
    "search": "| rest splunk_server=local /servicesNS/-/-/admin/transforms-lookup getsize=true \n| eval name = 'eai:acl.app' + \".\" + title \n| rename \"eai:acl.sharing\" AS sharing | eval is_temporal = if(isnull(time_field),0,1) \n| table name type is_temporal size sharing \n| join type=left name \n  [ rest splunk_server=local /servicesNS/-/-/admin/kvstore-collectionstats \n    | table data \n    | mvexpand data \n    | spath input=data \n    | table ns size \n    | rename ns as name ] \n| sort - size \n| eval size=round(size/1024/1024)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report will work in Splunk 7.3.3 and above as the getsize=true option is available on the REST endpoint, prior to this version the file-explorer endpoint can be used (see dashboard ... ?). Contributed by an anonymous source",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "38 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Lookup updates within SHC",
    "search": "`comment(\"Excessive CSV lookup updates can trigger extra bundle replication issues to the indexer cluster(s), this report identifies the lookups with the largest number of updates\")`\nindex=_internal `searchheadhosts` source=*conf.log sourcetype=splunkd_conf lookups \"data.asset_uri{}\"=lookups \"data.optype_desc\"=NOTIFY_UPDATE_LOOKUP `comment(\"data.task=acceptPush also appears to work...\")` data.task=addCommit \n| rename \"data.asset_uri{}\" AS asset\n| eval lookup_user=mvindex(asset, 0), lookup_app=mvindex(asset, 1), lookup_file=mvindex(asset, 3)\n| stats min(_time) AS firstSeen, max(_time) AS lastSeen, count by lookup_file\n| eval frequencyOfUpdatesInMins = round(((lastSeen-firstSeen)/60)/count)\n| eval firstSeen=strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\")\n| sort - count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Excessive CSV lookup updates can trigger extra bundle replication issues to the indexer cluster(s), this report identifies the lookups with the largest number of updates",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Macro report",
    "search": "| rest \"/servicesNS/-/-/configs/conf-macros?count=-1\" splunk_server=local\n| search eai:acl.sharing!=\"user\"\n| rename eai:acl.sharing AS sharing\n| search `comment(\"Potentially having an additional field for server group would add another level of accuracy to the lookup however in this env the chance of macros with the seame name but different definitions is low enough that this might be a waste of time...\")`\n| rename eai:acl.app AS app\n| fields title, app, definition, sharing\n| eval splunk_server=`splunkadmins_splunk_server_name` \n| search `comment(\"At this point you can add in remote search heads for macros by using the macro (with backticks) | append [ <backtick>AuditLogsMacroReport_Helper(\\\"<your remote host>\\\", \\\"remote_user\\\", \\\"remote_password\\\")<backtick> | splunk_server=... ]\")`\n| search `comment(\"The lookup is going to use the first value it sees, so just dedup on app/title\")`\n| eval app=if(sharing==\"global\",\"global\",'app')\n| stats first(definition) AS definition, values(sharing) AS sharing by title, app, splunk_server\n| outputlookup splunkadmins_macros",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is required to support SearchHeadLevel - Scheduled searches not specifying an index macro version AND SearchHeadLevel - User - Dashboards searching all indexes macro version. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "50 5 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Maximum memory utilisation per search",
    "search": "`comment(\"As originally found on https://answers.splunk.com/answers/500973/how-to-improve-my-search-to-identify-queries-which.html / DalJeanis with minor modifications. Max memory used per search process at search head level\")`\nindex=_introspection `searchheadhosts` sourcetype=splunk_resource_usage component=PerProcess data.search_props.sid=*\n| stats max(data.mem_used) AS peak_mem_usage,\n    latest(data.search_props.mode) AS mode,\n    latest(data.search_props.type) AS type,\n    latest(data.search_props.role) AS role,\n    latest(data.search_props.app) AS app,\n    latest(data.search_props.user) AS user,\n    latest(data.search_props.provenance) AS provenance,\n    latest(data.search_props.label) AS label,\n    latest(host) AS splunk_server,\n    min(_time) AS min_time,\n    max(_time) AS max_time\n    by data.search_props.sid, host\n| sort - peak_mem_usage\n| head 50\n| table provenance, peak_mem_usage, label, mode, type, role, app, user, min_time, max_time, data.search_props.sid splunk_server\n| eval min_time=strftime(min_time, \"%+\"), max_time=strftime(max_time, \"%+\")\n| rename data.search_props.sid AS sid,\n    peak_mem_usage AS \"Peak Physical Memory Usage (MB)\",\n    min_time AS \"First time seen\",\n    max_time AS \"Last time seen\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. As found on SplunkAnswers check the maximum memory usage per-search ( https://answers.splunk.com/answers/500973/how-to-improve-my-search-to-identify-queries-which.html )",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - RMD5 to savedsearch_name lookupgen report",
    "search": "`comment(\"Search the audit logs to find the RMD5 entries and record them into a lookup file\")` index=_audit info=completed RMD5* search_id!=\"'rsa_*\"\n| regex search_id!=\"^'subsearch_\"\n| rex field=search_id \"(?P<RMDvalue>RMD5[^_]+)\"\n| stats values(savedsearch_name) AS savedsearch_name by RMDvalue\n| lookup splunkadmins_rmd5_to_savedsearchname RMDvalue OUTPUT savedsearch_name AS savedsearch_name_currrent\n| where isnull(savedsearch_name_currrent)\n| outputlookup splunkadmins_rmd5_to_savedsearchname append=true",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is required to support various other searches that translate the RMD5 values back into real savedsearch names. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "50 5 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Realtime Scheduled Searches are in use",
    "search": "| rest /servicesNS/-/-/saved/searches `splunkadmins_restmacro` timeout=900 \n| search `comment(\"Find realtime scheduled searches, they should not be enabled\")` `splunkadmins_realtime_scheduledsearches`\n| table title author, realtime_schedule, cron_schedule, description, disabled, dispatch.earliest_time, dispatch.index_earliest, dispatch.index_latest, dispatch.latest_time, dispatchAs, eai:acl.app, eai:acl.owner, eai:acl.owner, updated, qualifiedSearch, is_scheduled, next_scheduled_time, alert_type, schedule_priority\n| search dispatch.earliest_time=rt* next_scheduled_time!=\"\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Realtime searches should not be scheduled, please only enable this alert if it relevant for your environment. Fields such as relation or alert_condition can be used if you want to look for realtime alerts only. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "8 0,4,8,12,16,20 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Realtime Search Queries in dashboards",
    "search": "| rest/servicesNS/-/-/data/ui/views `splunkadmins_restmacro` | regex eai:data=\"<(earliest|latest)(Time)?>rt\"\n| search `comment(\"Shows realtime search usage within dashboards\")` eai:data=*query* `splunkadmins_realtime_dashboard`\n| regex eai:data=\"<search.*\"\n| rex field=eai:data \"(?s)(?P<theSearch><search(?!String)[^>]*>[^<]*<query>.*?)<\\/query>\" max_match=200 \n| mvexpand theSearch \n| rex field=theSearch \"(?s)<search(?P<searchInfo>[^>]*)>[^<]*<query>(?P<theQuery>.*)\" \n| search searchInfo!=\"*base*\" `comment(\"Exclude queries which have a base, in general they will not have a earliest/latesttime so this gets confusing\")`\n`comment(\"It might be possible to use mvzip / mvexpand or mvindex to match the correct earliesttime/latesttime with each search query but it proved extremely difficult. So just keeping this as-is as the dashboard needs to be reviewed if it has too many realtime searches anyway\")`\n| rex field=eai:data \"(?s)<earliest(Time)?>(?P<earliesttime>[^<]+)\" max_match=200 \n| rex field=eai:data \"(?s)<latest(Time)?>(?P<latesttime>[^<]+)\" max_match=200 \n| table title, eai:appName, searchInfo, theQuery,eai:acl.owner, eai:acl.sharing, label, earliesttime, latesttime, splunk_server",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-48h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. Just a summary of all dashboards that use realtime searching...Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "11 4 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Role access list by user",
    "search": "| rest /services/authentication/users splunk_server=\"local\" \n| eval comment=\"This search aims to provide a giant list of users and a list of wildcards (or index names if specified in the Splunk config) for srchIndexesAllowed/srchIndexesDefault\" \n| table title roles\n| append [ | makeresults | eval title=\"splunk-system-user\", roles=\"admin\" ]\n| rename title as user \n| mvexpand roles \n| join type=left roles \n    [ rest /services/authorization/roles splunk_server=\"local\" \n    | table title, srchIndexesAllowed, srchIndexesDefault, imported_srchIndexesAllowed, imported_srchIndexesDefault\n    | rename title as roles] \n| makemv srchIndexesAllowed tokenizer=(\\S+) \n| makemv srchIndexesDefault tokenizer=(\\S+)\n| makemv imported_srchIndexesAllowed tokenizer=(\\S+)\n| makemv imported_srchIndexesDefault tokenizer=(\\S+)\n| eval srchIndexesAllowed = mvappend(srchIndexesAllowed, imported_srchIndexesAllowed)\n| eval srchIndexesDefault = mvappend(srchIndexesDefault, imported_srchIndexesDefault)\n| fillnull srchIndexesDefault, srchIndexesAllowed value=\"removeme\"\n| mvexpand srchIndexesAllowed\n| eval srchIndexesAllowed=if(srchIndexesAllowed==\"removeme\",null(),srchIndexesAllowed)\n| fields srchIndexesAllowed, srchIndexesDefault, user\n| stats values(*) as * by user\n| mvexpand srchIndexesDefault\n| eval srchIndexesDefault=if(srchIndexesDefault==\"removeme\",null(),srchIndexesDefault)\n| stats values(*) as * by user",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-5m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report outputs a list of users and which role they are in, and what the srchIndexesDefault and srchIndexesAvailable exist for that particular user, this does not list individual index names, the report \"SearchHeadLevel - Index access list by user\" exists to list index names...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - SHC Captain unable to establish common bundle",
    "search": "`comment(\"Attempt to detect bundle issues on any member of the search head cluster for an extended period of time. It can fail per-member if distsearch is customised\")` index=_internal `searchheadhosts` sourcetype=splunkd source=*splunkd.log \"Gave up waiting for the captain to establish a common bundle version\" OR \"Cannot determine a latest common bundle\" \n| search `comment(\"Exclude shutdown times\")` AND NOT [`splunkadmins_shutdown_time(indexerhosts,0,0)`]\n| timechart span=5m count by host\n| fillnull\n| untable _time, host, count\n| stats max(_time) AS mostRecent, min(_time) AS firstSeen, last(count) AS lastCount by host\n| eval mostRecent=strftime(mostRecent, \"%+\"), firstSeen=strftime(firstSeen, \"%+\")\n| where lastCount>0",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Failures to establish a common search bundle result in delayed searches, this can occur per-member if distsearch.conf is changed",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - SHC conf log summary",
    "search": "`comment(\"Measure search head clustering writes to particular config objects via the conf.log file\")` index=_internal source=*conf.log data.task=addCommit sourcetype=splunkd_conf `searchheadhosts`\n| spath output=username path=data.asset_uri{0}\n| spath output=app path=data.asset_uri{1}\n| spath output=type path=data.asset_uri{2}\n| spath output=objname path=data.asset_uri{3}\n| fields objname, type, username, app, data.asset_id, host\n| stats count by data.asset_id, objname, type, username, app, host\n| fields - data.asset_id\n| sort 0 - count\n| table count, objname, type, app, username, host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report attempts to summarise replication activity within the search head cluster via the conf.log file, the report 'SearchHeadLevel - Lookup updates within SHC' is designed more specifically for lookup updates only",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - SHCluster Artifact Replication Issues",
    "search": "`comment(\"When this issue occurs it is likely related to some kind of issue post-restart of the indexer/search head cluster. Restarting the search head cluster appears to resolve the issue in 6.5.2\")`\nindex=_internal \"ERROR SHCArtifactId - *This GUID does not match the member's current GUID\" sourcetype=splunkd (`splunkadmins_splunkd_source`) `searchheadhosts` \n| eventstats max(_time) AS lasterror, min(_time) AS firsterror\n| cluster showcount=true \n| table host, cluster_count, _raw, lasterror, firsterror\n| eval lasterror = strftime(lasterror, \"%+\"), firsterror = strftime(firsterror, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "In this scenario either something has changed or one or more search heads are not syncing the artifacts as expected, a restart of the SH cluster usually resolves this.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "54 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Saved Searches with privileged owners and excessive write perms",
    "search": "| rest /servicesNS/-/-/saved/searches splunk_server=local \n| fields title, eai:acl.sharing, eai:acl.perms.read, eai:acl.perms.write, description, disabled, eai:acl.owner, dispatchAs, eai:acl.app \n| search `comment(\"Find alerts that the owner is set to a user (not nobody), and the sharing is non-private, and finally the owner has an admin or power role\")` eai:acl.owner!=\"nobody\" eai:acl.sharing!=\"user\" dispatchAs=owner \n    [| rest /services/authentication/users `searchheadsplunkservers` \n    | search roles=admin OR roles=power \n    | fields title \n    | rename title AS eai:acl.owner] \n| eval writeCount=mvcount('eai:acl.perms.write') \n| eval writePerms=mvjoin('eai:acl.perms.write', \",\") \n| search `comment(\"Exclude by macro\")` `splunkadmins_privilegedowners` \n| search `comment(\"If only the admin or power role can write to the alert then it's no problem...\")` NOT (writeCount=1 (eai:acl.perms.write=\"admin\" OR eai:acl.perms.write=\"power\")) \n| search `comment(\"power users have admin-like abilities, these users have a similar level of read access so less of a security concern...\")` writePerms!=\"admin,power\" \n| search `comment(\"If the alert is coming from an application that only admins can see to I'm not concerned as the user should not be able to access the app to edit the search...(in theory). We also ignore if there are no read permissions at all...\")` \n    NOT ( \n    [| rest /services/apps/local splunk_server=local \n    | fields title, visible, eai:acl.perms.read \n    | search `comment(\"If we cannot access the application then I'm assuming making it visible does not matter...\")` visible=1 \n    | eval readCount=mvcount('eai:acl.perms.read') \n    | search `comment(\"If the application can only be written to by admin or power users, then we can safely ignore the alerts within it...\")` (readCount=1 (eai:acl.perms.read=\"admin\" OR eai:acl.perms.read=\"power\") ) \n    | fields title \n    | rename title AS eai:acl.app]) \n| where isnotnull('eai:acl.perms.write') \n| rename title AS \"Alert Name\", eai:acl.perms.read AS \"read perms\", eai:acl.perms.write AS \"write perms\", eai:acl.app AS app, eai:acl.owner AS owner \n| sort app, \"Alert Name\" \n| table app, \"Alert Name\", description, disabled, \"read perms\", \"write perms\", owner",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "This is a rudimentary way of detecting scheduled searches or reports that could be used by a non-privileged user to run the alert/report as a privileged user through search scheduling functionality. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "56 5 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - SavedSearches using special characters",
    "search": "| rest \"/servicesNS/-/-/saved/searches\" splunk_server=local \n| regex search!=\"(?s)^[\\\\\\d\\s\\w\\|`\\\"\\*\\(\\)\\[\\]\\+=\\-;:!,\\./%\\?<>{}^#$@'&~]+$\" \n| rex field=search \"(?s)(?P<before_special_character>^[\\\\\\d\\s\\w\\|`\\\"\\*\\(\\)\\[\\]\\+=\\-;:!,\\./%\\?<>{}^#$@'&~]*)(?P<special_character>.)\" \n| rename eai:acl.owner AS owner, eai:acl.sharing AS sharing, eai:acl.app AS app \n| table title, owner, app, sharing, special_character, before_special_character",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Find special characters in saved searches, they are often copy & pasted in from another application and break searches",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Scheduled Search Efficiency",
    "search": "`comment(\"This likely came from a Splunk conf presentation but I cannot remember which one so cannot attribute the original author!\")`\n`comment(\"Determine the length of time a scheduled search takes to run compared to how often it is configured to run, excluding acceleration jobs\")`\nindex=_internal `searchheadhosts` sourcetype=scheduler source=*scheduler.log (user=*) savedsearch_name!=\"_ACCELERATE_DM*\"\n| stats avg(run_time) as average_runtime_in_sec count(savedsearch_name) as num_times_per_week sum(run_time) as total_runtime_sec by savedsearch_name user app host\n| eval ran_every_x_mins=round(60/(num_times_per_week/168))\n| eval average_runtime_duration=tostring(round(average_runtime_in_sec/60,2), \"duration\")\n| eval average_runtime_in_sec=round(average_runtime_in_sec, 2)\n| eval efficiency=round(((60/(num_times_per_week/168))/(average_runtime_in_sec/60)), 2)\n| sort efficiency\n| table savedsearch_name, app, average_runtime_duration, num_times_per_week, ran_every_x_mins, efficiency, user, host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-30d@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report was orginally found on answers or a Splunk conf talk, it lists the scheduled searches, how often they run and how long they have taken to run",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Scheduled Searches Configured with incorrect sharing",
    "search": "| rest /servicesNS/-/-/saved/searches `splunkadmins_restmacro`\n| search `comment(\"The problem with alerts that are configured privately is that no one beyond the author can use the results link and non-admins cannot even see the alert in Splunk!\")`\n`comment(\"Therefore we find anything that emails that is not shared correctly *and* anything that uses a script as often the script will include a results link.\")`\n`comment(\"The idea here is to let the end user know so they can share it appropriately, the noop search is excluded to remove scheduled views from this list\")`\nis_scheduled=1 disabled=0 eai:acl.sharing!=\"global\" eai:acl.sharing!=\"app\" search!=\"| noop\" actions!=\"\" `splunkadmins_scheduled_incorrectsharing`\n| eval numberOfEmailed = mvcount(split('action.email.to',\"@\"))-1\n| table title, eai:acl.app, author, eai:acl.sharing, actions, action.email.to, numberOfEmailed\n| where numberOfEmailed>1 OR isnull(numberOfEmailed)\n| sort author",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. These searches are triggering scripts or alerts which will provide a results link to Splunk. But the sharing is not app or global and therefore the link is unusable to anyone who is not the owner...Can be fixed by the end user? Yes. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 5 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Scheduled Searches That Cannot Run",
    "search": "`comment(\"These searches are scheduled but for some reason cannot run (eg. invalid search syntax)\")`\nindex=_internal `searchheadhosts` sourcetype=scheduler `splunkadmins_scheduledsearches_cannot_run`\n`comment(\"Additional rex due to someone using app= inside their saved search name...\")`\n| rex \"app=\\\"(?P<app>[^\\\"]+)\\\"\"\n| eval message=coalesce(message,event_message)\n| fillnull message\n| search `comment(\"The below 3 lines will catch map/lookup errors or similar that look like message=\\\"Error in 'map': Did not find value for required attribute 'attr'.\\\". No actions executed. This may or may not be a good thing...\")` \n| rex \"- savedsearch_id=\\\"(?P<user2>[^;]+);(?P<app2>[^;]+);(?P<savedsearch_name_2>[^\\\"]+)\" \n| eval savedsearch_name=coalesce(savedsearch_name,savedsearch_name_2), app=coalesce(app,app2), user=coalesce(user,user2) \n| fillnull status value=\"error\" \n| eventstats values(user) AS user by savedsearch_name, app \n| eval user=if(mvcount(user)>1,mvfilter(!(match(user, \"nobody\"))),user) \n| stats max(_time) AS mostRecentlySeen, values(success) AS success by message, savedsearch_name, app, log_level, user, status \n| stats count(eval(status=\"success\")) AS successCount, count(eval(success==0)) AS reportFailureCount, count(eval(searchmatch(\"log_level=WARN OR log_level=ERROR OR status=delegated_remote_error\"))) AS warnerrorcount, max(mostRecentlySeen) AS mostRecentlySeen, values(status) AS status, values(message) AS message by savedsearch_name, app, user\n| where warnerrorcount>0\n| append \n    [ search index=_internal `searchheadhosts` sourcetype=scheduler status=delegated_remote_error \n    | eval message=coalesce(message,event_message)\n    | stats max(_time) AS mostRecentlySeen, first(message) AS message by savedsearch_name, app, log_level, user, status] \n| selfjoin overwrite=true keepsingle=true savedsearch_name, app, user \n| append \n    [ search `comment(\"macro failures in the search syntax result in the log only appearing in splunkd, and the absence of delegated_remote_completion in scheduler.log\")` \n        index=_internal `searchheadhosts` ERROR \"failed job\" sourcetype=splunkd `splunkadmins_splunkd_source` saved_search=* \n    | search `comment(\"Exclude time periods where shutdowns were occurring\")` AND NOT \n        [ `splunkadmins_shutdown_time(searchheadhosts,0,0)`] \n    | rex \"saved_search=([^;]+);(?P<app>[^;]+);(?P<savedsearch_name>.*?) err=\" \n    | rex \"(?P<messagewithoutheader>saved_search=.*uri=)http(s)?://[^/]+(?P<messagewithoutheader2>.*)\" \n    | eval messagewithoutheader=messagewithoutheader . messagewithoutheader2 \n    | eval message=coalesce(message,event_message)\n    | stats max(_time) AS mostRecentlySeen, first(message) AS message by messagewithoutheader, savedsearch_name, app \n    | eval log_level=\"ERROR\" \n    | fields - messagewithoutheader \n    | sort - mostRecentlySeen] \n| selfjoin overwrite=true keepsingle=true savedsearch_name, app \n| where successCount<1 \n| sort - warnerrorcount, savedsearch_name \n| rename message as Message, count as runCount \n| eval mostRecentlySeen = strftime(mostRecentlySeen, \"%+\") \n| fields - cluster_label, status, savedsearch_id, host, status",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-8h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. As found in the DMC console, moving it into an alert so we can get alerted to the problem rather than checking a dashboard/log about this. Can be fixed by the end user? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "16 6,10,18 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Scheduled Searches without a configured earliest and latest time",
    "search": "| rest /servicesNS/-/-/saved/searches `splunkadmins_restmacro` \n| search `comment(\"Find scheduled searches where they are searching over all time, this is generally not good practice and can cause performance issues\")`\ndispatch.earliest_time=\"\" OR dispatch.earliest_time=\"0\" next_scheduled_time!=\"\" `splunkadmins_scheduledsearches_without_earliestlatest`\n| table title author, realtime_schedule, cron_schedule, description, disabled, dispatch.earliest_time, dispatch.latest_time, eai:acl.app, eai:acl.owner, updated, qualifiedSearch, is_scheduled, is_visible, next_scheduled_time, splunk_server \n| regex qualifiedSearch=\"^\\s*(search|tstats) \" \n| rex field=qualifiedSearch \"earliest=(?P<earliestTime>\\S+)\"\n| where isnull(earliestTime) \n| fields - earliestTime\n| rename eai:acl.owner AS owner, eai:appName AS Application",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. A scheduled search without time limits could kill the Splunk indexers with CPU / IO issues depending on the criteria of the search. Can be fixed by the end user? Yes. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "8 0,4,8,12,16,20 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Scheduled searches failing in cluster with 404 error",
    "search": "`comment(\"A 404 error appearing on some search head peers but not others might imply a synchronisation issue within the search head cluster has occurred, this might require correction potentially through a re-sync http://docs.splunk.com/Documentation/Splunk/latest/DistSearch/HowconfrepoworksinSHC#Replication_synchronization_issues \")`\nindex=_internal `searchheadhosts` \"find saved search with name\" sourcetype=splunkd `splunkadmins_splunkd_source`\n| rex field=err \"/servicesNS/(?P<username>[^/]+)/(?P<appName>[^/]+)\"\n| rex \"'(?P<searchname>[^']+)'.$\"\n| eval message=coalesce(message,event_message)\n| stats count, min(_time) AS firstSeen, max(_time) AS lastSeen, values(username) AS username, values(appName) AS appName, values(searchname) AS searchName by message, peer\n| eval firstSeen = strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\")\n| table username, appName, searchName, firstSeen, lastSeen, count, peer, message",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. If 404's are occurring within the search head cluster then it is possible there is a member out of sync...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "23 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Scheduled searches not specifying an index",
    "search": "| rest /servicesNS/-/-/saved/searches `splunkadmins_restmacro`\n| search `comment(\"Look over all scheduled searches and find those not specifying/narrowing down to an index, or using the index=* trick\")`\n| table title, eai:acl.owner, description, eai:acl.app, qualifiedSearch, next_scheduled_time\n| search next_scheduled_time!=\"\" `splunkadmins_scheduledsearches_without_index` \n| regex qualifiedSearch!=\".*index\\s*(!?)=\\s*([^*]|\\*\\S+)\" \n| regex qualifiedSearch=\"^\\s*search \"\n| regex qualifiedSearch!=\"^\\s*search\\s*\\[\\s*\\|\\s*inputlookup\"\n| rex field=qualifiedSearch \"(?s)^(?P<exampleQueryToDetermineIndexes>[^\\|]+)\"\n| regex exampleQueryToDetermineIndexes!=\"\\`\"\n| eval exampleQueryToDetermineIndexes=exampleQueryToDetermineIndexes . \"| stats values(index) AS index | format | fields search | eval search=replace(search,\\\"\\\\)\\\",\\\"\\\"), search=replace(search,\\\"\\\\(\\\",\\\"\\\"), search=if(search==\\\"NOT \\\",\\\"No indexes found\\\",search)\"\n| rename eai:acl.owner AS owner, eai:acl.app AS Application",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. These searches are either using index=* or not specifying an index at all and relying on the default set of indexes. Can be fixed by the end user? Yes. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "14 6 * * 1-5",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Scheduled searches not specifying an index macro version",
    "search": "| rest /servicesNS/-/-/saved/searches\n| search `comment(\"Look over all scheduled searches and find those not specifying/narrowing down to an index, or using the index=* trick. This version is dealing with those using macros. Please ensure the SearchHeadLevel - Macro report is also enabled for this to work as expected. Attempted to use https://answers.splunk.com/answers/186698/how-can-i-expand-a-macro-definition-in-the-search.html but the intentionsparser does not work via a REST call within Splunk, only by an external call in Splunk 7...\")` `splunkadmins_scheduledsearches_without_index_macro`\n| table title , description, eai:acl.app, eai:acl.owner, qualifiedSearch, next_scheduled_time \n| search next_scheduled_time!=\"\" \n| regex qualifiedSearch!=\".*index\\s*(!?)=\\s*([^*]|\\*\\S+)\" \n| regex qualifiedSearch=\"^\\s*search \" \n| rex field=qualifiedSearch \"(?s)^(?P<exampleQueryToDetermineIndexes>[^\\|]+)\"\n| regex exampleQueryToDetermineIndexes=\"`\" \n| rename eai:acl.owner AS owner, eai:acl.app AS Application \n| fields title, owner, description, Application, qualifiedSearch, next_scheduled_time \n| eval search=qualifiedSearch \n| `splunkadmins_audit_logs_macro_sub_v8` \n| `splunkadmins_audit_logs_macro_sub_v8` \n| eval exampleQueryToDetermineIndexes=exampleQueryToDetermineIndexes . \"| stats values(index) AS index | format | fields search | eval search=replace(search,\\\"\\\\)\\\",\\\"\\\"), search=replace(search,\\\"\\\\(\\\",\\\"\\\"), search=if(search==\\\"NOT \\\",\\\"No indexes found\\\",search)\"\n| stats values(search) AS search, first(exampleQueryToDetermineIndexes) AS exampleQueryToDetermineIndexes by title, owner, description, Application, next_scheduled_time \n| nomv search \n| regex search!=\".*index\\s*(!?)=\\s*([^*]|\\*\\S+)\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. These searches are either using index=* or not specifying an index at all and relying on the default set of indexes. Can be fixed by the end user? Yes. Search Head specific? Yes. Please ensure the SearchHeadLevel - Macro report is also enabled for this to work as expected",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "37 6 * * 1-5",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Scheduled searches status",
    "search": "| rest /servicesNS/-/-/saved/searches `splunkadmins_restmacro`\n  | fields qualifiedSearch, next_scheduled_time, title, eai:acl.owner, eai:acl.app\n  | search `comment(\"Based on the discussion with Burch on https://answers.splunk.com/answers/702075/what-is-the-best-way-to-find-searches-without-sour.html just a simple way of reporting if sourcetype/index fields are used in saved searches\")`\n  | where match( qualifiedSearch , \"^\\s*search\\s*\" )\n  | rex field=qualifiedSearch \"(?s)^(?<base_search>search[^\\|\\[]+)\"\n  | eval\n      check-sourcetype = if( match( base_search , \"\\s+sourcetype\\s*=\" ) , \"defined\" , \"missing\" ) ,\n      check-index = if( match( base_search , \"\\s+index\\s*(=|IN)\" ) , \"defined\" , \"missing\" ) ,\n      check-index-contains-wildcard = if( match( base_search , \"\\s+index\\s*(=\\s*[^\\*]+(\\s|$)|IN\\s*\\([^\\)\\*]+\\s*\\))\" ) , \"missing\" , \"defined\" ) ,\n      check-index-starts-wildcard = if( match( base_search , \"\\s+index\\s*(=\\s*\\*|IN\\s*\\(\\s*\\*)\" ) , \"defined\" , \"missing\" ) ,\n      check-hidden = if( match( base_search , \"\\s+((tag|eventtype)\\s*=)\" ) , \"defined\" , \"missing\" ) ,\n      check-macro = if( match( base_search , \"\\`[^\\`]+\\`\" ) , \"defined\" , \"missing\" ) ,\n      check-scheduled = if( match( next_scheduled_time , \".+\" ) , \"defined\" , \"missing\" )\n  | rename eai:acl.* AS namespace-*\n  | search ( check-sourcetype=\"missing\" OR check-index=\"missing\" ) check-hidden=\"missing\" check-scheduled=\"defined\"\n  | table title, check-index, check-sourcetype, base_search, namespace-*, check-index-contains-wildcard, check-index-starts-wildcard, check-macro",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-30d@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Based on the discussion with Burch on https://answers.splunk.com/answers/702075/what-is-the-best-way-to-find-searches-without-sour.html just a simple way of reporting if sourcetype/index fields are used in saved searches",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Script failures in the last day",
    "search": "`comment(\"Shell scripts running from Splunk are failing to run or throwing errors, or another sendmodalert failure has occurred\")` \nindex=_internal `searchheadhosts` sourcetype=splunkd log_level=\"ERROR\" OR log_level=\"WARN\" `splunkadmins_scriptfailures` command=\"runshellscript\" OR ScriptRunner OR \"Alert script returned error code\" OR \"ERROR sendmodalert\" OR \"WARN  sendmodalert\" OR \"Killing script\" OR (\"ERROR SearchScheduler\" \"sendalert\") NOT sendemail.py NOT \"InsecureRequestWarning\" \n| bin _time span=30s \n| eval search_head=host\n| eval search_head_cluster=`search_head_cluster`\n| stats values(_raw) AS _raw by _time \n| search _raw!=\"*404: Not Found\" _raw!=\"*Connection refused>\" _raw!=\"*HTTP Error 403: Forbidden\" _raw!=\"*Name or service not known>\" NOT (_raw=\"*Connection reset by peer\" _raw=\"*sendalert' command*\") \n| rex field=_raw \"[/\\\\\\]dispatch[/\\\\\\](?P<sid_from_dispatch>[^/]+)\" \n| rex \"sid:(?P<sid2>\\S+)\" \n| eval sid=coalesce(sid2,sid_from_dispatch) \n| append \n    [ search index=_internal `searchheadhosts` sourcetype=scheduler WARN SavedSplunker maximum time allowed \n    | eval search_head=host \n    | eval search_head_cluster=`search_head_cluster` \n    | stats count, last(event_message) AS event_message, last(component) AS component, latest(_time) AS _time, values(search_head) AS search_head, values(search_head_cluster) AS search_head_cluster by savedsearch_id \n    | rex field=savedsearch_id \"^(?P<username3>[^;]+);(?P<app3>[^;]+);(?P<searchname>.*)\" \n    | eval _raw = \"count=\" . count . \" \" . component . \" \" . event_message \n    | rex field=_raw \"sid=\\\"(?P<sid>[^\\\"]+)\" ] \n| `search_type_from_sid(sid)` \n| lookup splunkadmins_rmd5_to_savedsearchname RMDvalue AS report OUTPUT savedsearch_name\n| eval searchname=coalesce(savedsearch_name, searchname) \n| `base64decode(base64appname)` \n| eval app4=\"N/A\" \n| eval app=coalesce(app,base64appname,app3,app4) \n| eval _raw=mvindex(_raw,0,20), searchname=mvindex(searchname,0,20) \n| `base64decode(base64username)` \n| eval username=coalesce(username,base64username,username3) \n| fillnull value=\"N/A\" app, searchname, username, search_head_cluster \n| stats count, values(_raw) AS _raw by app, searchname, username, search_head_cluster, _time \n| table _time, username, app, searchname, search_head_cluster, _raw",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Scripts or webhooks are throwing errors which may indicate an issue, requires \"SearchHeadLevel - RMD5 to savedsearch_name lookupgen report\" to translate the search name accurately.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 22 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Search Messages admins only",
    "search": "`comment(\"Attempt to find various messages in the splunk_search_messages which are related to scheduled searches or dashboards which may require correcting, ignore ad-hoc searches\")`\n`comment(\"This does require the limits.conf log_search_messages=true setting to be enabled to work\")`\n    index=_internal `searchheadhosts` sourcetype=splunk_search_messages (Unable peer) OR bundles OR corrupt OR connecting OR ReadWrite OR Socket OR Timed OR incomplete OR cleanly OR Timeout OR Timed OR process OR insufficient OR (bucket failed) OR \"occur when processing chunks in running lookup command\" OR \"because KV Store status is currently unknown\" OR (File line) OR (SearchPipelineExecutor NOT \"exceeded configured match_limit\") OR S2BucketCache OR DistributedSearchResultCollectionManager OR (\"Field extractor\" \"unusually slow\") OR \"line *:\" NOT \"Unable to find tag\" NOT \"Unable to parse the search\" NOT (\"Eventtype\" \"does not exist\") NOT \"Error in 'outputlookup' command: You have insufficient privileges\" NOT \"insufficient data in ITSI summary index for policies\" \nNOT (\"Failed to fetch REST endpoint\" \"/services/data/indexes-extended\" \"Check that the URI path provided exists in the REST API\" OR \"Not Found\")\n`splunkadmins_searchmessages_admin_1`\n    `comment(\"Potential issues that are not included SearchEvaluatorBasedExpander, shows if eventtypes/tags are disabled/do not exist or similar\")` \nNOT (\"Failed to fetch REST endpoint\" \"/services/data/indexes-extended\" \"Check that the URI path provided exists in the REST API\" OR \"Not Found\") NOT \"Found no results to append to collection\"\n`splunkadmins_searchmessages_admin_1`\n| `search_type_from_sid(sid)`\n| eval type=case(from==\"scheduler\",\"scheduled\",from==\"SummaryDirector\",\"acceleration\",isnotnull(searchname),\"dashboard\",1=1,\"ad-hoc\")\n| search `splunkadmins_searchmessages_admin_2`\n| `base64decode(base64username)` \n| `base64decode(base64appname)` \n| eval app3=\"N/A\" \n| eval report=coalesce(searchname,searchname2), app=coalesce(app,base64appname,app3), username=coalesce(username,base64username) \n| fillnull app, username, report, message, orig_component value=\"N/A\"\n| eval search_head=host\n| eval search_head_cluster=`search_head_cluster`\n| eval combined=message . type . orig_component . search_head_cluster\n| cluster showcount=true field=combined t=0.90\n| stats sum(cluster_count) AS count, latest(_time) AS _time, values(search_head_cluster) AS search_head_cluster, values(orig_component) AS orig_component, values(sid) AS search_ids by app, message, type\n| eval search_ids=mvindex(search_ids,0,10)\n| table count, app, message, _time, type, count, search_head_cluster, orig_component, search_ids\n| append [ | makeresults | eval count=99999, app=\"N/A\", message=\"cluster command in use, all apps/type/search head cluster may not be accurate. The type messages is the important point\" | fields - _time ]\n| sort - count, mostrecent",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. This is designed for use with something like sendresults to send the failures to the owner of the mentioned search, this does require the limits.conf setting log_search_messages=true.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "28 4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Search Messages field extractor slow",
    "search": "index=_internal `searchheadhosts` sourcetype=splunk_search_messages \"extractor\" \"slow\"\n| `search_type_from_sid(sid)`\n| eval type=case(from==\"scheduler\",\"scheduled\",from==\"SummaryDirector\",\"acceleration\",isnotnull(searchname),\"dashboard\",1=1,\"ad-hoc\")\n| multireport [ | `base64decode(base64username)` ] [ | eval keepme=\"yes\"]\n| `base64decode(base64appname)` \n| eval app3=\"N/A\" \n| eval report=coalesce(searchname,searchname2), app=coalesce(app,base64appname,app3)\n| rex field=message \"^(\\[subsearch\\])?\\s*\\[[^\\]]+\\]\\s+(?P<sub_message>.*?\\()\"\n| fillnull app, username, report, sub_message value=\"N/A\"\n| stats count, latest(_time) AS mostrecent, earliest(_time) AS firstseen, values(message) AS message, values(host) AS hosts by app, username, type, report, sub_message\n| lookup splunkadmins_rmd5_to_savedsearchname RMDvalue AS report OUTPUT savedsearch_name\n| eval report=case(match(report,\"^RMD\") AND isnotnull(savedsearch_name),savedsearch_name,match(report,\"^RMD\"),\"N/A\",1=1,report) \n| table username, app, report, message, mostrecent, firstseen, type, count, hosts\n| sort - mostrecent\n| eval mostrecent=strftime(mostrecent, \"%+\"), firstseen=strftime(firstseen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Splunk search messages are showing slow field extractor messages, this does require the limits.conf setting log_search_messages=true",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "52 5 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Search Messages user level",
    "search": "`comment(\"Attempt to find various messages in the splunk_search_messages which are related to scheduled searches or dashboards which may require correcting, ignore ad-hoc searches\")`\n`comment(\"This does require the limits.conf log_search_messages=true setting to be enabled to work\")`\nNOT \"KV Store lookup table is empty or has not yet been replicated to the search peer\" \n        index=_internal `searchheadhosts` sourcetype=splunk_search_messages \"MultiValueProcessor\" OR \"SearchStatusEnforcer\" OR \"SearchOperator\" OR \"SQL\" OR \"truncated\" OR \"script\" OR \"KV\" OR \"External\" OR \"outputcsv\" OR \"reset\" OR \"match_limit\" OR authorized OR terminated OR depth_limit OR driver OR dbx OR command OR java OR reading OR training OR DensityFunction OR (TERM(in-memory) limit) OR terminated OR invalid OR (missing NOT orig_component=\"SummaryIndexProcessor\") OR Unable OR reset OR AutoLookupDriver OR ImportError OR jdbc OR SSLError OR TERM(code=) OR REST OR SearchParser\n`comment(\"Potential issues that are not included SearchEvaluatorBasedExpander, shows if eventtypes/tags are disabled/do not exist or similar\")` `splunkadmins_searchmessages_user_1` \n    NOT \"KV Store lookup table is empty\" NOT \"message=Restricting results of the \\\"rest\\\" operator to the local instance because you do not have the\" NOT \"Failed to fetch REST endpoint uri=https://127.0.0.1:8089/services/data/indexes-extended/\" NOT \"Unexpected status for to fetch REST endpoint uri=https://127.0.0.1:8089/services/data/indexes-extended\" NOT \"Failed to fetch REST endpoint uri=https://127.0.0.1:8089/services/data/indexes\" NOT \"The REST request on the endpoint URI /services/data/indexes\" NOT \"message=Could not locate the time (_time) field on some results returned from the external search command 'curl'\" NOT \"message=Found no results to append to collection\" NOT \"The search you ran returned a number of fields that exceeded the current indexed field extraction limit\" NOT \"message=Found no results to append to collection\" NOT \"The search you ran returned a number of fields that exceeded the current indexed field extraction limit\" NOT \"Connection failed with Read Timeout\" NOT \"message=Search was canceled\" NOT \"message=Search auto-canceled\" NOT \"The timewrap command is designed to work on the output of timechart\" NOT (\"Field\" \"does not exist\") NOT \"Connection reset by peer\" NOT \"Reading error while waiting for peer\" NOT \"Restricting results of the \\\"rest\\\" operator to the local instance\" NOT \"occur when processing chunks in running lookup command\" NOT \"because KV Store initialization has not completed yet\" NOT \"The following options were specified but have no effect\" NOT \"https://127.0.0.1:8089/servicesNS/nobody/SA-ITOA/itoa_interface/generate_entity_filter\" NOT \"because KV Store status is currently unknown\" NOT (\"https://127.0.0.1:8089/services/server/introspection/kvstore/collectionstats\" OR \"https://127.0.0.1:8089/services/server/sysinfo\" (\"exists in the REST API\" OR \"Forbidden\")) NOT (\"https://127.0.0.1:8089/services/data/indexes-extended\" OR \"https://127.0.0.1:8089/services/data/indexes\" (\"Not Found\" OR \"exists in the REST API\")) NOT \"Only the last one will appear, and previous\" NOT (\"Field extractor\" \"unusually slow\") \n    NOT \"Unable to distribute to peer\" NOT (Eventtype \"does not exist or is disabled\") NOT \"Unable to find tag\" NOT \"reference cycle in the lookup configuration\" \n`comment(\"OR TERM(filters) was originally in the query, but the error \\\"Search filters specified using splunk_server/splunk_server_group do not match any search peer.\\\" can occur anytime there are zero results, even if the splunk_server=/splunk_server_group= was not the cause of the issue, therefore this particular warning is not useful in it's current form...\")` \n| regex sid!=\"^(rt_)?(ta_)?(subsearch_)*(nested_[^_]+_)?\\d+\" \n| `search_type_from_sid(sid)`\n| eval type=case(from==\"scheduler\",\"scheduled\",from==\"SummaryDirector\",\"acceleration\",isnotnull(searchname),\"dashboard\",1=1,\"ad-hoc\") \n| search `comment(\"Depending on how noisy this alert is you may wish to add type!=dashboard using the macro splunkadmins_searchmessages_user_2\")` NOT (\"command=\\\"predict\\\", Too few data points\" AND type=\"dashboard\") NOT (type=\"dashboard\" \"https://127.0.0.1:8089/servicesNS/-/-/admin/file-explorer\") NOT (type=\"dashboard\" \"https://127.0.0.1:8089/servicesNS/-/-/admin/file-explorer\" OR \"The specified span would result in too many\") NOT (type=\"ad-hoc\" \"DAG Execution Exception: Search has been cancelled\") `splunkadmins_searchmessages_user_2` \n| `base64decode(base64username)` \n| `base64decode(base64appname)` \n| eval app3=\"N/A\" \n| eval report=coalesce(searchname,searchname2), app=coalesce(app,base64appname,app3), username=coalesce(username,base64username) \n| fillnull app, username, report value=\"N/A\" \n| eval search_head=host \n| eval search_head_cluster=`search_head_cluster` \n| stats count, latest(_time) AS mostrecent, earliest(_time) AS firstseen, values(message) AS message, values(search_head_cluster) AS search_head_cluster, values(orig_component) AS orig_component, values(sid) AS search_ids by app, report, username, type \n| eval search_ids=mvindex(search_ids,0,10) \n| lookup splunkadmins_rmd5_to_savedsearchname RMDvalue AS report OUTPUT savedsearch_name \n| eval report=case(match(report,\"^RMD\") AND isnotnull(savedsearch_name),savedsearch_name,match(report,\"^RMD\"),\"N/A\",1=1,report) \n| eval reason=case(type==\"dashboard\",\"Errors from viewing one or more dashboards, the dashboard owner can likely fix this if you can determine which dashboard is an issue, or contact the Splunk admin team\",type==\"scheduled\",\"Please review and correct this error or contact the Splunk admin team for assistance\",type==\"acceleration\",\"Broken acceleration/summary search, admin investigation required via audit index\",1=1,\"Unknown type\") \n| eval message=mvindex(message,0,30) \n| table username, reason, app, report, message, mostrecent, firstseen, type, count, search_head_cluster, orig_component, search_ids \n| eval mostrecent=strftime(mostrecent, \"%+\"), firstseen=strftime(firstseen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. This is designed for use with something like sendresults to send the failures to the owner of the mentioned search, this does require the limits.conf setting log_search_messages=true. This alert relies on \"SearchHeadLevel - RMD5 to savedsearch_name lookupgen report\" to obtain accurate results for the savedsearch name",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "28 4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Search Queries By Type Audit Logs",
    "search": "`comment(\"Based on the audit logs attempt to determine which types of searches are running and provide a rough % for each one\")`\n    index=_audit \", info=granted \" `searchheadhosts` \"search='\" search_id!=\"'rsa_*\"\n| rex \"(?s), search='(?P<search>.*)\\]$\" \n| rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n| rex field=search \"^(\\s*\\|)?(?P<searchbeforepipe>[^|]+)\" \n| rex mode=sed field=searchbeforepipe \"s/search \\(index=\\* OR index=_\\*\\) index=/search index=/\"\n| rex mode=sed field=searchbeforepipe \"s/search index=\\s*\\S+\\s+index=/search index=/\"\n| eval indexNotSpecified = if(NOT match(searchbeforepipe,\"(index(::|\\s*=))|(index\\s*IN)\") AND match(searchbeforepipe,\"^\\s*search \"),\"1\",\"0\")\n| eval macroWithIndexClause = if(isnotnull(searchbeforepipe) AND (match(searchbeforepipe,\"(?s)^\\s*search\\s.*(index(\\s*=|::)|(index\\s*IN)\") AND match(searchbeforepipe,\"`\")),\"1\",\"0\")\n| stats count, count(eval(match(searchbeforepipe,\"(index(\\s*=|::))|(index\\s*IN)\"))) AS indexClause, count(eval(match(searchbeforepipe,\"(index(\\s*=|::)\\s*\\S*\\*)|(index\\s+IN\\s*\\([^\\)]*\\*)\"))) AS indexWildcard, count(eval(match(searchbeforepipe,\"\\`[^\\`]+\\`\"))) AS macroNoIndex, count(eval(match(search,\"^\\s*\\|\\s*summarize\"))) AS summarize, count(eval(match(search,\"(?i)^\\s*\\|\\s*savedsearch\"))) AS savedsearch, count(eval(match(search,\"(?i)^\\s*\\|\\s*(from\\s*)?datamodel\"))) AS datamodel, count(eval(match(search,\"(?i)^\\s*\\|\\s*loadjob\"))) AS loadjob, count(eval(match(search,\"(?i)^\\s*\\|\\s*(multisearch|union)\"))) AS multisearch, count(eval(match(search,\"(?i)^\\s*\\|\\s*(pivot)\"))) AS pivot, count(eval(match(search,\"(?i)^\\s*\\|\\s*(metadata)\"))) AS metadata, count(eval(indexNotSpecified==1)) AS indexNotSpecified, count(eval(macroWithIndexClause==1)) AS macroWithIndexClause, count(eval(match(search,\"(?i)^\\s*\\|\\s*(tstats)\"))) AS tstats, count(eval(match(search,\"(?i)^\\s*\\|\\s*(rest)\"))) AS rest, count(eval(match(search,\"(?i)^\\s*\\|\\s*(mcatalog|mstats)\"))) AS metrics, count(eval(match(search,\"(?i)^\\s*\\|\\s*(from\\s+)?inputlookup\"))) AS inputlookup, count(eval(match(search_id,\"^'ta_\"))) AS typeahead\n| eval macroNoIndex = macroNoIndex-macroWithIndexClause, indexClause = indexClause - indexWildcard\n| eval unknown = count - (indexClause + macroNoIndex + summarize + savedsearch + datamodel + loadjob + multisearch + pivot + metadata + indexNotSpecified + tstats + rest + metrics + inputlookup + typeahead)\n| fields - macroWithIndexClause, count\n| transpose column_name=\"xaxis\" header_field=\"perc\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. A pie graph to show statistics on the number of searches by type of search (index specified, index wildcard used) et cetera, \"SearchHeadLevel - Search Queries By Type Audit Logs macro version\" includes macro substitution but is otherwise the same report",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Search Queries By Type Audit Logs macro version",
    "search": "`comment(\"Based on the audit logs attempt to determine which types of searches are running and provide a rough % for each one\")`\n    index=_audit `searchheadhosts` \", info=granted \" \"search='\" search_id!=\"'rsa_*\"\n| rex \"(?s), search='(?P<search>.*)\\]$\" \n| rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n| `splunkadmins_audit_logs_macro_sub_v8` \n| `splunkadmins_audit_logs_macro_sub_v8` \n| rex field=search \"(?s)^(\\s*\\|)?(?P<searchbeforepipe>[^|]+)\" \n| rex mode=sed field=searchbeforepipe \"s/search \\(index=\\* OR index=_\\*\\) index=/search index=/\" \n| rex mode=sed field=searchbeforepipe \"s/search index=\\s*\\S+\\s+index=/search index=/\" \n| eval indexNotSpecified = if(NOT match(searchbeforepipe,\"(index(\\s*=|::))|(index\\s*IN)\") AND match(searchbeforepipe,\"^\\s*search \"),\"1\",\"0\")\n| eval macroWithIndexClause = if(isnotnull(searchbeforepipe) AND (match(searchbeforepipe,\"(?s)^\\s*search\\s.*(index(\\s*=|::))|(index\\s*IN)\") AND hasMacro==\"1\"),\"1\",\"0\")\n| stats count, count(eval(match(searchbeforepipe,\"(index(\\s*=|::))|(index\\s*IN)\"))) AS indexClause, count(eval(match(searchbeforepipe,\"(index(\\s*=\\s*|::)\\S*\\*)|(index\\s+IN\\s*\\([^\\)]*\\*)\"))) AS indexWildcard, count(eval(hasMacro==\"1\")) AS macroNoIndex, count(eval(match(search,\"^\\s*\\|\\s*summarize\"))) AS summarize, count(eval(match(search,\"(?i)^\\s*\\|\\s*savedsearch\"))) AS savedsearch, count(eval(match(search,\"(?i)^\\s*\\|\\s*(from\\s*)?datamodel\"))) AS datamodel, count(eval(match(search,\"(?i)^\\s*\\|\\s*loadjob\"))) AS loadjob, count(eval(match(search,\"(?i)^\\s*\\|\\s*(multisearch|union)\"))) AS multisearch, count(eval(match(search,\"(?i)^\\s*\\|\\s*(pivot)\"))) AS pivot, count(eval(match(search,\"(?i)^\\s*\\|\\s*(metadata)\"))) AS metadata, count(eval(indexNotSpecified==1)) AS indexNotSpecified, count(eval(macroWithIndexClause==1)) AS macroWithIndexClause, count(eval(match(search,\"(?i)^\\s*\\|\\s*(tstats)\"))) AS tstats, count(eval(match(search,\"(?i)^\\s*\\|\\s*(rest)\"))) AS rest, count(eval(match(search,\"(?i)^\\s*\\|\\s*(mcatalog|mstats)\"))) AS metrics, count(eval(match(search,\"(?i)^\\s*\\|\\s*(from\\s+)?inputlookup\"))) AS inputlookup, count(eval(match(search_id,\"^'ta_\"))) AS typeahead\n| eval indexClause = indexClause-indexWildcard, macroNoIndex = macroNoIndex-macroWithIndexClause\n| eval total = indexClause + indexWildcard + macroNoIndex + summarize + savedsearch + datamodel + loadjob + multisearch + pivot + metadata + indexNotSpecified + tstats + rest + metrics + typeahead\n| eval unknown = count - total\n| fields - total, searchcommandcount, macroWithIndexClause, count\n| transpose column_name=\"xaxis\" header_field=\"perc\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. A pie graph to show statistics on the number of searches by type of search (index specified, index wildcard used) et cetera this version attempts to substitute macros, \"SearchHeadLevel - Search Queries By Type Audit Logs\" does not include macro substitution but is otherwise the same report. Requires \"SearchHeadLevel - Macro report\"",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Search Queries By Type Audit Logs macro version other",
    "search": "`comment(\"Based on the audit logs attempt to determine which types of searches are running and provide a rough % for each one\")`\n    index=_audit `searchheadhosts` \", info=granted \" \"search='\" search_id!=\"'rsa_*\"\n| rex \"(?s), search='(?P<search>.*)\\]$\" \n| rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n| `splunkadmins_audit_logs_macro_sub_v8` \n| `splunkadmins_audit_logs_macro_sub_v8` \n| rex field=search \"(?s)^(\\s*\\|)?(?P<searchbeforepipe>[^|]+)\"\n| rex mode=sed field=searchbeforepipe \"s/search \\(index=\\* OR index=_\\*\\) index=/search index=/\"\n| rex mode=sed field=searchbeforepipe \"s/search index=\\s*\\S+\\s+index=/search index=/\"\n| eval indexNotSpecified = if(NOT match(searchbeforepipe,\"(index(\\s*=|::))|(index\\s*IN)\") AND match(searchbeforepipe,\"^\\s*search \"),\"1\",\"0\")\n| eval macroWithIndexClause = if(isnotnull(searchbeforepipe) AND (match(searchbeforepipe,\"(?s)^\\s*search\\s.*(index(\\s*=|::))|(index\\s*IN)\") AND hasMacro==\"1\"),\"1\",\"0\")\n| eval indexClause = if(match(searchbeforepipe,\"(index(\\s*=|::))|(index\\s*IN)\"),\"1\",\"0\")\n| eval indexWildcard = if(match(searchbeforepipe,\"(index(\\s*=\\s*|::)\\S*\\*)|(index\\s+IN\\s*\\([^\\)]*\\*)\"),\"1\",\"0\")\n| eval macroNoIndex = if(hasMacro==\"1\",\"1\",\"0\")\n| eval summarize = if(match(search,\"^\\s*\\|\\s*summarize\"),\"1\",\"0\")\n| eval savedsearch = if(match(search,\"(?i)^\\s*\\|\\s*(from\\s+)?savedsearch\"),\"1\",\"0\")\n| eval datamodel = if(match(search,\"(?i)^\\s*\\|\\s*(from\\s+)?datamodel\"),\"1\",\"0\")\n| eval loadjob = if(match(search,\"(?i)^\\s*\\|\\s*loadjob\"),\"1\",\"0\")\n| eval multisearch = if(match(search,\"(?i)^\\s*\\|\\s*(multisearch|union)\"),\"1\",\"0\")\n| eval pivot = if(match(search,\"(?i)^\\s*\\|\\s*(pivot)\"),\"1\",\"0\")\n| eval metadata = if(match(search,\"(?i)^\\s*\\|\\s*(metadata)\"),\"1\",\"0\")\n| eval tstats = if(match(search,\"(?i)^\\s*\\|\\s*(tstats)\"),\"1\",\"0\")\n| eval rest = if(match(search,\"(?i)^\\s*\\|\\s*(rest)\"),\"1\",\"0\")\n| eval inputlookup = if(match(search,\"(?i)^\\s*\\|\\s*(from\\s+)?inputlookup\"),\"1\",\"0\")\n| eval metrics = if(match(search,\"(?i)^\\s*\\|\\s*(mcatalog|mstats)\"),\"1\",\"0\")\n| eval typeahead = if(match(search_id,\"^'ta_\"),\"1\",\"0\")\n| search indexClause=0 AND indexWildcard=0 AND macroNoIndex=0 AND summarize=0 AND savedsearch=0 AND datamodel=0 AND loadjob=0 AND pivot=0 AND multisearch=0 AND  metadata=0 AND indexNotSpecified=0 AND macroWithIndexClause=0 AND tstats=0 AND rest=0 AND inputlookup=0 AND metrics=0 AND typeahead=0\n| cluster  field=search t=0.01 showcount=true\n| table search, cluster_count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report relates to the \"SearchHeadLevel - Search Queries By Type Audit Logs\" and equivalent macro version but exists to print out the entries that did not fit into any of the categories. Requires \"SearchHeadLevel - Macro report\".",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Search Queries Per Day Audit Logs",
    "search": "`comment(\"Count the number of non-system Splunk queries that use a search command, excludes rest/metrics/data model acceleration et cetera\")`\nindex=_audit `searchheadhosts` \", info=granted \" \"search='search \" search_id!=\"'SummaryDirector_*\" search_id!=\"'rsa_*\" user!=admin user!=splunk-system-user \n| bin _time span=1d \n| rex \"info=granted , search_id='(?P<search_id>[^']+)\"\n| rex \"', savedsearch_name=\\\"(?P<savedsearch_name>[^\\\"]*)\"\n| `search_type_from_sid(search_id)` \n| stats dc(user) AS activeUserCount, count AS totalSearchCount, count(eval(type==\"scheduled\")) AS savedsearchCount, count(eval(type==\"dashboard\")) AS searchesFromDashboards, count(eval(type==\"ad-hoc\")) AS adhocSearches by _time",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-2d@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. A query to list the number of searches per/day by type of search (dashboard/saved search/ad-hoc)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "@d",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Search Queries summary exact match",
    "search": "| multisearch \n    [ search `comment(\"Last modified 2022-02-14 Attempt to extract out which indexes are accessed per search query by any search and compute statistics on them. The multisearch is only required if you want to capture sub-searches from join, append or similar, these require a bit more work so that's why the multisearch is there, in fact anything containing one of those keywords is dealt with in the second search, not this one...\")` \n    `comment(\"Note that the regexes need more work, for now, limits.conf [rex] match_limit = 1000000 is my workaround (main issue is the union/set/multisearch rex)\")` \n        index=_audit \"info=completed\" search_id!=\"'SummaryDirector_*\" search_id!=\"'rsa_*\" search_id!=\"'RemoteStorageRetrieveBuckets_*\" scan_count>0 \n    | rex \"(?s), search='(?P<search>.*)\\]$\" \n    | rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n    | rex field=search mode=sed \"s/```.*?```/ /g\" \n    | eval search=if(substr(search,len(search),len(search)-1)==\"'\",substr(search,0,len(search)-1),search) \n    | eval search_id=replace(search_id,\"'\",\"\") \n    | `search_type_from_sid(search_id)` \n    | `base64decode(base64appname)` \n    | eval app3=\"N/A\" \n    | eval app_name=coalesce(app,base64appname,app3) \n    | fillnull app_name value=\"*\" \n    | eval splunk_server = `splunkadmins_splunk_server_name` \n    | search `comment(\"Replace macros, but then replace datamodels, then tags, then eventtypes, but what if the eventtype refers to an eventtype? Or tag? Or more macros? This isn't perfect so just substitute a hope for the best. IndexerLevel - RemoteSearches Indexes Stats doesn't have all these issues so it may be safer to see what happens at indexing tier...Note pre Splunk 8.0 you will need to replace splunkadmins_audit_logs_macro_sub_v8 with splunkadmins_audit_logs_macro_sub\")` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | regex search=\"^\\s*(\\|?)\\s*(search|tstats|mstats|mcatalog|mutlisearch|union|set|summarize|datamodel|from\\s*:?\\s*datamodel|datamodelsimple)\\s+\" \n    | regex search!=\"\\|\\s*(append|union|multisearch|set|appendcols|appendpipe|join|map)\" \n    | `splunkadmins_audit_logs_datamodel_sub` \n    | `splunkadmins_audit_logs_tags_sub` \n    | `splunkadmins_audit_logs_eventtypes_sub` \n    | `splunkadmins_audit_logs_eventtypes_sub` \n    | `splunkadmins_audit_logs_tags_sub` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | rex field=search mode=sed \"s/```.*?```/ /g\" \n    | rex field=search \"(?s)^(?P<prepipe>\\s*\\|?([^\\|]+))\" ] \n    [ search `comment(\"Attempt to extract out which indexes are accessed per search query by any search and compute statistics on them. This search works on searches with an append/multisearch or other command that has a slightly different regex requirement. Note had to nomv the multivalued field before concatenation or it sliently disappeared!\")` \n        index=_audit \"info=completed\" search_id!=\"'SummaryDirector_*\" search_id!=\"'rsa_*\" search_id!=\"'RemoteStorageRetrieveBuckets_*\" scan_count>0 \n    | rex \"(?s), search='(?P<search>.*)\\]$\" \n    | rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n    | rex field=search mode=sed \"s/```.*?```/ /g\" \n    | eval search=if(substr(search,len(search),len(search)-1)==\"'\",substr(search,0,len(search)-1),search) \n    | eval search_id=replace(search_id,\"'\",\"\") \n    | `search_type_from_sid(search_id)` \n    | `base64decode(base64appname)` \n    | eval app3=\"N/A\" \n    | eval app_name=coalesce(app,base64appname,app3) \n    | fillnull app_name value=\"*\" \n    | eval splunk_server = `splunkadmins_splunk_server_name` \n    | search `comment(\"Replace macros, but then replace datamodels, then tags, then eventtypes, but what if the eventtype refers to an eventtype? Or tag? Or more macros? This isn't perfect so just substitute a hope for the best. IndexerLevel - RemoteSearches Indexes Stats doesn't have all these issues so it may be safer to see what happens at indexing tier...\")` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | regex search=\"\\|\\s*(append|union|multisearch|set|appendcols|appendpipe|join|map)\" \n    | `splunkadmins_audit_logs_datamodel_sub` \n    | `splunkadmins_audit_logs_tags_sub` \n    | `splunkadmins_audit_logs_eventtypes_sub` \n    | `splunkadmins_audit_logs_eventtypes_sub` \n    | `splunkadmins_audit_logs_tags_sub` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | rex field=search mode=sed \"s/```.*?```/ /g\" \n    | rex field=search max_match=50 \"(?s)\\|?\\s*(append|appendcols|appendpipe|map|union)\\s+\\[(?P<subsearch>.*?)\\]\\s*(\\||$)\" \n    | rex field=search max_match=50 \"(?s)\\|?\\s*(join)\\s+.*?\\[(?P<subsearch>.*?)\\]\\s*(\\||$)\" \n    | rex field=search max_match=50 \"(?s)\\|?\\s*(union|set|multisearch)\\s+(?P<part1>\\[.*?\\](\\s*\\[.*?\\])+\\s*(`[^`]+`\\s*)*(\\||$))\" \n    | rex field=part1 max_match=50 \"(?s).*?\\[(?P<subsearch>.*?)\\]\\s*(\\||$|)\" \n    | rex field=search max_match=50 \"(?s)\\|?\\s*(map)\\s+(maxsearches\\s*=\\s*\\d+)?\\s*search\\s*=\\s*\\\"(?P<subsearch>.*?)\\\"\\s*(\\||$)\" \n    | rex field=search \"^(?P<prepipe>\\s*\\|?([^\\|]+))\" \n    | rex field=subsearch \"(?s)^\\s*\\|?(?P<prepipe_subsearch>([^\\|]+))\" \n    | nomv prepipe_subsearch \n    | eval prepipe = prepipe . \" \" . prepipe_subsearch \n        ] \n| eval search=prepipe \n| search `comment(\"The (index=* OR index=_*) index=<specific index> is a common use case for enterprise security, also some individuals like doing a similar trick so remove the index=*... as this is not a wildcard index search\")` \n| rex field=search \"(?P<esstylewildcard>\\(\\s*index=\\*\\s+OR\\s+index=_\\*\\s*\\))\" \n| rex mode=sed field=search \"s/search index=\\s*\\S+\\s+index\\s*=/search index=/g\" \n| eval search_head=host \n| eval search_head_cluster=`search_head_cluster` \n| stats values(total_run_time) AS total_run_time, values(event_count) AS event_count, values(scan_count) AS scan_count, values(search) AS search, values(search_et) AS search_et, values(search_lt) AS search_lt, values(savedsearch_name) AS savedsearch_name, min(_time) AS timestamp, values(search_head_cluster) AS search_head_cluster, max(duration_command_search_index) AS duration_index, max(duration_command_search_rawdata) AS duration_rawdata, max(invocations_command_search_index_bucketcache_hit) AS cache_index_hits, max(invocations_command_search_index_bucketcache_miss) AS cache_index_miss, max(duration_command_search_index_bucketcache_hit) AS cache_index_hit_duration, max(duration_command_search_index_bucketcache_miss) AS cache_index_miss_duration, max(invocations_command_search_rawdata_bucketcache_hit) AS cache_rawdata_hits, max(invocations_command_search_rawdata_bucketcache_miss) AS cache_rawdata_miss, max(duration_command_search_rawdata_bucketcache_hit) AS cache_rawdata_hit_duration, max(duration_command_search_rawdata_bucketcache_miss) AS cache_rawdata_miss_duration, values(esstylewildcard) AS esstylewildcard by user, type, search_id, app_name \n| search `comment(\"We now deal with cases where search earliest/latest times were not specified, assume all time is about 1 year in the past and latest time was the search run time\")` \n| eval search_lt=if(search_lt==\"N/A\",timestamp,search_lt), search_et=if(search_et==\"N/A\",now()-(365*24*60*60),search_et) \n| search `comment(\"Extract out index= or index IN (a,b,c) but avoid NOT index in (...) and NOT index=... and also NOT (...anything) statements\")` \n| rex field=search \"(?s)(NOT\\s+index(\\s*=\\s*|::)[^ ]+)|(NOT\\s+\\([^\\)]+\\))|(index(\\s*=\\s*|::)\\\"?(?P<indexregex>[\\*A-Za-z0-9-_]+))\" max_match=50 \n| rex field=search \"(?s)(NOT\\s+index\\s+[iI][nN]\\s*\\([^\\)]+)|(index\\s+[iI][nN]\\s*\\((?P<indexin>([^\\)\\\"]+)|\\\"[^\\)\\\"]+\\\"))\" max_match=50 \n| makemv delim=\",\" indexin \n| eval indexes=mvappend(indexregex,indexin) \n| eval indexes=if(isnotnull(esstylewildcard),mvfilter(NOT match(indexes,\"^_?\\*$\")),indexes) \n| eval wildcard=mvfilter(match(indexes,\"\\*\")) \n| where isnull(wildcard) \n| eval indexes=mvmap(indexes, replace(lower(indexes), \"\\\"\", \"\")) \n| eval indexes=mvmap(indexes, trim(replace(indexes, \"'\", \"\"))) \n| eval indexes=mvdedup(indexes) \n| eval multi=if(mvcount(indexes)>1,\"true\",\"false\") \n| stats values(timestamp) AS _time, values(total_run_time) AS total_run_time, values(event_count) AS event_count, values(scan_count) AS scan_count, values(search_et) AS search_et, values(search_lt) AS search_lt, values(savedsearch_name) AS savedsearch_name, values(multi) AS multi, max(duration_index) AS duration_index, max(duration_rawdata) AS duration_rawdata, max(cache_index_hits) AS cache_index_hits, max(cache_index_miss) AS cache_index_miss, max(cache_index_hit_duration) AS cache_index_hit_duration, max(cache_index_miss_duration) AS cache_index_miss_duration, max(cache_rawdata_hits) AS cache_rawdata_hits, max(cache_rawdata_miss) AS cache_rawdata_miss, max(cache_rawdata_hit_duration) AS cache_rawdata_hit_duration, max(cache_rawdata_miss_duration) AS cache_rawdata_miss_duration by user, type, indexes, search_head_cluster, search_id, app_name \n| eval period=search_lt-search_et \n| fields - indexin, indexregex \n| search `comment(\"Commands like multikv result in giant event count numbers compared to scan count, lower the lispy back down to normal to prevent the stats from been broken. lispy efficiency as per Martin Muller's conf presentations\")` \n| eval lispy_efficiency = if(event_count>scan_count,scan_count,event_count) / scan_count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is an attempt to use the Splunk audit logs to generate summary statistics on what indexes were accessed and the period of time they were accessed over. There is a lot of complexity here as the audit logs make this task very challenging. This version relates to entries where index=<indexname> where used without wildcards, an additional report \"SearchHeadLevel - Search Queries summary non-exact match\" also exists to perform this same function without an index specified or when wildcards are used. This report requires \"SearchHeadLevel - Index access list by user\" and \"SearchHeadLevel - Macro report\". Also note that you need to remove the comment around the lookup command within the search...this report works in Splunk 8.0 or newer (or 7.3 with some changes). Requires the splunkadmins_macros lookup file to exist, the datamodels, eventtypes and tags lookup files should also exit for this to be accurate. Finally, you may wish to try the report \"IndexerLevel - RemoteSearches Indexes Stats\" this uses the remote_searches.log and doesn't need to work with macros or similar as it runs on the indexing tier...Note pre Splunk 8.0 you will need to replace splunkadmins_audit_logs_macro_sub_v8 with splunkadmins_audit_logs_macro_sub",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Search Queries summary exact match by index",
    "search": "| savedsearch \"SearchHeadLevel - Search Queries summary exact match\"\n| eval period=case(period<900,\"a<=15 minutes\",period<3600,\"b<=1hour\",period<=14400,\"c<=4hours\",period<=86400,\"d<=24hours\",period<=604800,\"e<=7days\",period<=1209600,\"f<=14days\",period<=2592000,\"g<=30days\",period<=5184000,\"h<=60days\",period<=7776000,\"i<=90days\",period<=15552000,\"j<=180days\",period>15552000,\"k>180days\")\n| stats count by indexes, period\n| rename indexes AS index\n| search `comment(\"Temporary hack to make this visualize without errors below...\")`\n| eval indexfirstletter=substr(index,0,1)\n| stats sum(count) AS count by indexfirstletter, period\n| xyseries period indexfirstletter count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report uses the \"SearchHeadLevel - Search Queries summary exact match\" report and then reports per index",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Search Queries summary exact match by user",
    "search": "| savedsearch \"SearchHeadLevel - Search Queries summary exact match\"\n| search `comment(\"TODO breakdown the counts into time periods, perhaps 1hour, 4 hours, 8 hours, 24 hours, 3 days, 7 days, 14 days, 21 days, 30 days, 2 months, 3 months, 6months, > 60 months? or similar. Do a count for each one as that would be useful for a dashboard...\")` \n| stats count, dc(indexes) AS indexes, max(period) AS maxPeriod, avg(period) AS avgPeriod, median(period) AS medianPeriod, \n    avg(total_run_time) AS avg_total_run_time, max(total_run_time) AS max_total_run_time, median(total_run_time) AS median_total_run_time, avg(lispy_efficiency) AS avg_lispy_efficiency, max(lispy_efficiency) AS max_lispy_efficiency, min(lispy_efficiency) AS min_lispy_efficiency, median(lispy_efficiency) AS median_lispy_efficiency by user\n| fillnull max_lispy_efficiency, min_lispy_efficiency, median_lispy_efficiency \n| eval maxPeriod=tostring(maxPeriod,\"duration\"), avgPeriod=tostring(avgPeriod,\"duration\"), medianPeriod=tostring(medianPeriod,\"duration\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report uses the \"SearchHeadLevel - Search Queries summary exact match\" report and then reports per user",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Search Queries summary non-exact match",
    "search": "| multisearch \n    [ search `comment(\"Last modified 2022-02-14 Attempt to extract out which indexes are accessed per search query by any search and compute statistics on them. The multisearch is only required if you want to capture sub-searches from join, append or similar, these require a bit more work so that's why the multisearch is there, in fact anything containing one of those keywords is dealt with in the second search, not this one...\")` \n    `comment(\"Note that the regexes need more work, for now, limits.conf [rex] match_limit = 1000000 is my workaround (main issue is the union/set/multisearch rex)\")` \n        index=_audit \"info=completed\" search_id!=\"'SummaryDirector_*\" search_id!=\"'rsa_*\" search_id!=\"'RemoteStorageRetrieveBuckets_*\" scan_count>0 \n    | rex \"(?s), search='(?P<search>.*)\\]$\" \n    | rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n    | rex field=search mode=sed \"s/```.*?```/ /g\" \n    | eval search=if(substr(search,len(search),len(search)-1)==\"'\",substr(search,0,len(search)-1),search) \n    | eval search_id=replace(search_id,\"'\",\"\") \n    | `search_type_from_sid(search_id)` \n    | `base64decode(base64appname)` \n    | eval app3=\"N/A\" \n    | eval app_name=coalesce(app,base64appname,app3) \n    | fillnull app_name value=\"*\" \n    | eval splunk_server = `splunkadmins_splunk_server_name` \n    | search `comment(\"Replace macros, but then replace datamodels, then tags, then eventtypes, but what if the eventtype refers to an eventtype? Or tag? Or more macros? This isn't perfect so just substitute a hope for the best. IndexerLevel - RemoteSearches Indexes Stats doesn't have all these issues so it may be safer to see what happens at indexing tier...\")` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | regex search=\"^\\s*(\\|?)\\s*(search|tstats|mstats|mcatalog|mutlisearch|union|set|summarize|datamodel|from\\s*:?\\s*datamodel|datamodelsimple)\\s+\" \n    | regex search!=\"\\|\\s*(append|union|multisearch|set|appendcols|appendpipe|join|map)\" \n    | `splunkadmins_audit_logs_datamodel_sub` \n    | `splunkadmins_audit_logs_tags_sub` \n    | `splunkadmins_audit_logs_eventtypes_sub` \n    | `splunkadmins_audit_logs_eventtypes_sub` \n    | `splunkadmins_audit_logs_tags_sub` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | rex field=search mode=sed \"s/```.*?```/ /g\" \n    | rex field=search \"(?s)^(?P<prepipe>\\s*\\|?([^\\|]+))\" ] \n    [ search `comment(\"Attempt to extract out which indexes are accessed per search query by any search and compute statistics on them. This search works on searches with an append/multisearch or other command that has a slightly different regex requirement. Note had to nomv the multivalued field before concatenation or it sliently disappeared!\")` \n        index=_audit \"info=completed\" search_id!=\"'SummaryDirector_*\" search_id!=\"'rsa_*\" search_id!=\"'RemoteStorageRetrieveBuckets_*\" scan_count>0 \n    | rex \"(?s), search='(?P<search>.*)\\]$\" \n    | rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n    | rex field=search mode=sed \"s/```.*?```/ /g\" \n    | eval search=if(substr(search,len(search),len(search)-1)==\"'\",substr(search,0,len(search)-1),search) \n    | eval search_id=replace(search_id,\"'\",\"\") \n    | `search_type_from_sid(search_id)` \n    | `base64decode(base64appname)` \n    | eval app3=\"N/A\" \n    | eval app_name=coalesce(app,base64appname,app3) \n    | eval splunk_server = `splunkadmins_splunk_server_name` \n    | fillnull app_name value=\"*\" \n    | search `comment(\"Replace macros, but then replace datamodels, then tags, then eventtypes, but what if the eventtype refers to an eventtype? Or tag? Or more macros? This isn't perfect so just substitute a hope for the best. IndexerLevel - RemoteSearches Indexes Stats doesn't have all these issues so it may be safer to see what happens at indexing tier...\")` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | regex search=\"\\|\\s*(append|union|multisearch|set|appendcols|appendpipe|join|map)\" \n    | `splunkadmins_audit_logs_datamodel_sub` \n    | `splunkadmins_audit_logs_tags_sub` \n    | `splunkadmins_audit_logs_eventtypes_sub` \n    | `splunkadmins_audit_logs_eventtypes_sub` \n    | `splunkadmins_audit_logs_tags_sub` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | `splunkadmins_audit_logs_macro_sub_v8` \n    | rex field=search mode=sed \"s/```.*?```/ /g\" \n    | rex field=search max_match=50 \"(?s)\\|?\\s*(append|appendcols|appendpipe|map|union)\\s+\\[(?P<subsearch>.*?)\\]\\s*(\\||$)\" \n    | rex field=search max_match=50 \"(?s)\\|?\\s*(join)\\s+.*?\\[(?P<subsearch>.*?)\\]\\s*(\\||$)\" \n    | rex field=search max_match=50 \"(?s)\\|?\\s*(union|set|multisearch)\\s+(?P<part1>\\[.*?\\](\\s*\\[.*?\\])+\\s*(`[^`]+`\\s*)*(\\||$))\" \n    | rex field=part1 max_match=50 \"(?s).*?\\[(?P<subsearch>.*?)\\]\\s*(\\||$|)\" \n    | rex field=search max_match=50 \"(?s)\\|?\\s*(map)\\s+(maxsearches\\s*=\\s*\\d+)?\\s*search\\s*=\\s*\\\"(?P<subsearch>.*?)\\\"\\s*(\\||$)\" \n    | rex field=search \"^(?P<prepipe>\\s*\\|?([^\\|]+))\" \n    | rex field=subsearch \"(?s)^\\s*\\|?(?P<prepipe_subsearch>([^\\|]+))\" \n    | nomv prepipe_subsearch \n    | eval prepipe = prepipe . \" \" . prepipe_subsearch ] \n| eval search=prepipe \n| search `comment(\"The (index=* OR index=_*) index=<specific index> is a common use case for enterprise security, also some individuals like doing a similar trick so remove the index=*... as this is not a wildcard index search\")` \n| rex field=search \"(?P<esstylewildcard>\\(\\s*index=\\*\\s+OR\\s+index=_\\*\\s*\\))\" \n| rex mode=sed field=search \"s/search index=\\s*\\S+\\s+index\\s*=/search index=/g\" \n| search `comment(\"Extract out index= or index IN (a,b,c) but avoid NOT index in (...) and NOT index=... and also NOT (...anything) statements\")` \n| rex field=search \"(?s)(NOT\\s+index(\\s*=\\s*|::)[^ ]+)|(NOT\\s+\\([^\\)]+\\))|(index(\\s*=\\s*|::)\\\"?(?P<indexregex>[\\*A-Za-z0-9-_]+))\" max_match=50 \n| rex field=search \"(?s)(NOT\\s+index\\s+[iI][nN]\\s*\\([^\\)]+)|(index\\s+[iI][nN]\\s*\\((?P<indexin>([^\\)\\\"]+)|\\\"[^\\)\\\"]+\\\"))\" max_match=50 \n| makemv delim=\",\" indexin \n| eval indexes=mvappend(indexregex,indexin) \n| eval indexes=if(isnotnull(esstylewildcard),mvfilter(NOT match(indexes,\"^_?\\*$\")),indexes) \n| eval wildcard=mvfilter(match(indexes,\"\\*\")) \n| where isnotnull(wildcard) OR isnull(indexes) \n| eval short=mvmap(indexes,if(len(indexes)<=3,\"True\",null())) \n| eval short=if(isnull(short),\"False\",\"True\") \n| search `comment(\"We now deal with cases where search earliest/latest times were not specified, assume all time is about 1 year in the past and latest time was the search run time\")` \n| eval search_lt=if(search_lt==\"N/A\",_time,search_lt), search_et=if(search_et==\"N/A\",now()-(365*24*60*60),search_et) \n| eval period=search_lt-search_et \n| search `comment(\"Now that we have a giant list of indexes, we want to strip any quote characters and lowercase them in case we use a kvstore for lookups or similar.\")` \n| search `comment(\"Run a lookup to find the default indexes and the allowed indexes per user\")` \n| eval roles=replace(roles,\"'\",\"\") \n| makemv roles delim=\"+\" \n| lookup splunkadmins_indexes_per_role roles, splunk_server \n| fields indexes, user, period, total_run_time, event_count, scan_count, srchIndexesAllowed, srchIndexesDefault, search_id, search_et, search_lt, host, app_name, savedsearch_name, type, duration_command_search_index, duration_command_search_rawdata, invocations_command_search_index_bucketcache_hit, invocations_command_search_index_bucketcache_miss, duration_command_search_index_bucketcache_hit, duration_command_search_index_bucketcache_miss, invocations_command_search_rawdata_bucketcache_hit, invocations_command_search_rawdata_bucketcache_miss, duration_command_search_rawdata_bucketcache_hit, duration_command_search_rawdata_bucketcache_miss, short, _time \n| makemv srchIndexesAllowed tokenizer=(\\S+) \n| streamfilterwildcard pattern=indexes fieldname=indexes srchIndexesAllowed \n| eval indexes=if(isnull(indexes),srchIndexesDefault,indexes) \n| eval indexes=mvmap(indexes, replace(lower(indexes), \"\\\"\", \"\")) \n| eval indexes=mvmap(indexes, trim(replace(indexes, \"'\", \"\"))) \n| makemv indexes tokenizer=(\\S+) \n| eval search_head=host \n| eval search_head_cluster=`search_head_cluster` \n| eval indexes=mvdedup(indexes) \n| eval multi=if(mvcount(indexes)>1,\"true\",\"false\") \n| stats values(_time) AS _time, values(total_run_time) AS total_run_time, values(event_count) AS event_count, values(scan_count) AS scan_count, values(search_et) AS search_et, values(search_lt) AS search_lt, values(savedsearch_name) AS savedsearch_name, values(multi) AS multi, max(duration_command_search_index) AS duration_index, max(duration_command_search_rawdata) AS duration_rawdata, max(invocations_command_search_index_bucketcache_hit) AS cache_index_hits, max(invocations_command_search_index_bucketcache_miss) AS cache_index_miss, max(duration_command_search_index_bucketcache_hit) AS cache_index_hit_duration, max(duration_command_search_index_bucketcache_miss) AS cache_index_miss_duration, max(invocations_command_search_rawdata_bucketcache_hit) AS cache_rawdata_hits, max(invocations_command_search_rawdata_bucketcache_miss) AS cache_rawdata_miss, max(duration_command_search_rawdata_bucketcache_hit) AS cache_rawdata_hit_duration, max(duration_command_search_rawdata_bucketcache_miss) AS cache_rawdata_miss_duration by user, type, search_id, indexes, search_head_cluster, app_name, short \n| eval period=search_lt-search_et \n| search `comment(\"Commands like multikv result in giant event count numbers compared to scan count, lower the lispy back down to normal to prevent the stats from been broken. lispy efficiency as per Martin Muller's conf presentations\")` \n| eval lispy_efficiency = if(event_count>scan_count,scan_count,event_count) / scan_count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is an attempt to use the Splunk audit logs to generate summary statistics on what indexes were accessed and the period of time they were accessed over. There is a lot of complexity here as the audit logs make this task very challenging. This version relates to entries where either index names are specified with wildcards or no index is specified, an additional report \"SearchHeadLevel - Search Queries summary exact match\" also exists to perform this same function where an index=<indexname> is specified. This report requires \"SearchHeadLevel - Index access list by user\" and \"SearchHeadLevel - Macro report\". Also note that you need to remove the comment around the lookup within the search...this report works on Splunk 8.0 or newer or 7.3 with some modification. Requires the splunkadmins_macros and splunkadmins_indexes_per_role lookup files to exist. Note pre Splunk 8.0 you will need to replace splunkadmins_audit_logs_macro_sub_v8 with splunkadmins_audit_logs_macro_sub. Note that this search utilises the streamfilterwildcard custom search command included in the TA-Alerts for SplunkAdmins application on SplunkBase (or github)",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Searches dispatched as owner by other users",
    "search": "index=_audit info=granted search_id!=\"'rsa_*\" `comment(\"Report on searches dispatched with owner (not user) setting and the search involved\")` \n| rex \"info=granted , search_id='(?P<search_id>[^']+)\" \n| search `comment(\"A regex that includes what a userid looks like within your company will likely be faster than trying to exclude all alternatives like the below...\")` \n| regex search_id!=\"^((subsearch_)?\\d|rt_|(subsearch_)?scheduler|SummaryDirector_|ta_|RemoteStorageRetrieveIndexes_|md_|subsearch_searchparsetmp| alertsmanager_|subsearch_AlertActionsRequredFields|alertsmanager_|sd_)\" \n| rex field=search_id \"(?P<from>[^_]+)((_(?P<base64owner>[^_]+))|(__(?P<owner>[^_]+)))\" \n| `base64decode(base64owner)` \n| eval owner=coalesce(owner,base64owner) \n| where from!=owner AND user!=from \n| rex \"(?s), search='(?P<search>.*)\\]$\" \n| rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n| rex \"', savedsearch_name=\\\"(?P<savedsearch_name>[^\\\"]*)\" \n| table from, owner, savedsearch_name, search",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report only advises which reports are been used via dispatch and running as the owner (not as the user), this is standard functionality in Splunk...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "38 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - SmartStore cache misses - combined",
    "search": "`searchheadhosts` (index=_audit action=search search_id NOT typeahead NOT \"search_id='rsa_*\") `comment(\"Original version by Nico Van Der Walt, modified by Gareth Anderson\")`\ninvocations_command_search_index_bucketcache_miss>0 OR invocations_command_search_rawdata_bucketcache_miss>0 TERM(info=*) TERM(UI:Dashboard:*) OR TERM(UI:Search) \n| eval total_days_searched=(search_lt-search_et)/86400 \n| eval total_hours_searched=total_days_searched*24 \n| eval total_hours_searched=round(total_hours_searched,1) \n| eval total_days_searched=round(total_days_searched,0) \n| eval search_id=trim(search_id,\"\\'\") \n| eval search_id=coalesce(search_id,sid) \n| eval origSid=search_id \n| rex field=search_id \"subsearch_(?<search_id>.*)_\\d+\\.\\d+\" \n| eval api_et=if(api_et=\"N/A\", search_et, api_et) \n| eval total_hours_searched=if(api_et=\"N/A\", \"AllTime\",total_hours_searched) \n| eval total_days_searched=if(api_et=\"N/A\", \"AllTime\",total_days_searched) \n| eval provenance=if(provenance=\"N/A\",NULL,provenance) \n| eval provenance=if(provenance=\"UI:LocateData\",NULL,provenance) \n| rex \"(?s), search='(?P<search>.*)\\]$\" \n| rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n| eval search=if(match(search,\"^'\"),mvindex(search,1),search) \n| stats latest(_time) AS mostRecent, values(host) as host sum(duration_command_search_rawdata_bucketcache_miss) AS duration__raw_cache_miss sum(invocations_command_search_index_bucketcache_miss) as count_index_cache_miss sum(invocations_command_search_rawdata_bucketcache_miss) as count_rawdata_cache_miss values(total_hours_searched) AS total_hours_searched values(total_days_searched) AS total_days_searched values(user) AS users last(search) AS search values(savedsearch_name) AS savedsearch_name max(total_run_time) AS run_time values(result_count) AS result_count values(event_count) AS event_count values(searched_buckets) AS searched_buckets values(info) AS info values(provenance) AS provenance dc(origSid) AS numofsearchesinquery by search_id \n| `search_type_from_sid(search_id)` \n| `base64decode(base64appname)` \n| eval app3=\"N/A\", app=coalesce(app,app2,base64appname,app3) \n| eval total_cache_miss=count_index_cache_miss+count_rawdata_cache_miss \n| search total_cache_miss>0 \n| eval total_hours_searched=round(total_hours_searched,1) \n| stats latest(mostRecent) AS mostRecent, count as number_of_runs, values(host) as host values(total_hours_searched) AS total_hours_searched values(total_days_searched) AS total_days_searched max(run_time) AS max_run_time avg(run_time) AS avg_run_time sum(run_time) AS sum_run_time sum(total_cache_miss) as total_cache_miss max(result_count) AS result_count max(event_count) AS event_count max(searched_buckets) AS searched_buckets values(info) AS info values(numofsearchesinquery) AS numofsearchesinquery, values(provenance) AS provenance, values(app) AS app by users search \n| rex field=search \"(?s)(NOT\\s+index(\\s*=\\s*|::)[^ ]+)|(NOT\\s+\\([^\\)]+\\))|(index(\\s*=\\s*|::)\\\"?(?P<indexregex>[\\*A-Za-z0-9-_]+))\" max_match=50 \n| rex field=search \"(?s)(NOT\\s+index\\s+[iI][nN]\\s*\\([^\\)]+)|(index\\s+[iI][nN]\\s*\\((?P<indexin>([^\\)\\\"]+)|\\\"[^\\)\\\"]+\\\"))\" max_match=50 \n| makemv delim=\",\" indexin \n| eval indexes=mvappend(indexregex,indexin) \n| eval indexes=mvmap(indexes, replace(lower(indexes), \"\\\"\", \"\")) \n| eval indexes=mvmap(indexes, trim(replace(indexes, \"'\", \"\"))) \n| eval indexes=mvdedup(indexes) \n| eval has_pipe=if(match(search,\"\\|\"),\"true\",null())\n| rex max_match=100 field=search \"tag=(?<tags>[^\\s+\\||\\)]+)\" \n| rex max_match=100 field=search \"eventtype=(?<eventtypes>[^\\s+\\||\\)]+)\" \n| rex max_match=100 field=search \"(?<macros>\\`[^\\s]+\\`)\" \n| rex field=search \"(?P<search>[^\\|]+\\|)\" \n| eval search = if(isnotnull(has_pipe),search . \" ... (trimmed)\",search)\n| fields - has_pipe, indexin, indexregex \n| eval avg_run_time=round(avg_run_time,1) \n| eval provenance=replace(provenance, \"UI:Dashboard:\", \"\"), mostRecent=strftime(mostRecent,\"%+\") \n| rename provenance as dashboard\n| sort - total_cache_miss\n| table total_cache_miss, total_hours_searched, total_days_searched, mostRecent, users, number_of_runs, max_run_time, avg_run_time, sum_run_time, indexes, result_count, event_count, searched_buckets, info, numofsearchesinquery, dashboard, app, eventtypes, macros, tags, search",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is designed to find the number of cache misses by saved searches or dashboards, originally created by Nico Van Der Walt",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "33 */4 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - SmartStore cache misses - dashboards",
    "search": "`searchheadhosts` (index=_audit action=search search_id NOT typeahead NOT \"search_id='rsa_*\") `comment(\"Original version by Nico Van Der Walt, modified by Gareth Anderson\")` \ninvocations_command_search_index_bucketcache_miss>0 OR invocations_command_search_rawdata_bucketcache_miss>0 TERM(info=*) TERM(UI:Dashboard:*) \n| eval total_days_searched=(search_lt-search_et)/86400 \n| eval total_hours_searched=total_days_searched*24 \n| eval total_hours_searched=round(total_hours_searched,1) \n| eval total_days_searched=round(total_days_searched,0) \n| eval search_id=trim(search_id,\"\\'\") \n| eval search_id=coalesce(search_id,sid) \n| eval origSid=search_id \n| rex field=search_id \"subsearch_(?<search_id>.*)_\\d+\\.\\d+\" \n| eval api_et=if(api_et=\"N/A\", search_et, api_et) \n| eval total_hours_searched=if(api_et=\"N/A\", \"AllTime\",total_hours_searched) \n| eval total_days_searched=if(api_et=\"N/A\", \"AllTime\",total_days_searched) \n| eval provenance=if(provenance=\"N/A\",NULL,provenance) \n| eval provenance=if(provenance=\"UI:LocateData\",NULL,provenance) \n| rex \"(?s), search='(?P<search>.*)\\]$\" \n| rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n| eval search=if(match(search,\"^'\"),mvindex(search,1),search) \n| stats latest(_time) AS mostRecent, values(host) as host sum(duration_command_search_rawdata_bucketcache_miss) AS duration__raw_cache_miss sum(invocations_command_search_index_bucketcache_miss) as count_index_cache_miss sum(invocations_command_search_rawdata_bucketcache_miss) as count_rawdata_cache_miss values(total_hours_searched) AS total_hours_searched values(total_days_searched) AS total_days_searched values(user) AS users last(search) AS search values(savedsearch_name) AS savedsearch_name max(total_run_time) AS run_time values(result_count) AS result_count values(event_count) AS event_count values(searched_buckets) AS searched_buckets values(info) AS info values(provenance) AS provenance dc(origSid) AS numofsearchesinquery by search_id \n| `search_type_from_sid(search_id)` \n| `base64decode(base64appname)` \n| eval app3=\"N/A\", app=coalesce(app,app2,base64appname,app3) \n| eval total_cache_miss=count_index_cache_miss+count_rawdata_cache_miss \n| search total_cache_miss>0 \n| search provenance=*Dashboard* \n| eval total_hours_searched=round(total_hours_searched,1) \n| rex field=search \"(?s)(NOT\\s+index(\\s*=\\s*|::)[^ ]+)|(NOT\\s+\\([^\\)]+\\))|(index(\\s*=\\s*|::)\\\"?(?P<indexregex>[\\*A-Za-z0-9-_]+))\" max_match=50 \n| rex field=search \"(?s)(NOT\\s+index\\s+[iI][nN]\\s*\\([^\\)]+)|(index\\s+[iI][nN]\\s*\\((?P<indexin>([^\\)\\\"]+)|\\\"[^\\)\\\"]+\\\"))\" max_match=50 \n| makemv delim=\",\" indexin \n| eval indexes=mvappend(indexregex,indexin) \n| eval indexes=mvmap(indexes, replace(lower(indexes), \"\\\"\", \"\")) \n| eval indexes=mvmap(indexes, trim(replace(indexes, \"'\", \"\"))) \n| eval indexes=mvdedup(indexes) \n| eval has_pipe=if(match(search,\"\\|\"),\"true\",null()) \n| rex field=search \"(?P<search>[^\\|]+\\|)\" \n| eval search = if(isnotnull(has_pipe),search . \" ... (trimmed)\",search)\n| stats latest(mostRecent) AS mostRecent count as number_of_searches_run dc(savedsearch_name) as num_panels values(host) as host max(run_time) AS max_run_time avg(run_time) AS avg_run_time sum(run_time) AS sum_run_time sum(total_cache_miss) as total_cache_miss sum(result_count) AS result_count sum(event_count) AS event_count sum(searched_buckets) AS searched_buckets values(users) as users, values(indexes) AS indexes, values(search) AS search, values(info) AS info by provenance \n| eval avg_run_time=round(avg_run_time,1) \n| eval provenance=replace(provenance, \"UI:Dashboard:\", \"\"), mostRecent=strftime(mostRecent,\"%+\") \n| rename provenance as dashboard \n| sort - total_cache_miss",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is designed to find the number of cache misses by dashboards, originally created by Nico Van Der Walt",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "33 */4 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - SmartStore cache misses - savedsearches",
    "search": "`searchheadhosts` (index=_audit action=search search_id NOT typeahead NOT \"search_id='rsa_*\") `comment(\"Original version by Nico Van Der Walt, modified by Gareth Anderson\")`\ninvocations_command_search_index_bucketcache_miss>0 OR invocations_command_search_rawdata_bucketcache_miss>0 TERM(info=*) TERM(UI:Search) \n| eval total_days_searched=(search_lt-search_et)/86400 \n| eval total_hours_searched=total_days_searched*24 \n| eval total_hours_searched=round(total_hours_searched,1) \n| eval total_days_searched=round(total_days_searched,0) \n| eval search_id=trim(search_id,\"\\'\") \n| eval search_id=coalesce(search_id,sid) \n| eval origSid=search_id \n| rex field=search_id \"subsearch_(?<search_id>.*)_\\d+\\.\\d+\" \n| eval api_et=if(api_et=\"N/A\", search_et, api_et) \n| eval total_hours_searched=if(api_et=\"N/A\", \"AllTime\",total_hours_searched) \n| eval total_days_searched=if(api_et=\"N/A\", \"AllTime\",total_days_searched) \n| eval provenance=if(provenance=\"N/A\",NULL,provenance) \n| eval provenance=if(provenance=\"UI:LocateData\",NULL,provenance) \n| rex \"(?s), search='(?P<search>.*)\\]$\" \n| rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n| eval search=if(match(search,\"^'\"),mvindex(search,1),search) \n| stats latest(_time) AS mostRecent, values(host) as host sum(duration_command_search_rawdata_bucketcache_miss) AS duration__raw_cache_miss sum(invocations_command_search_index_bucketcache_miss) as count_index_cache_miss sum(invocations_command_search_rawdata_bucketcache_miss) as count_rawdata_cache_miss values(total_hours_searched) AS total_hours_searched values(total_days_searched) AS total_days_searched values(user) AS users last(search) AS search values(savedsearch_name) AS savedsearch_name max(total_run_time) AS run_time values(result_count) AS result_count values(event_count) AS event_count values(searched_buckets) AS searched_buckets values(info) AS info values(provenance) AS provenance dc(origSid) AS numofsearchesinquery by search_id \n| eval total_cache_miss=count_index_cache_miss+count_rawdata_cache_miss \n| search total_cache_miss>0 \n| search provenance=UI:Search \n| eval total_hours_searched=round(total_hours_searched,1) \n| `search_type_from_sid(search_id)` \n| `base64decode(base64appname)` \n| eval app3=\"N/A\", app=coalesce(app,app2,base64appname,app3) \n| stats latest(mostRecent) AS mostRecent, count as number_of_runs values(host) as host values(total_hours_searched) AS total_hours_searched values(total_days_searched) AS total_days_searched max(run_time) AS max_run_time avg(run_time) AS avg_run_time sum(run_time) AS sum_run_time sum(total_cache_miss) as total_cache_miss max(result_count) AS result_count max(event_count) AS event_count max(searched_buckets) AS searched_buckets values(info) AS info values(numofsearchesinquery) AS numofsearchesinquery, values(app) AS app by users search \n| rex field=search \"(?s)(NOT\\s+index(\\s*=\\s*|::[^ ]+)|(NOT\\s+\\([^\\)]+\\))|(index(\\s*=\\s*|::)\\\"?(?P<indexregex>[\\*A-Za-z0-9-_]+))\" max_match=50 \n| rex field=search \"(?s)(NOT\\s+index\\s+[iI][nN]\\s*\\([^\\)]+)|(index\\s+[iI][nN]\\s*\\((?P<indexin>([^\\)\\\"]+)|\\\"[^\\)\\\"]+\\\"))\" max_match=50 \n| makemv delim=\",\" indexin \n| eval indexes=mvappend(indexregex,indexin) \n| eval indexes=mvmap(indexes, replace(lower(indexes), \"\\\"\", \"\")) \n| eval indexes=mvmap(indexes, trim(replace(indexes, \"'\", \"\"))) \n| eval indexes=mvdedup(indexes) \n| rex max_match=100 field=search \"tag=(?<tags>[^\\s+\\||\\)]+)\" \n| rex max_match=100 field=search \"eventtype=(?<eventtypes>[^\\s+\\||\\)]+)\" \n| rex max_match=100 field=search \"(?<macros>\\`[^\\s]+\\`)\" \n| eval has_pipe=if(match(search,\"\\|\"),\"true\",null()) \n| rex field=search \"(?P<search>[^\\|]+\\|)\" \n| eval search = if(isnotnull(has_pipe),search . \" ... (trimmed)\",search), mostRecent=strftime(mostRecent,\"%+\") \n| fields - has_pipe, indexin, indexregex \n| eval avg_run_time=round(avg_run_time,1) \n| sort - total_cache_miss",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is designed to find the number of cache misses by saved searches, originally created by Nico Van Der Walt",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "33 */4 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Splunk Max Historic Search Limits Reached",
    "search": "`comment(\"Once the max historic search limit has been reached the search jobs will be queued, this can be an issue if the limit is set too low (or too high)\")` \nindex=_internal `splunkenterprisehosts` \"The maximum number of historical concurrent system-wide searches has been reached\" OR \"The system is approaching the maximum number of historical searches that can be run concurrently\" (`splunkadmins_splunkd_source`) \n| fields _time, host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Splunk Max Historic Search Limits Reached",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 11 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Splunk Scheduler logs have not appeared in the last",
    "search": "`comment(\"this works except in 8.2.5 it's slower than stats... | tstats count where index=_internal `searchheadhosts` source=*scheduler.log sourcetype=scheduler by host, _time span=1m\")` \nindex=_internal `searchheadhosts` source=*scheduler.log sourcetype=scheduler \n| bin _time span=1m \n| stats count by host, _time \n| timechart limit=0 span=5m aligntime=latest sum(count) AS count by host \n| fillnull \n| untable _time, host, count \n| stats max(_time) AS mostRecent, min(_time) AS firstSeen, last(count) AS lastCount by host \n| search `comment(\"Exclude time periods where shutdowns were occurring\")` AND NOT [`splunkadmins_shutdown_time(searchheadhosts,60,60)`] \n| where lastCount=0 \n| eval logMessages=\"Zero log entries found at this time, this might be a Splunkd issue, please investigate\" \n| fields - lastCount \n| eval mostRecent = strftime(mostRecent, \"%+\"), firstSeen=strftime(firstSeen, \"%+\") \n| eval search_head=host \n| eval search_head_cluster=`search_head_cluster` \n| table host, firstSeen, mostRecent, logMessages, search_head_cluster",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. If the scheduler logs have stopped on a search head then there is likely an issue",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 2,6,10,14,18,22 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Splunk Users Violating the Search Quota",
    "search": "`comment(\"The listed users have reached the search quota and may need to be informed of this, or they may need to be added to the macro for this alert\")`\nindex=_internal `searchheadhosts` (`splunkadmins_splunkd_source`) \"was previously reported to be hung but has completed\" OR \"The maximum number of concurrent \" `splunkadmins_users_violating_searchquota`\n| rex \"Queued job id\\s+=\\s+(rt_)?(?P<username2>[^_]+)\"\n| eval username=coalesce(username, username2)\n| bin span=24h _time\n| top showperc=false limit=200 username, reason, host, _time, provenance\n| where count>10",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Low. These users have reached the search quota but may not be aware of this issue.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 3 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Splunk alert actions exceeding the max_action_results limit",
    "search": "`comment(\"If the max_action_results is exceeded the alert action receives only part of the results to work with, this can be a problem with the lookup alert action or others...\")` index=_internal `searchheadhosts` sourcetype=scheduler alert_actions!=\"\" `splunkadmins_alertactions_max_action_results`\n    [| rest /services/configs/conf-limits splunk_server=local f=title f=max_action_results\n    | search title=scheduler\n    | eval search=\"result_count>\" . max_action_results\n    | fields search]\n| stats count, values(alert_actions) AS alert_actions, earliest(_time) AS firstSeen, latest(_time) AS lastSeen, max(result_count) AS result_count by user, app, savedsearch_name \n| append \n    [| rest /services/configs/conf-limits splunk_server=local f=title f=max_action_results \n    | search title=scheduler \n    | fields max_action_results ] \n| eventstats max(max_action_results) AS max_action_results \n| eval firstSeen = strftime(firstSeen, \"%+\"), lastSeen=strftime(lastSeen, \"%+\") \n| where isnotnull(count) \n| appendpipe \n    [| map search=\"| rest /servicesNS/$user$/$app$/saved/searches splunk_server=local | search title=\\\"$savedsearch_name$\\\" \n    | eval savedsearch_name=\\\"$savedsearch_name$\\\", app=\\\"$app$\\\", user=\\\"$user$\\\"\n            | table actions, action.*, savedsearch_name, app, user\" maxsearches=20\n        ]\n| stats values(*) AS * by savedsearch_name, app, user\n| eval remove=case('action.email'=\"1\" AND isnull(action.email.sendresults),\"remove\",1=1,null())\n| where isnull(remove)\n| eval message=\"One or more of your alerts are attempting to use an alert action with a number of events/results that exceeds the max_action_results limit, Splunk will truncate results beyond the max_action_results limit listed in the table when running the alert action...\" \n| table message, user, app, savedsearch_name, alert_actions, result_count, max_action_results, count, firstSeen, lastSeen",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. One or more alerts exceeeded the max_action_results set in limits.conf, if the max_action_results is exceeded the alert action receives only part of the results to work with, this can be a problem with the lookup alert action or others...Note that there is no log entry for this in splunkd as of 8.1.1, refer to https://ideas.splunk.com/ideas/EID-I-781 to vote on having log messages for this issue",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "14 */4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Splunk login attempts from users that do not have any LDAP roles",
    "search": "`comment(\"The listed users have attempted to login but were unable to, it is likely they do not have any LDAP role yet and should be informed of this\")` \nindex=_internal `searchheadhosts` \"Couldn't find matching groups for user\" OR \"but none are mapped to Splunk roles\" OR \"SSO failed - User does not exist\" (`splunkadmins_splunkd_source`) `splunkadmins_loginattempts`\n| join user [search index=_internal `searchheadhosts` action=login status=failure reason=user-initiated OR reason=sso-failed] \n| dedup user \n| table _time, user, host",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. These usernames have appeared in the logs but they have no mapped roles. Can be fixed by the end user? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 8 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Summary searches using realtime search scheduling",
    "search": "| rest \"/servicesNS/-/-/saved/searches\" count=0 splunk_server=local f=search f=realtime_schedule f=eai:* f=action.summary_index \n| search `comment(\"Find reports running summary indexing that are not scheduled using continuous scheduling (realtime_schedule=0)\")`\n| where realtime_schedule=1 \n| rex field=search \"(?s)\\|\\s*(?P<command>mcollect|meventcollect|collect)\\s+\" \n| where isnotnull(command) OR 'action.summary_index'==1 \n| rename eai:acl.app AS app, eai:acl.owner AS owner \n| table title, app, owner, search, action.summary_index, id, updated",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Searches using collect commands should likely use realtime_schedule=0, there are also issues in the UI of some Splunk versions not seting this version automatically",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "38 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Tags report",
    "search": "| rest /servicesNS/-/-/configs/conf-tags splunk_server=local \n| search eai:acl.sharing!=\"user\" \n| eval _raw=\"\" \n| foreach \"*\" \n    [| eval field=if(match(\"<<FIELD>>\",\"^(title|eai:|splunk_server|author|id|updated|published)\"),\"\",\"<<FIELD>> = \".'<<FIELD>>') \n    | eval _raw=mvappend(_raw,field) ] \n| rex max_match=0 field=_raw \"(?P<tag>\\S+)\\s+=\\s+enabled\" \n| table tag, title, eai:acl.app, eai:acl.sharing \n| rename title AS definition, eai:acl.app AS app, eai:acl.sharing AS sharing \n| mvexpand tag \n| eval splunk_server=\"default\" \n| outputlookup splunkadmins_tags",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report is required to support the audit log summary searches. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "50 5 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - User - Dashboards searching all indexes",
    "search": "| rest /servicesNS/-/-/data/ui/views `splunkadmins_restmacro`\n| search `comment(\"A dashboard searching all indexes is an issue just like a scheduled search querying all indexes or using the index=* trick\")`\neai:data=*query* `splunkadmins_dashboards_allindexes`\n| regex eai:data=\"<search.*\" \n| rex field=eai:data \"(?s)(?P<theSearch><search(?!String)[^>]*>[^<]*<query>.*?)<\\/query>\" max_match=200 \n| mvexpand theSearch \n| rex field=theSearch \"(?s)<search(?P<searchInfo>[^>]*)>[^<]*<query>(?P<theQuery>.*)\" \n| search `comment(\"If we are seeing post process search then we don't want to check if it has index= because that is likely only in the base query. These are also various exclusions for legitimate searches that will not involve scanning all indexes, such as rest or a savedsearch or similar\")` searchInfo!=\"*base*\"\n| rename eai:appName AS application, eai:acl.sharing AS sharing, eai:acl.owner AS owner, label AS name\n| table theQuery, application, owner, sharing, name, splunk_server, title\n| regex theQuery!=\"index\\s*=(?!\\s*\\*)\" \n| regex theQuery!=\"^(\\()?\\s*(\\`|\\$[^|]+\\$|eventtype=|<!\\[CDATA\\[\\s*\\|\\s*((acl)?inputlookup|rest) |\\|)\"\n| rex field=theQuery \"(?s)^(?P<exampleQueryToDetermineIndexes>[^\\|]+)\"\n| eval exampleQueryToDetermineIndexes=exampleQueryToDetermineIndexes . \"| stats values(index) AS index | format | fields search | eval search=replace(search,\\\"\\\\)\\\",\\\"\\\"), search=replace(search,\\\"\\\\(\\\",\\\"\\\"), search=if(search==\\\"NOT \\\",\\\"No indexes found\\\",search)\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. All dashboard panels that do not have an index= setting or use index=* are highlighted by this alert. Can be fixed by the end user? Yes. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "30 5 * * 1-5",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - User - Dashboards searching all indexes macro version",
    "search": "| rest /servicesNS/-/-/data/ui/views `splunkadmins_restmacro`\n| search `comment(\"A dashboard searching all indexes is an issue just like a scheduled search querying all indexes or using the index=* trick. This version is dealing with those using macros. Please ensure the SearchHeadLevel - Macro report is also enabled for this to work as expected.\")` \n    eai:data=*query* NOT (eai:appName=simple_xml_examples eai:acl.sharing=app) NOT (eai:appName=nmon eai:acl.sharing=app) NOT (eai:appName=splunk_app_aws eai:acl.sharing=app) \n| regex eai:data=\"<search.*\" \n| mvexpand theSearch \n| rex field=eai:data \"(?s)(?P<theSearch><search(?!String)[^>]*>[^<]*<query>.*?)<\\/query>\" max_match=200 \n| mvexpand theSearch \n| rex field=theSearch \"(?s)<search(?P<searchInfo>[^>]*)>[^<]*<query>(?P<theQuery>.*)\" \n| search `comment(\"If we are seeing post process search then we don't want to check if it has index= because that is likely only in the base query. These are also various exclusions for legitimate searches that will not involve scanning all indexes, such as rest or a savedsearch or similar\")` searchInfo!=\"*base*\" \n| rename eai:appName AS application, eai:acl.sharing AS sharing, eai:acl.owner AS identity, label AS name \n| table theQuery, application, identity, sharing, name, splunk_server, title \n| regex theQuery!=\"index\\s*=(?!\\s*\\*)\" \n| regex theQuery!=\"^(\\()?\\s*(\\`|\\$[^|]+\\$|eventtype=|rest |<!\\[CDATA\\[\\s*\\|\\s*((acl)?inputlookup|rest) |\\|)\" \n| rex field=theQuery \"(?s)^(?P<exampleQueryToDetermineIndexes>[^\\|]+)\" \n| regex exampleQueryToDetermineIndexes=\"\\`\"\n| eval exampleQueryToDetermineIndexes=exampleQueryToDetermineIndexes . \"| stats values(index) AS index | format | fields search | eval search=replace(search,\\\"\\\\)\\\",\\\"\\\"), search=replace(search,\\\"\\\\(\\\",\\\"\\\"), search=if(search==\\\"NOT \\\",\\\"No indexes found\\\",search)\" \n| eval search=theQuery\n| `splunkadmins_audit_logs_macro_sub_v8` \n| `splunkadmins_audit_logs_macro_sub_v8` \n| stats values(search) AS search, first(exampleQueryToDetermineIndexes) AS exampleQueryToDetermineIndexes by theQuery, application, identity, sharing, name, title, splunk_server\n| table search, application, identity, sharing, name, splunk_server, title, exampleQueryToDetermineIndexes\n| nomv search\n| regex search!=\"index\\s*=(?!\\s*\\*)\"",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. All dashboard panels that are using a macro and do not have an index= setting or use index=* are highlighted by this alert. Can be fixed by the end user? Yes. Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "32 6 * * 1-5",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Users exceeding the disk quota",
    "search": "`comment(\"The listed users have reached the maximum disk quota, they may be unaware so it is best to let them know about this issue...\")` \n`comment(\"Note that the REST API call accesses the jobs list which can expire for ad-hoc jobs in 10 minutes, so this may find zero results. The status.csv inside the dispatch directory records the size per job but it is not indexed by Splunk so either this alert needs to run very often or it will sometimes run after the issue has occurred and send an empty top 10 jobs list...\")`\n`comment(\"The introspection index version is called SearchHeadLevel - Users exceeding the disk quota introspection, it is not search head specific but it is also less accurate for disk space used\")`\nindex=_internal sourcetype=splunkd `splunkenterprisehosts` (`splunkadmins_splunkd_source`) \"maximum disk usage quota\" `splunkadmins_users_exceeding_diskquota`\n| stats max(_time) AS mostRecent by username, reason, host\n| eval mostRecent = strftime(mostRecent, \"%+\")\n| search `comment(\"We use this bizarre field naming so when we append the actual search results we don't have 20 columns of data to read, also it looks nicer in an email. However since we want this over multiple lines and we only want to run the map command once we use a temporary field which we later expand to a multi-line field. Furthermore mvexpand on the search field can result in multiple rows per search which is why a temporary field is used\")`\n| eval renameToSearch=\"Why am I, \" + username + \", receiving this? |\" + reason + \" (from) \" + host + \"|_|Last seen? |\" + mostRecent + \"|_|Your top 10 largest jobs are listed below\"\n| fields - reason, mostRecent, host\n| search `comment(\"The below is the complex attempt to include the largest jobs by querying the REST API. If we use map without the appendpipe we lose the original reason why we are sending this email. The initial workaround of makeresults and eval commands did work but this seemed slightly cleaner. Although there would be other ways to do this...\")`\n| append [ | makeresults | eval username=\"workaround for map errors\", body=\"to pass appinspect\" ]\n| appendpipe\n    [\n| map \n    [| rest /services/search/jobs `splunkadmins_restmacro` \n    | search `comment(\"Attempt to show the customer the top 10 jobs using disk and the related search commands/search names, also if it relates to their scheduled searches or not...\")` author=$username$ diskUsage>0 \n    | fields diskUsage, eai:acl.app, latestTime, label, provenance, runDuration, searchEarliestTime, searchLatestTime, title, updated, ttl \n    | rename title AS search, eai:acl.app AS app, label AS searchName \n    | sort - diskUsage \n    | eval diskUsage=round(diskUsage/1024/1024,2), searchEarliestTime=strftime(searchEarliestTime, \"%+\"), searchLatestTime=strftime(searchLatestTime, \"%+\") \n    | eval expiry=strftime(strptime(updated, \"%Y-%m-%dT%H:%M:%S.%3N%z\")+ttl, \"%+\")\n    | eval runDuration=substr(tostring(runDuration,\"duration\"),0,8) \n    | eval search=substr(search,0,300) \n    | fields - provenance, ttl, updated \n    | eval searchName=if(searchName==\"\",\"ad-hoc search\",searchName)\n    | eval renameToSearch=\"X\"\n    | table searchName, app, diskUsage, expiry, runDuration, searchEarliestTime, searchLatestTime, search, renameToSearch\n    | head 10 ] \n]\n| where username!=\"workaround for map errors\"\n| makemv delim=\"|\" renameToSearch\n| mvexpand renameToSearch\n| eval search=if(renameToSearch!=\"X\",renameToSearch,search)\n| table username, searchName, app, diskUsage, expiry, runDuration, searchEarliestTime, searchLatestTime, search",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-2h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. One or more users have reached the disk quota limit and may not be aware of this... Can be fixed by the end user? Yes. You may wish to use sendresults with the output of this command...Search Head specific? Yes",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "21 */2 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Users exceeding the disk quota introspection",
    "search": "`comment(\"The listed users have reached the maximum disk quota, they may be unaware so it is best to let them know about this issue...\")`\n`comment(\"Note that the REST API call accesses the jobs list which can expire for ad-hoc jobs in 10 minutes, the introspection index has data for a longer period of time however it's not as accurate as the rest version. This alert also outputs the emailed users to a lookup so they don't continue to receive this same email without some kind of throttle per-user\")`\n`comment(\"The REST version is called SearchHeadLevel - Users exceeding the disk quota and is search head specific\")`\nindex=_internal sourcetype=splunkd `splunkenterprisehosts` (`splunkadmins_splunkd_source`) \"maximum disk usage quota\" `splunkadmins_users_exceeding_diskquota` NOT [| inputlookup splunkusersexceedingdiskquota.csv | fields username ]\n| stats max(_time) AS mostRecent by username, reason, host\n| rename username AS username_from_search\n| eval mostRecent = strftime(mostRecent, \"%+\")\n| search `comment(\"For each result we find we're going to run the map command to send an email to each individual user who has had the issue, if they have been emailed before the inputlookup will exclude them. Username renamed due to issues with username variable in a scheduled search\")`\n| eval body=\"Why am I receiving this? <br />\" + reason + \"<br /><br /> This occurred on host \" + host + \"<br /><br />The issue was last noticed on \" + mostRecent + \"<br /><br />\" + \"Your top 20 searches are listed below\" + \"<br /><br />\"\n| search `comment(\"The below is the attempt to include the largest jobs by querying the introspection index. If we use map without the appendpipe we lose parts of the original search we need. The initial workaround of makeresults and eval commands did work but this seemed slightly cleaner. Although there would be other ways to do this...\")`\n| head 30\n| append [ | makeresults | eval username_from_search=\"workaround for map errors\", body=\"to pass appinspect\" ]\n| appendpipe\n    [| map\n        [ search `comment(\"The intropsection data provides a written_mb field which is not going to advise an accurate real-disk usage for a job, but it does provide an estimate and provides data for longer than a 10 minute time period for ad-hoc jobs...the | rest version is the alternative available which is more accurate but may return zero results if this is not run at least every 10 minutes, it also must run on the same search head cluster as the disk usage quota, unlike this introspection version. max(data.written_mb) was used instead of last() as sometimes the quota kicks in before the temporary files are removed.\")`\n            index=_introspection \"data.search_props.role\"=head data.written_mb>1 sourcetype=splunk_resource_usage \"data.search_props.user\"=$username_from_search$\n        | stats max(data.written_mb) AS MBwritten, last(data.elapsed) AS approxDuration, max(_time) AS searchLatestTime, min(_time) AS searchEarliestTime by \"data.search_props.app\", \"data.search_props.provenance\", \"data.search_props.type\", \"data.search_props.sid\"\n        | stats count, sum(MBwritten) AS MBwritten, max(approxDuration) AS approxDuration, max(searchLatestTime) AS searchLatestTime, min(searchEarliestTime) AS searchEarliestTime by \"data.search_props.app\", \"data.search_props.provenance\", \"data.search_props.type\"\n        | eval searchLatestTime=strftime(searchLatestTime, \"%+\"), searchEarliestTime=strftime(searchEarliestTime, \"%+\")\n        | rename data.search_props.app AS app, data.search_props.provenance AS dashboardURLorSearchName, data.search_props.type AS type\n        | sort - MBwritten\n        | eval approxDuration=substr(tostring(approxDuration,\"duration\"),0,8)\n        | appendcols\n            [ search `comment(\"At this point you need to either lookup the username to email translation, here's an example using ldapsearch: ldapsearch search=\\\"(&(CN=$username_from_search$)(objectClass=organizationalPerson))\\\" attrs=mail | fields mail\")` ]\n        | eval email_to=mail\n        | fields - mail\n        | search `comment('Remove search/comment and replace <EQ> with equals to use sendresults to make this fully automated. AppInspect badge does not allow dependencies. sendresults subject<EQ>\"Splunk Disk Quota Exceeded\" body<EQ>$body$ msgstyle<EQ>\"table {font-family:Arial;font-size:12px;border: 1px solid black;padding:3px}th {background-color:#AAAAAA;color:#fff;border-left: solid 1px #e9e9e9} td {border:solid 1px #e9e9e9}\" showemail<EQ>f')`\n        | head 20 ] maxsearches=30\n        ]\n| where username_from_search!=\"workaround for map errors\"\n| table app, MBwritten, dashboardURLorSearchName, approxDuration, searchEarliestTime, searchLatestTime, username_from_search, type, count, email_to\n| fields username_from_search \n| rename username_from_search AS username\n| eval currtime=now()\n| fields currtime, username\n| where isnotnull(username)\n| outputlookup splunkusersexceedingdiskquota.csv append=true",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. One or more users have reached the disk quota limit and may not be aware of this... Can be fixed by the end user? Yes. This version requires sendresults and customisation to work as expected.\nAlso refer to SearchHeadLevel - Users exceeding the disk quota introspection cleanup for the lookup cleaner. For testing you may wish to hardcode the email_to field and remove all lines after the line starting with table app...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "11 6,10,14,18,22,2 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Users exceeding the disk quota introspection cleanup",
    "search": "| inputlookup splunkusersexceedingdiskquota.csv \n| where currtime > now() - (7*60*60*24) \n| outputlookup splunkusersexceedingdiskquota.csv",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Relates to the alert SearchHeadLevel - Users exceeding the disk quota introspection\nThis report cleans up the lookup file created by the disk quota alert so that users will re-receive the alert after a period of time. Hardcoded to 1 week for now",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "53 4,10,16,22 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - Users with auto-finalized searches",
    "search": "`comment(\"Find searches which have been auto-finalized and the search contents which was running when the search was auto-finalized. Very similar to Users Exceeding the disk quota however this covers both disk quota and srchMaxTime. Note this alert requires further testing\")`\n`comment(\"This does require the limits.conf log_search_messages=true setting to be enabled to worK\")`\nindex=_internal `searchheadhosts` sourcetype=splunk_search_messages auto-finalized\n `comment(\"for even more info...OR canceled OR auto-canceled OR cancelled\")` \n| rex field=source \"[/\\\\\\]dispatch[/\\\\\\](?P<sid>[^/\\\\\\]+)\"\n| rex \"(?P<message>auto-finalized[^\\\"]+)\"\n| fillnull message value=\"Unknown\"\n| append [ | makeresults | eval sid=\"workaround for map errors\", message=\"to pass appinspect\" ]\n| map\n    [ search index=_audit `searchheadhosts` \"info=granted\" \"search_id='$sid$'\"\n    | rex \"(?s), search='(?P<search>.*)\\]$\" \n    | rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n    | eval search=substr(search,0,100)\n    | eval message=$message$ ] maxsearches=50\n| stats values(timestamp) AS time, values(message) AS message, values(search) AS search, values(apiStartTime) AS startTime, values(apiEndTime) AS endTime, values(savedsearch_name) AS savedsearch_name by user",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Determine who has had a search auto-finalized due to time or disk quota. This does require the limits.conf setting log_search_messages=true",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - WLM aborted searches",
    "search": "`comment(\"Detect searches aborted by WLM rules and using the sid find the details of the search, provide the details so this could be used with an application like sendresults for automated user notification of rule violation\")` \nindex=_internal \"The search\" \"was aborted\" `searchheadhosts` sourcetype=splunkd `splunkadmins_splunkd_source` \n| rex \"WorkloadManager - The search (?P<search_id>[^ ]+)\" \n| eval search_id=\"'\" . search_id . \"'\" \n| appendpipe \n    [| map [ search index=_audit info=completed OR info=failed host=$host$ search_id=\"$search_id$\" | eval search_lt=if(search_lt==\"N/A\",_time,search_lt), search_et=if(search_et==\"N/A\",now()-(365*24*60*60),search_et) | eval period=tostring(search_lt-search_et,\"duration\") | table user, total_run_time, search, search_id, period, savedsearch_name ] maxsearches=20 ] \n| table user, total_run_time, search, search_id, period, savedsearch_name \n| eval savedsearch_name=if(isnull(savedsearch_name),\"ad-hoc\",savedsearch_name) \n| stats values(*) AS * by search_id",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. This alert exists to use sendresults or similar to email the users about their search termination, as the current WLM notification system is limited as of version 8.0.5",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "7 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - audit logs showing all time searches",
    "search": "`comment(\"Search the audit logs for any search that does not have a earliest time set. Note search_et is not set for canceled/failed status so deal with this later...As per the comments on https://ideas.splunk.com/ideas/E-I-49 this can miss the _index_earliest flag passed in via API but it works for most cases\")` \nindex=_audit sourcetype=audittrail search_id!=\"rsa_*\" `searchheadhosts` info=\"failed\" OR info=\"completed\" OR info=\"canceled\" search=* search_et=\"N/A\" `splunkadmins_audit_alltime` \n| regex search=\"(?s)^'\\s*\\|?search\\s+\" \n| regex search_id!=\"^'subsearch_\" \n| eval has_earliest=if((info=\"failed\" OR info=\"canceled\") AND api_et!=\"N/A\",true(),null()) \n| where isnull(has_earliest) \n| eval search_id=substr(search_id,1) \n| `search_type_from_sid(search_id)` \n| eval total_run_time=round(total_run_time) \n| where total_run_time>0 \n| sort - total_run_time \n| `base64decode(base64appname)` \n| eval app_name=coalesce(app,base64appname) \n| fillnull app_name, savedsearch_name value=\"\" \n| stats count, latest(_time) AS most_recent, values(info) AS info, list(total_run_time) AS total_run_time, values(search) AS search_example by user, type, savedsearch_name, app_name \n| eval total_run_time=mvdedup(total_run_time), most_recent=strftime(most_recent, \"%+\") \n| sort - total_run_time",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report will attempt to find all-time searches that have been run and provide information about the context in which they were run. There are various other alerts/reports that can assist in identifying them more proactively, this one reports that they have happened...Note that this is not 100% accurate as via API you can set _index_earliest without setting an earliest= as per the comments on https://ideas.splunk.com/ideas/E-I-49",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "38 6 * * 1",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - authorize.conf settings will prevent some users from appearing in the UI",
    "search": "| rest /servicesNS/-/-/authorization/roles splunk_server=local \n| eval comb_capabilities=mvappend(imported_capabilities,capabilities) \n| where (title=\"admin\" OR imported_roles=\"admin\") AND isnotnull(grantable_roles) \n| eval isadminrole=\"true\" \n| stats count by comb_capabilities, title, isadminrole \n| rename count AS admin_count \n| rename comb_capabilities AS capabilities \n| append \n    [| rest /servicesNS/-/-/authorization/roles splunk_server=local \n    | where (title!=\"admin\" AND imported_roles!=\"admin\") OR isnull(grantable_roles) \n    | stats count by capabilities, title ] \n| fillnull isadminrole value=\"false\" \n| stats count, values(isadminrole) AS isadminrole, values(title) AS title, max(admin_count) AS admin_count by capabilities \n| eventstats max(admin_count) AS admin_count \n| where isadminrole=\"false\" AND NOT isadminrole=\"true\" AND admin_count>0 \n| stats values(capabilities) AS capabilities by title \n| rename title AS role \n| eval comment=\"If the mentioned roles are granted to zero or more users, then the users will no longer be visible in the Settings -> Users UI page, or in the /services/authentication/users REST endpoint due to the grantableRoles setting as per https://docs.splunk.com/Documentation/Splunk/latest/Admin/authorizeconf. Therefore you may wish to either remove the grantableRoles setting from the mentioned admin role(s) or alternatively add additional inherited roles/capabilities into the mentioned admin role(s) to ensure the users are visible in the mentioned REST endpoint / UI page. If no users have this role ignore this message...\" \n| table comment, role, capabilities \n| search `splunkadmins_authorize_conf_prevent_users`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. this alert attempts to find a list of roles that have capabilities the admin (or roles inheriting) the admin role do not have. The issue with this is that the Settings -> Users UI page, or in the /services/authentication/users REST endpoint will not show users *if* the grantableRoles setting is used on that particular role. Since this setting can be set by the UI itself it an issue can occur that some users do not appear in Settings -> Users but are cached by Splunk correctly, you just cannot see them. \nThe page https://docs.splunk.com/Documentation/Splunk/latest/Admin/authorizeconf descrbies the grantableRoles setting in more detail, this is definitely an edge case but it may be worth detecting...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "43 4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - datamodel errors in splunkd",
    "search": "index=_internal `searchheadhosts` sourcetype=splunkd `splunkadmins_splunkd_source` \"ERROR DataModelObject\" OR \"ERROR DataModel\" NOT \"because KV Store initialization has not completed yet\" NOT \"KV Store is shutting down\"\n| rex \"(?s)^(\\S+\\s+){3}(?P<error>.*)\"\n| stats count, latest(_time) AS mostrecent, earliest(_time) AS firstseen, values(host) AS hosts by error\n| eval mostrecent=strftime(mostrecent, \"%+\"), firstseen=strftime(firstseen, \"%+\")\n| table count, mostrecent, firstseen, hosts, error",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. One or more datamodel errors exist in the splunkd logs. This may require additional investigation.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "23 4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - dispatch metadata files may need removal",
    "search": "`comment(\"This warning when occurring repetitively tends to indicate some kind of issue that will require the file to be manually removed. For example a zero sized metadata file that cannot be reaped by the dispatch reaper\")` \nindex=_internal sourcetype=splunkd `splunkadmins_splunkd_source` WARN DispatchSearchMetadata \n| stats count by event_message, host \n| where count>100 \n| rex field=event_message \"file: (?P<filename>.*)\" \n| table filename, host, event_message",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-4h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? High. When this particular warning occurs repetitively it usually reuqires manual intervention from the Splunk admin to remove the dispatch directory.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "57 */4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - platform_stats access summary",
    "search": "| multisearch \n    [ search index=_internal `searchheadhosts` sourcetype=splunk_web_access GET \"/app/\" status=200 \n    | regex _raw=\"^[^\\]]+\\] \\\"GET /[^/]+/app/\" \n    | regex _raw!=\"^[^\\]]+\\] \\\"GET /[^/]+/app/([^/ ]+/?)? HTTP\" \n    | rex \"GET /[^/]+/app/(?<app>[^/ ?]+)/(?P<view>[^/ ?]+)\" \n    | eval decoded_uri_query=urldecode(uri_query) \n    | rex field=decoded_uri_query \"/saved/searches/(?P<report>[^&]+)\" \n    | eval report=urldecode(report) ] \n    [ search index=_internal `searchheadhosts` method=GET sourcetype=splunkd_ui_access \n    | regex uri=\"^(/([^/]+/){2}__raw/services/search/jobs\\?output_mode=json&id=)|(/([^/]+/){2}__raw/servicesNS/([^/]+/){2}search/jobs/[^\\?/]+\\?output)\" \n    | rex field=uri \"id=(?P<sid>[^&]+)\" max_match=20 \n    | eval app=null(), report=null(), view=null() \n    | rex field=uri \"^/([^/]+/){2}__raw/servicesNS/([^/]+/)(?P<app>[^/]+)/search/jobs/(?P<sid_2>[^\\?]+)\\?output\" \n    | eval sid=coalesce(sid,sid_2) \n    | eval prebintime=_time \n    | bin _time span=2m] \n    [ search index=_internal `searchheadhosts` method=POST status=201 sourcetype=splunkd_ui_access \n    | regex uri=\"/saved/searches/[^/]+/dispatch$\" \n    | rex field=uri \"(/[^/]+){5}/(?P<app>[^/]+)(/saved/searches/(?P<report>[^/]+))?\" \n    | eval view=\"N/A\", report=report . \"_dispatch\" \n    | eval report=urldecode(report) ] \n| fillnull sid, view value=\"N/A\" \n| eval prebintime=coalesce(prebintime,_time) \n| stats earliest(prebintime) AS prebintime, max(spent) AS spent, values(app) AS app, values(report) AS report by sid, _time, user, useragent, host, view \n| rex field=sid \"(rt_)?(subsearch_)*(?P<from>[^_]+)((_(?P<base64username>[^_]+))|(__(?P<username>[^_]+)))((__(?P<app2>[^_]+)__(?P<report2>[^_]+))|(_(?P<base64appname>[^_]+)__(?P<report3>[^_]+)))\" \n| `base64decode(base64appname)` \n| eval app3=\"N/A\" \n| eval report=coalesce(report,report2,report3), app=coalesce(app,app2,base64appname,app3) \n| eval _time=prebintime \n| eval comment=\"RMD appears to be an encoded report name that only appears in audit.log, scheduler.log and sometimes remote_searches.log. Not creating yet another lookup for this...\" \n| eval report=if(match(report,\"^RMD\"),\"N/A\",report) \n| eval report=case(from==\"scheduler\",report . \"_scheduler\",isnotnull(report3) OR isnotnull(report2),report . \"_dashboard\",match(sid,\"^\\d+\\.\"),\"N/A_adhoc\",1=1,report) \n| fillnull value=\"N/A\" view, report \n| stats count by _time, app, view, report, spent, user, useragent, host \n| table _time, app, view, report, spent, user, useragent, sourceHost, env, count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. This report provides information around the Splunk access UI logs such as dashboard, report or loads of various Splunk pages, perfect for summary indexing...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "15 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - platform_stats.audit metrics api",
    "search": "`comment(\"Attempt to find all API calls into Splunk but do not include the API calls triggered by the local system (ignore localhost\")` \n    index=_internal sourcetype=splunkd_access source=\"*splunkd_access.log\" \"/search/jobs/export\" OR (\"/search/jobs\" method=POST) NOT control clientip!=127.0.0.1 status=200 OR status=201 \n| stats count AS api_search_count by host \n| eval prefix=\"platform_stats.audit.\" \n| rename host AS search_head \n| eval search_head_cluster=`search_head_cluster` \n| addinfo \n| rename info_max_time AS _time \n| fields - info_* \n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix search_head search_head_cluster\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to measure search requests using the REST API via access logs (note realtime_schedule = 0). Note: tested on 7.3 only, may not work on earlier versions",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "*/10 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - platform_stats.audit metrics searches",
    "search": "`comment(\"Count the number of of Splunk queries that use a search command, excludes rest/data model acceleration et cetera\")` \n    index=_audit \", info=granted \" search_id!=\"'rsa_*\" \n| rex \"(?s), search='(?P<search>.*)\\]$\" \n| rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" \n| regex search!=\"^(\\| (copy|archive)buckets|typeahead|\\s*archivebuckets)\" \n| search `comment(\"Unsure what this search does but it appears to be running when no reports are accelerated...\")` \n| regex search!=\"^summarize (tstats=t maintain=\\\"\\\" summaryprefix=\\\"[^\\\"]+\\\"|maintain=\\\"%22SUMMARY_ID%22%2C%22EARLIEST_TIME%22%2C%22REMOTE_SEARCH%22%2C%22NORM_SUMMARY_ID%22%2C%22NORM_REMOTE_SEARCH%22%0A\\\" summaryprefix=\\\"[^\\\"]+\\\")$\" \n| rex \"info=granted , search_id='(?P<search_id>[^']+)\" \n| rex \"', savedsearch_name=\\\"(?P<savedsearch_name>[^\\\"]*)\" \n| `search_type_from_sid(search_id)` \n| search `comment(\"Split out the information by system vs non-system users. Adhoc/scheduled/dashboards split (as accurately as possible), furthermore you can trigger ad-hoc searches via scripted inputs which is similar to a scheduled search (but not via the scheduler)\")` \n| eval user=if(user==\"admin\" OR user==\"splunk-system-user\",\"system\",\"other\") \n| stats count AS search_count by host, type, user \n| eval prefix=\"platform_stats.audit.\" \n| rename host AS search_head \n| eval search_head_cluster=`search_head_cluster` \n| addinfo \n| rename info_max_time AS _time \n| fields - info_* \n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix search_head search_head_cluster type user\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to measure queries per day from _audit logs (note realtime_schedule = 0). Note: tested on 7.3 only, may not work on earlier versions",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "*/10 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - platform_stats.audit metrics users",
    "search": "| multisearch \n    [ search `comment(\"Count the number of of Splunk queries that use a search command\")` \n    index=_audit \", info=granted \" search_id!=\"'rsa_*\" \n    | rex \"(?s), search='(?P<search>.*)\\]$\" \n    | rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" ] \n    [ search `comment(\"Attempt to find all API calls into Splunk but do not include the API calls triggered by the local system (ignore localhost\")` \n        index=_internal sourcetype=splunkd_access source=\"*splunkd_access.log\" \"/search/jobs/export\" OR (\"/search/jobs\" method=POST) NOT control clientip!=127.0.0.1 status=200 OR status=201 ] \n| search `comment(\"Split out the information by users vs api users\")` \n| eval from=if(index==\"_audit\",\"ui\",\"rest\") \n| stats dc(user) AS active_users by host, from \n| eval prefix=\"platform_stats.audit.\" \n| rename host AS search_head \n| eval search_head_cluster=`search_head_cluster` \n| addinfo \n| rename info_max_time AS _time \n| fields - info_* \n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix search_head search_head_cluster from\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-15m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to measure counts of active REST/UI users via audit and internal indexes (note realtime_schedule = 0). Note: tested on 7.3 only, may not work on earlier versions",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "*/10 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - platform_stats.audit metrics users 24hour",
    "search": "| multisearch \n    [ search `comment(\"Count the number of of Splunk queries that use a search command\")` \n    index=_audit \", info=granted \" search_id!=\"'rsa_*\" \n    | rex \"(?s), search='(?P<search>.*)\\]$\" \n    | rex \"(?s)^(?:[^'\\n]*'){4},\\s+\\w+='(?P<search>[\\s\\S]+)'\\]($|\\[[^\\]]+\\]$)\" ] \n    [ search `comment(\"Attempt to find all API calls into Splunk but do not include the API calls triggered by the local system (ignore localhost\")` \n        index=_internal sourcetype=splunkd_access source=\"*splunkd_access.log\" \"/search/jobs/export\" OR (\"/search/jobs\" method=POST) NOT control clientip!=127.0.0.1 status=200 OR status=201 ] \n| search `comment(\"Split out the information by users vs api users\")` \n| eval from=if(index==\"_audit\",\"ui\",\"rest\")\n| eval search_head=host\n| eval search_head_cluster=`search_head_cluster`\n| stats dc(user) AS active_users_24hour, dc(host) AS host_count by search_head_cluster, from\n| eval prefix=\"platform_stats.audit.\" \n| addinfo\n| rename info_max_time AS _time\n| fields - info_*\n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix search_head search_head_cluster from\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to measure search requests using the REST API via access logs (note realtime_schedule = 0). Note: tested on 7.3 only, may not work on earlier versions",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "@h",
    "cron_schedule": "*/10 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - platform_stats.remote_searches metrics populating search",
    "search": "`comment(\"Attempt to gather stats from the remote_searches log on the indexing tier relating to the searches from various search heads. These may include search heads where we do not see the _audit index. Added regex to ignore the strange presummarize that comes in from search heads that do not have accelerated reports...\")` \nindex=_internal `indexerhosts` sourcetype=splunkd_remote_searches source=\"/opt/splunk/var/log/splunk/remote_searches.log\" terminated `comment(\"Note that TERM(starting) has the apiStartTime, apiEndTime stats, but lacks the useful stats from a search that is complete. Also note that on indexers scan_count=events_count (in my testing). Finally various fields failed to auto-extract so regexes are used now, perhaps due to the length of some searches...\")` \n| rex \"(?s) elapsedTime=(?P<elapsedTime>[0-9\\.]+), search='(?P<search>.*?)(', savedsearch_name|\\\", drop_count=\\d+)\" \n| regex search!=\"^(pretypeahead|copybuckets)\" \n| regex search!=\"^presummarize (tstats=t maintain=\\\"\\\" summaryprefix=\\\"[^\\\"]+\\\"|maintain=\\\"%22SUMMARY_ID%22%2C%22EARLIEST_TIME%22%2C%22REMOTE_SEARCH%22%2C%22NORM_SUMMARY_ID%22%2C%22NORM_REMOTE_SEARCH%22%0A\\\" summaryprefix=\\\"[^\\\"]+\\\")\\s*$\" \n| rex \"drop_count=[0-9]+, scan_count=(?P<scan_count>[0-9]+)\" \n| rex \"(,|}\\.\\.\\.) savedsearch_name=\\\"(?P<savedsearch_name>[^\\\"]*)\\\",\" \n| rex \"terminated: search_id=(?P<search_id>[^,]+)\" \n| eval indexer_cluster=`indexer_cluster_name(host)` \n| rex \"search_id=[^,]+,\\s+server=(?P<server>[^,]+)\" \n| rename server AS search_head \n| eval search_head_cluster=`search_head_cluster` \n| fillnull savedsearch_name value=\"\" \n| rex field=search_id \"^remote_(?P<sid>.*)\" \n| eval server_with_underscore = search_head. \"_\" \n| eval sid=replace(sid, server_with_underscore, \"\") \n| `search_type_from_sid(sid)` \n| eval type=if(match(search,\"^presummarize\"),\"acceleration\",type) \n| eval user=if(username==\"nobody\" OR username==\"admin\" OR (type==\"acceleration\" AND isnull(username)),\"system\",\"other\") \n| stats dc(search_id) AS search_count, max(elapsedTime) AS max_elapsed_time, avg(elapsedTime) AS avg_elapsed_time, sum(scan_count) AS total_scan_count by search_head, indexer_cluster, search_head_cluster, type, user \n| eval prefix=\"platform_stats.remote_searches.\" \n| addinfo \n| rename info_max_time AS _time \n| fields - info_* \n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix search_head, search_head_cluster, indexer_cluster, type, user\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to find stats via the remote_searches.log on the indexing tier (useful if you do not have audit logs for all search heads) (note realtime_schedule = 0). Note: tested on 7.3 only, may not work on earlier versions",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "38 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - platform_stats.user_stats.introspection metrics populating search",
    "search": "index=_introspection `indexerhosts` sourcetype=splunk_resource_usage data.search_props.sid::* data.search_props.type!=other\n| eval mem_used = 'data.mem_used' \n| eval app = 'data.search_props.app' \n| eval elapsed = 'data.elapsed' \n| eval label = 'data.search_props.label' \n| eval intro_type = 'data.search_props.type' \n| eval mode = 'data.search_props.mode' \n| eval user = 'data.search_props.user' \n| eval cpuperc = 'data.pct_cpu' \n| eval search_head = 'data.search_props.search_head' \n| eval read_mb = 'data.read_mb' \n| eval provenance='data.search_props.provenance' \n| eval label=coalesce(label, provenance) \n| eval sid='data.search_props.sid' \n| rex field=sid \"^remote_(?P<search_id_local>.*)\" \n| eval server_with_underscore = search_head . \"_\" \n| eval search_id_local=replace(search_id_local, server_with_underscore, \"\") \n| eval sid = \"'\" . sid . \"'\" \n| `search_type_from_sid(search_id_local)` \n| eval type=case(intro_type==\"ad-hoc\",if(type==\"dashboard\",\"dashboard\",intro_type),1=1,intro_type) \n| stats max(elapsed) as runtime max(mem_used) as mem_used, sum(cpuperc) AS totalCPU, avg(cpuperc) AS avgCPU, max(read_mb) AS read_mb, values(sid) AS sids by type, mode, app, user, label, host, search_head, data.pid\n| eval type=replace(type,\" \",\"-\")\n| eval search_head_cluster=`search_head_cluster`\n| eval indexer_cluster=`indexer_cluster_name(host)` \n| stats dc(sids) AS search_count, sum(totalCPU) AS total_cpu, sum(mem_used) AS total_mem_used, max(runtime) AS max_runtime, avg(runtime) AS avg_runtime, avg(avgCPU) AS avgcpu_per_indexer, sum(read_mb) AS read_mb, values(app) AS app by type, user, search_head_cluster\n| eval prefix=\"user_stats.introspection.\"\n| addinfo \n| rename info_max_time AS _time \n| fields - info_* \n| foreach user_stats.introspection.* [eval <<FIELD>>=round('<<FIELD>>',2)] \n| fillnull \n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix search_head_cluster, type, user, indexer_cluster, app\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-65m@m",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to find user metrics around CPU usage, indexer impact et cetera from the introspection index (note realtime_schedule = 0). Note: tested on 7.3 only, may not work on earlier versions",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "-5m@m",
    "cron_schedule": "33 * * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - platform_stats.users dashboards",
    "search": "| rest /servicesNS/-/-/data/ui/views timeout=900 splunk_server=local f=title f=eai:acl*\n| eval app='eai:acl.app', user='eai:acl.owner', search_head=splunk_server \n| eval search_head_cluster=`search_head_cluster` \n| stats count by search_head_cluster, user, app\n| eval prefix=\"user_stats.dashboards.\" \n| eval _time=now() \n| fields - info_* \n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix search_head_cluster, user, app\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to measure ...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "@h",
    "cron_schedule": "33 */4 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - platform_stats.users savedsearches",
    "search": "| rest /servicesNS/-/-/saved/searches timeout=900 splunk_server=local f=title f=is_scheduled f=eai:acl* f=disabled \n| eval app='eai:acl.app', user='eai:acl.owner', search_head=splunk_server \n| eval search_head_cluster=`search_head_cluster` \n| eval scheduled=case(disabled==1,0,disabled==0 AND is_scheduled==1,1,1=1,0) \n| stats count by search_head_cluster, user, scheduled, app\n| eval prefix=\"user_stats.savedsearches.\" \n| eval _time=now() \n| fields - info_*\n| search `comment(\"mcollect index=a_metrics_index split=true prefix_field=prefix search_head_cluster, user, scheduled, app\")`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Metrics? Yes. This summary (mcollect) search attempts to measure ...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "@h",
    "cron_schedule": "33 */4 * * *",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - savedsearches invalid character in splunkd",
    "search": "index=_internal `searchheadhosts` sourcetype=splunkd `splunkadmins_splunkd_source` \"invalid\" \"toAtom\"\n| rex \"(?s)^(\\S+\\s+){3}(?P<error>.*)\"\n| stats count, latest(_time) AS mostrecent, earliest(_time) AS firstseen, values(host) AS hosts by error\n| eval mostrecent=strftime(mostrecent, \"%+\"), firstseen=strftime(firstseen, \"%+\")",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d@h",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. One or more invalid character messages appeared in the Splunkd logs. This may require additional investigation.",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "17 4 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "SearchHeadLevel - splunk_search_messages dispatch",
    "search": "`comment(\"Look for dispatch problems in the splunk_search_messages sourcetype\")` \n`comment(\"This does require the limits.conf log_search_messages=true setting to be enabled to work\")`\nindex=_internal `searchheadhosts` orig_component=\"DispatchThread\" sourcetype=splunk_search_messages \n| cluster t=0.4 showcount=true \n| table _time, cluster_count, _raw \n| sort - cluster_count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "1",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-60m",
    "eai:acl.sharing": "app",
    "description": "Chance the alert requires action? Moderate. Search error messages are generally visible to users and often an issue in the environment. Note this alert requires the splunk_search_messages sourcetype and the [search]\nlog_search_messages = true\nIn the limits.conf file and then use the search_messages.log file",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "7 * * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "Splunk errors last 24 hours",
    "search": "index=_internal \" error \" NOT debug source=*splunkd.log*",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "search",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "What Access Do I Have Without REST?",
    "search": "| `whataccessdoihave`",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "sc_admin",
    "eai:acl.perms.read": "sc_admin",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "@d",
    "eai:acl.sharing": "app",
    "description": "Report only? Yes. Determine the access of the currently logged in user assuming they cannot run REST queries against the indexers. Search Head specific? Yes. Please open in search and re-execute to make this work...",
    "eai:acl.app": "SplunkAdmins",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.anonymized.eventsByTime",
    "search": "(index=_introspection OR index=_telemetry) sourcetype=splunk_telemetry source=\"http-stream\" visibility=*anonymous* | append [| savedsearch instrumentation.licenseUsage]",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.anonymous.firstEvent",
    "search": "(index=_introspection OR index=_telemetry) sourcetype=splunk_telemetry source=\"http-stream\" visibility=*anonymous* | append [savedsearch instrumentation.licenseUsage] | where date >= \"$beginDate$\" AND date <= \"$endDate$\" | head 1",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.authentication.jwt",
    "search": "| rest splunk_server=local /services/properties/authorize/tokens_auth/disabled | table value | rename value as disabled | appendcols [| search index=_audit action=create_token | stats count | rename count as created] | appendcols [| rest splunk_server=local /services/configs/conf-authentication |  eval test=if(isnull(scriptPath), 0, 1) | table test | stats sum | rename sum(test) as scriptedExtensionsEnabled] | appendcols [| search index = _internal source=*/splunkd.log jsonwebtoken validation failed | stats count | rename count as failures] | makejson output=data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.deployment.app",
    "search": "| rest /services/apps/local | eval _time=now() | fields splunk_server title updated version disabled | eval data=\"{\\\"host\\\":\\\"\"+splunk_server+\"\\\",\\\"name\\\":\\\"\"+title+\"\\\",\\\"version\\\":\\\"\"+coalesce(version, \"\")+\"\\\",\\\"enabled\\\":\"+if(disabled=0, \"true\", \"false\")+\"}\" | eval date=strftime(_time, \"%Y-%m-%d\") | fields data _time date",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.deployment.clustering.indexer",
    "search": "| makeresults annotate=true | append [localop | rest /services/cluster/config] | sort -mode | head 1 | eval data=if(mode==\"master\",\"{\\\"host\\\":\\\"\"+splunk_server+\"\\\",\\\"timezone\\\":\\\"\"+strftime(now(),\"%z\")+\"\\\",\\\"multiSite\\\":\"+multisite+\",\\\"summaryReplication\\\":\"+if(summary_replication=1,\"true\",\"false\")+\",\\\"enabled\\\":true,\\\"replicationFactor\\\":\"+tostring(replication_factor)+\",\\\"siteReplicationFactor\\\":\"+coalesce(replace(replace(site_replication_factor, \"origin\", \"\\\"origin\\\"\"), \"total\", \"\\\"total\\\"\"), \"null\")+\",\\\"siteSearchFactor\\\":\"+coalesce(replace(replace(site_search_factor, \"origin\", \"\\\"origin\\\"\"), \"total\", \"\\\"total\\\"\"),\"null\")+\",\\\"searchFactor\\\":\"+tostring(search_factor)+\"}\",\"{\\\"host\\\":\\\"\"+splunk_server+\"\\\",\\\"timezone\\\":\\\"\"+strftime(now(),\"%z\")+\"\\\",\\\"enabled\\\":false}\") | eval _time=now() | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.deployment.featureFlags",
    "search": "| rest /services/properties splunk_server=local | rename title as conf |  map maxsearches=200 search=\"| rest /services/properties/$conf$ fillContents=1 | rename title as featureFlag  | search featureFlag=_feature-flag*\" | fields featureFlag, name, creationDate, disabled, description |  makejson featureFlag, name, description, disabled, creationDate output=data | outputtelemetry input=data anonymous=true support=true component=\"deployment.featureFlags\" optinrequired=3 type=event",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.deployment.forwarders",
    "search": "index=_internal source=*metrics.log* TERM(group=tcpin_connections) (TERM(connectionType=cooked) OR TERM(connectionType=cookedSSL)) fwdType=* guid=* | rename sourceIp as forwarderHost | eval connectionType=case(fwdType==\"uf\" or fwdType==\"lwf\" or fwdType==\"full\", fwdType, 1==1,\"Splunk fwder\") | eval version=if(isnull(version),\"pre 4.2\",version) | bin _time span=1d | stats sum(kb) as kb, latest(connectionType) as connectionType, latest(arch) as arch, latest(os) as os, latest(version) as version latest(forwarderHost) as forwarderHost by guid _time | stats estdc(forwarderHost) as numHosts estdc(guid) as numInstances `instrumentation_distribution_values(kb)` by connectionType arch os version _time | eval data=\"{\\\"hosts\\\":\"+tostring(numHosts)+\",\\\"instances\\\":\"+tostring(numInstances)+\",\\\"architecture\\\":\\\"\"+arch+\"\\\",\\\"os\\\":\\\"\"+os+\"\\\",\\\"splunkVersion\\\":\\\"\"+version+\"\\\",\\\"type\\\":\\\"\"+connectionType+\"\\\",\\\"bytes\\\":{\" + `instrumentation_distribution_strings(\"kb\",1024,0)` + \"}}\" | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.deployment.index",
    "search": "| rest /services/data/indexes | join type=outer splunk_server title [| rest /services/data/indexes-extended] \n| append [| rest /services/data/indexes datatype=metric | join type=outer splunk_server title [| rest /services/data/indexes-extended datatype=metric]] \n| eval warm_bucket_size = if(isnotnull('bucket_dirs.home.warm_bucket_size'), 'bucket_dirs.home.warm_bucket_size', 'bucket_dirs.home.size') \n| eval cold_bucket_size_gb = tostring(round(coalesce('bucket_dirs.cold.bucket_size', 'bucket_dirs.cold.size', 0) / 1024, 2)) \n| eval warm_bucket_size_gb = tostring(round(coalesce(warm_bucket_size,0) / 1024, 2)) \n| eval hot_bucket_size = tostring(round(coalesce(total_size / 1024 - cold_bucket_size_gb - warm_bucket_size_gb, 0),2)) \n| eval hot_bucket_size_gb = tostring(round(coalesce(hot_bucket_size,0) / 1024, 2)) \n| eval thawed_bucket_size_gb = tostring(round(coalesce('bucket_dirs.thawed.bucket_size', 'bucket_dirs.thawed.size',0) / 1024, 2)) \n| eval warm_bucket_count = tostring(coalesce('bucket_dirs.home.warm_bucket_count', 0)) \n| eval hot_bucket_count = tostring(coalesce('bucket_dirs.home.hot_bucket_count',0)) \n| eval cold_bucket_count = tostring(coalesce('bucket_dirs.cold.bucket_count',0)) \n| eval thawed_bucket_count = tostring(coalesce('bucket_dirs.thawed.bucket_count',0)) \n| eval home_event_count = tostring(coalesce('bucket_dirs.home.event_count',0)) \n| eval cold_event_count = tostring(coalesce('bucket_dirs.cold.event_count',0)) \n| eval thawed_event_count = tostring(coalesce('bucket_dirs.thawed.event_count',0)) \n| eval home_bucket_capacity_gb = coalesce(if('homePath.maxDataSizeMB' == 0, \"\\\"unlimited\\\"\", round('homePath.maxDataSizeMB' / 1024, 2)), \"\\\"unlimited\\\"\") \n| eval cold_bucket_capacity_gb = coalesce(if('coldPath.maxDataSizeMB' == 0, \"\\\"unlimited\\\"\", round('coldPath.maxDataSizeMB' / 1024, 2)), \"\\\"unlimited\\\"\") \n| eval currentDBSizeGB = tostring(round(coalesce(currentDBSizeMB,0) / 1024, 2)) \n| eval maxTotalDataSizeGB = tostring(if(maxTotalDataSizeMB = 0, \"unlimited\", coalesce(round(maxTotalDataSizeMB / 1024, 2), \"null\"))) \n| eval minTime = tostring(coalesce(strptime(minTime,\"%Y-%m-%dT%H:%M:%S%z\"),\"null\")) \n| eval maxTime = tostring(coalesce(strptime(maxTime,\"%Y-%m-%dT%H:%M:%S%z\"),\"null\")) \n| eval total_bucket_count = tostring(if(isnotnull(total_bucket_count), total_bucket_count, 0)) \n| eval totalEventCount = tostring(coalesce(totalEventCount, 0)) \n| eval total_raw_size_gb = tostring(coalesce(round(total_raw_size / 1024, 2), \"null\")) \n| eval timeResolution = IF('metric.timestampResolution'==\"ms\",\"millisec\",\"sec\") \n| eval index_type = coalesce(datatype ,\"event\") \n| rename eai:acl.app as App \n| eval _time=now() \n| fields splunk_server, title,index_type, timeResolution,\ncurrentDBSizeGB, totalEventCount, total_bucket_count, \ntotal_raw_size_gb, minTime, maxTime, home_bucket_capacity_gb, cold_bucket_capacity_gb, \nhot_bucket_size_gb, warm_bucket_size_gb, cold_bucket_size_gb, thawed_bucket_size_gb, \nhot_bucket_count, warm_bucket_count, cold_bucket_count, thawed_bucket_count, \nhome_event_count, cold_event_count, thawed_event_count, \nmaxTotalDataSizeGB, maxHotBuckets, maxWarmDBCount App _time \n| eval data=\"{\\\"host\\\":\\\"\"+splunk_server+\"\\\",\\\"name\\\":\\\"\"+title+\"\\\",\\\"type\\\":\\\"\"+index_type+\"\\\",\\\"timeResolution\\\":\\\"\"+timeResolution+\"\\\",\\\"app\\\":\\\"\"+App+\"\\\",\\\"total\\\":{\\\"currentDBSizeGB\\\":\"+currentDBSizeGB+\",\\\"maxDataSizeGB\\\":\"+maxTotalDataSizeGB+\",\\\"events\\\":\"+totalEventCount+\",\\\"buckets\\\":\"+total_bucket_count+\",\\\"rawSizeGB\\\":\"+total_raw_size_gb+\",\\\"minTime\\\":\"+minTime+\",\\\"maxTime\\\":\"+maxTime+\"},\\\"buckets\\\":{\\\"homeCapacityGB\\\":\"+home_bucket_capacity_gb+\",\\\"homeEventCount\\\":\"+home_event_count+\",\\\"coldCapacityGB\\\":\"+cold_bucket_capacity_gb+\",\\\"hot\\\":{\\\"sizeGB\\\":\"+hot_bucket_size_gb+\",\\\"count\\\":\"+hot_bucket_count+\",\\\"max\\\":\"+maxHotBuckets+\"},\\\"warm\\\":{\\\"sizeGB\\\":\"+warm_bucket_size_gb+\",\\\"count\\\":\"+warm_bucket_count+\"},\\\"cold\\\":{\\\"sizeGB\\\":\"+cold_bucket_size_gb+\",\\\"count\\\":\"+cold_bucket_count+\",\\\"events\\\":\"+cold_event_count+\"},\\\"thawed\\\":{\\\"sizeGB\\\":\"+thawed_bucket_size_gb+\",\\\"count\\\":\"+thawed_bucket_count+\",\\\"events\\\":\"+thawed_event_count+\"}}}\" \n| eval date=strftime(_time, \"%Y-%m-%d\") | fields data _time date",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.deployment.node",
    "search": "index=_introspection sourcetype=splunk_disk_objects component::Partitions | bin _time span=1d | stats latest(data.free) as partitionFree, latest(data.capacity) as partitionCapacity by host data.fs_type data.mount_point _time | eval partitionUtilized=round(1-partitionFree/partitionCapacity,2) | eval partitions=\"{\\\"utilization\\\":\"+`instrumentation_number_format(partitionUtilized,1,2)`+\",\\\"capacity\\\":\"+`instrumentation_number_format(partitionCapacity,1048576,0)`+\",\\\"fileSystem\\\":\\\"\"+'data.fs_type' + \"\\\"}\" | stats delim=\",\" values(partitions) as partitions by host _time | rename _time as date | mvcombine partitions | rename date as _time | join type=left host _time [search index=_introspection sourcetype=splunk_resource_usage component::Hostwide | eval cpuUsage = 'data.cpu_system_pct' + 'data.cpu_user_pct' | rename data.mem_used as memUsage | bin _time span=1d | stats latest(data.cpu_count) as coreCount, latest(data.virtual_cpu_count) as virtualCoreCount, latest(data.mem) as memAvailable, latest(data.splunk_version) as splunkVersion, latest(data.cpu_arch) as cpuArch, latest(data.os_name) as osName, latest(data.os_name_ext) as osNameExt, latest(data.os_version) as osVersion, `instrumentation_distribution_values(cpuUsage)`, `instrumentation_distribution_values(memUsage)`, latest(data.instance_guid) as guid by host _time] | fillnull value=\"null\" coreCount virtualCoreCount memAvailable | eval splunkVersion=coalesce(\"\\\"\"+splunkVersion+\"\\\"\", \"null\"), cpuArch=coalesce(\"\\\"\"+cpuArch+\"\\\"\", \"null\"), osName=coalesce(\"\\\"\"+osName + \"\\\"\", \"null\"), osNameExt=coalesce(\"\\\"\"+osNameExt+\"\\\"\", \"null\"), osVersion=coalesce(\"\\\"\"+osVersion+\"\\\"\", \"null\"), guid=coalesce(\"\\\"\"+guid+\"\\\"\", \"null\") | eval data = \"{\\\"guid\\\":\"+guid+\",\\\"host\\\":\\\"\"+replace(host,\"\\\"\", \"\\\\\\\"\")+\"\\\",\\\"partitions\\\": \" + coalesce(\"[\" + partitions + \"]\", \"null\") + \",\\\"cpu\\\":{\\\"architecture\\\":\"+cpuArch+\",\\\"coreCount\\\":\" + tostring(coreCount)+ \",\\\"virtualCoreCount\\\":\"+tostring(virtualCoreCount)+\",\\\"utilization\\\":{\" + `instrumentation_distribution_strings(\"cpuUsage\",.01,2)` + \"}},\\\"memory\\\":\"+\"{\\\"capacity\\\":\"+ `instrumentation_number_format(memAvailable,1048576,0)`+\",\\\"utilization\\\":{\" + `instrumentation_distribution_strings(\"memUsage\",1/memAvailable,2)` + \"}},\\\"os\\\":\"+osName+\",\\\"osExt\\\":\"+osNameExt + \",\\\"osVersion\\\":\"+osVersion+\",\\\"splunkVersion\\\":\"+splunkVersion+\"}\" | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.lastSent",
    "search": "index=_telemetry source=telemetry sourcetype=splunk_telemetry_log status=success | fillnull value=anonymous visibility | eval anonymous_send_time = if(visibility LIKE \"%anonymous%\", _time, null) | eval license_send_time = if(visibility LIKE \"%license%\", _time, null) | eval support_send_time = if(visibility LIKE \"%support%\", _time, null) | stats latest(anonymous_send_time) as latest_anonymous_send_time latest(license_send_time) as latest_license_send_time latest(support_send_time) as latest_support_send_time",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.license.firstEvent",
    "search": "| savedsearch instrumentation.licenseUsage | where date >= \"$beginDate$\" AND date <= \"$endDate$\" | head 1",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.licenseUsage",
    "search": "NOT() | append [search index=_telemetry type=RolloverSummary | eval date=strftime(_time-43200, \"%Y-%m-%d\") | eval licenseIDs=coalesce(replace(replace(replace(replace(licenseGuids,\"\\[\",\"[\\\"\"),\"\\]\",\"\\\"]\"),\",\",\"\\\",\\\"\"),\" \", \"\"),\"null\"), subgroup_id=coalesce(subgroupId, \"Production\"), group_id=coalesce(\"\\\"\"+licenseGroup+\"\\\"\", \"null\"), lmGuid=coalesce(\"\\\"\"+guid+\"\\\"\", \"null\"), productType=coalesce(\"\\\"\"+productType+\"\\\"\", \"null\"), type_id=if(substr(stack,1,16)=\"fixed-sourcetype\", \"fixed-sourcetype\",stack) | stats max(_time) as lastTime latest(stacksz) as stack_quota, latest(poolsz) as pool_quota, sum(b) as consumption by pool stack host lmGuid licenseIDs type_id group_id subgroup_id productType date | rename stack as stack_id | eval pool=\"{\\\"quota\\\":\" + pool_quota+\",\\\"consumption\\\":\"+consumption+\"}\" | stats delim=\",\" values(pool) as pools, max(lastTime) as lastTime max(stack_quota) as stack_quota sum(consumption) as stack_consumption by stack_id group_id subgroup_id type_id lmGuid host licenseIDs productType date | mvcombine pools | eval _raw=\"{\\\"component\\\":\\\"licensing.stack\\\",\\\"data\\\":{\\\"host\\\":\\\"\"+host+\"\\\",\\\"guid\\\":\"+lmGuid+\",\\\"name\\\":\\\"\"+replace(stack_id,\"\\\"\", \"\\\\\\\"\")+\"\\\",\\\"type\\\":\\\"\" + type_id + \"\\\",\\\"subgroup\\\":\\\"\" + subgroup_id + \"\\\",\\\"product\\\":\"+productType+\",\\\"quota\\\":\" + stack_quota+\",\\\"consumption\\\":\"+stack_consumption+\",\\\"pools\\\":[\"+pools+\"],\\\"licenseIDs\\\":\"+licenseIDs+\"}, \\\"date\\\":\\\"\"+date+\"\\\",\\\"visibility\\\":\\\"anonymous,license\\\"}\", _time=lastTime]",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.licensing.stack",
    "search": "index=_telemetry source=*license_usage_summary.log* sourcetype=splunkd TERM(type=RolloverSummary) | eval date=strftime(_time, \"%m-%d-%Y\"), licenseIDs=coalesce(replace(replace(replace(replace(licenseGuids,\"\\[\",\"[\\\"\"),\"\\]\",\"\\\"]\"),\",\",\"\\\",\\\"\"),\" \", \"\"),\"null\"), subgroup_id=coalesce(subgroupId, \"Production\"), group_id=coalesce(\"\\\"\"+licenseGroup+\"\\\"\", \"null\"), lmGuid=coalesce(\"\\\"\"+guid+\"\\\"\", \"null\"), productType=coalesce(\"\\\"\"+productType+\"\\\"\", \"null\"), type_id=if(substr(stack,1,16)=\"fixed-sourcetype\", \"fixed-sourcetype\",stack) | stats latest(stacksz) as stack_quota, latest(poolsz) as pool_quota, sum(b) as consumption by pool stack host lmGuid licenseIDs type_id group_id subgroup_id productType date | rename stack as stack_id | eval pool=\"{\\\"quota\\\":\" + pool_quota+\",\\\"consumption\\\":\"+consumption+\"}\" | stats delim=\",\" values(pool) as pools, max(stack_quota) as stack_quota sum(consumption) as stack_consumption by stack_id group_id subgroup_id type_id lmGuid host licenseIDs productType date | mvcombine pools | eval data=\"{\\\"host\\\":\\\"\"+host+\"\\\",\\\"guid\\\":\"+lmGuid+\",\\\"name\\\":\\\"\"+replace(stack_id,\"\\\"\", \"\\\\\\\"\")+\"\\\",\\\"type\\\":\\\"\" + type_id + \"\\\",\\\"subgroup\\\":\\\"\" + subgroup_id + \"\\\",\\\"product\\\":\"+productType+\",\\\"quota\\\":\" + stack_quota+\",\\\"consumption\\\":\"+stack_consumption+\",\\\"pools\\\":[\"+pools+\"],\\\"licenseIDs\\\":\"+licenseIDs+\"}\" | eval _time=strptime(date, \"%m-%d-%Y\")-43200 | fields data _time",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.performance.bundleReplication",
    "search": "index=_internal source=*/metrics.log TERM(group=bundles_uploads) | bin _time span=1d | stats count as bundles_uploads_count avg(peer_count) as avg_peer_count avg(average_baseline_bundle_bytes) as avg_baseline_bundle_bytes max(average_baseline_bundle_bytes) as max_baseline_bundle_bytes avg(average_delta_bundle_bytes) as avg_delta_bundle_bytes max(average_delta_bundle_bytes) as max_delta_bundle_bytes sum(total_count) as total_count sum(delta_count) as total_delta_count sum(success_count) as total_success_count sum(baseline_count) as total_baseline_count sum(already_present_count) as total_already_present_count sum(total_msec_spent) as total_msec_spent sum(delta_msec_spent) as total_delta_msec_spent sum(total_bytes) as total_bytes sum(delta_bytes) as total_delta_bytes by host _time | makejson output=data | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.performance.bundleReplicationCycle",
    "search": "index=_internal source=*/metrics.log splunk_server=local TERM(group=bundle_replication) TERM(name=cycle_dispatch) \n| stats count(cycle_id) as cycleCount avg(peer_count) as avgPeerCount avg(peer_success_count) as avgPeerSuccessCount avg(replication_time_msec) as avgReplicationTimeMsec avg(bundle_bytes) as avgBundleBytes avg(delta_bundle_bytes) as avgDeltaBundleBytes \n| appendcols [| rest /services/search/distributed/bundle/replication/config splunk_server=local | fields replicationPolicy] \n| eval avgPeerCount=round(avgPeerCount,2) | eval avgPeerSuccessCount=round(avgPeerSuccessCount,2) \n| eval avgReplicationTimeMsec=round(avgReplicationTimeMsec,2) | eval avgBundleBytes=round(avgBundleBytes,2) | eval avgDeltaBundleBytes=round(avgDeltaBundleBytes,2) \n| makejson output=data | eval _time=now(), date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.performance.indexing",
    "search": "index=_internal TERM(group=thruput) TERM(name=index_thruput) source=*metrics.log* | bin _time span=30s | stats sum(kb) as kb sum(instantaneous_kbps) as instantaneous_kbps by host _time | bin _time span=1d | stats sum(kb) as totalKB `instrumentation_distribution_values(instantaneous_kbps)` by host _time | eval data=\"{\\\"host\\\":\\\"\"+host+\"\\\",\\\"thruput\\\":{\\\"total\\\":\" + tostring(round(totalKB*1024)) + \",\" + `instrumentation_distribution_strings(\"instantaneous_kbps\",1024,0)`+\"}}\" | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.performance.search",
    "search": "index=_audit sourcetype=audittrail TERM(action=search) TERM(info=completed) total_run_time=* | eval search_et=if(search_et=\"N/A\", 0, search_et) | eval search_lt=if(search_lt=\"N/A\", exec_time, min(exec_time,search_lt)) | eval timerange=search_lt-search_et | bin _time span=1d | stats latest(searched_buckets) as searched_buckets latest(total_slices) as total_slices latest(scan_count) as scan_count latest(timerange) as timerange latest(total_run_time) as runtime by search_id _time | stats `instrumentation_distribution_values(runtime)`, `instrumentation_distribution_values(searched_buckets)`, `instrumentation_distribution_values(total_slices)`, `instrumentation_distribution_values(scan_count)`, `instrumentation_distribution_values(timerange)` count as numSearches by _time | eval data=\"{\\\"searches\\\":\"+tostring(numSearches)+\",\\\"latency\\\":{\"+`instrumentation_distribution_strings(\"runtime\",1,2)`+\"},\\\"buckets\\\":{\"+`instrumentation_distribution_strings(\"searched_buckets\",1,2)`+\"},\\\"slices\\\":{\"+`instrumentation_distribution_strings(\"total_slices\",1,2)`+\"},\\\"scanCount\\\":{\"+`instrumentation_distribution_strings(\"scan_count\",1,2)`+\"},\\\"dayRange\\\":{\"+`instrumentation_distribution_strings(\"timerange\",1/86400,2)`+\"}}\" | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.reporting",
    "search": "index=_telemetry source=telemetry sourcetype=splunk_telemetry_log | fields _raw | spath | eval time_formatted = strftime(_time, \"%Y-%m-%d %H:%M:%S\") | search (status=success OR status=failed)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.reporting.errors",
    "search": "index=_telemetry source=telemetry sourcetype=splunk_telemetry_log status=failed visibility=*$visibility$*",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.reportingErrorCount",
    "search": "index=_telemetry source=telemetry sourcetype=splunk_telemetry_log status=failed | fillnull value=anonymous visibility | stats count(eval(visibility LIKE \"%anonymous%\")) as anonymous_errors count(eval(visibility LIKE \"%license%\")) as license_errors count(eval(visibility LIKE \"%support%\")) as support_errors",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.support.eventsByTime",
    "search": "(index=_introspection OR index=_telemetry) sourcetype=splunk_telemetry source=\"http-stream\" visibility=*support* | append [| savedsearch instrumentation.licenseUsage]",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.support.firstEvent",
    "search": "(index=_introspection OR index=_telemetry) sourcetype=splunk_telemetry source=\"http-stream\" visibility=*support* | append [savedsearch instrumentation.licenseUsage] | where date >= \"$beginDate$\" AND date <= \"$endDate$\" | head 1",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.topology.deployment.clustering.member",
    "search": "| localop | rest /services/cluster/master/peers | eval data=\"{\\\"master\\\":\\\"\"+splunk_server+\"\\\",\\\"member\\\":{\\\"host\\\":\\\"\"+label+\"\\\",\\\"guid\\\":\\\"\"+title+\"\\\",\\\"status\\\":\\\"\"+status+\"\\\"},\\\"site\\\":\\\"\"+site+\"\\\"}\" | where isnotnull(data) | eval _time=now() | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.topology.deployment.clustering.searchhead",
    "search": "| localop | rest /services/cluster/master/searchheads | where splunk_server!=label | eval data=\"{\\\"master\\\":\\\"\"+splunk_server+\"\\\",\\\"searchhead\\\":{\\\"host\\\":\\\"\"+label+\"\\\",\\\"guid\\\":\\\"\"+title+\"\\\",\\\"status\\\":\\\"\"+status+\"\\\"},\\\"site\\\":\\\"\"+site+\"\\\"}\" | where isnotnull(data) | eval _time=now() | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.topology.deployment.distsearch.peer",
    "search": "| localop | rest /services/search/distributed/peers | eval data=\"{\\\"host\\\":\\\"\"+splunk_server+\"\\\",\\\"peer\\\":{\\\"host\\\":\\\"\"+peerName+\"\\\",\\\"guid\\\":\\\"\"+guid+\"\\\",\\\"status\\\":\\\"\"+status+\"\\\"}}\" | where isnotnull(data) | eval _time=now() | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.topology.deployment.licensing.slave",
    "search": "| localop | rest /services/licenser/slaves | eval data=\"{\\\"master\\\":\\\"\"+splunk_server+\"\\\",\\\"slave\\\":{\\\"host\\\":\\\"\"+label+\"\\\",\\\"guid\\\":\\\"\"+title+\"\\\",\\\"pool\\\":\\\"\"+active_pool_ids+\"\\\"}}\" | where isnotnull(data) | eval _time=now() | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.topology.deployment.shclustering.member",
    "search": "| localop | rest /services/shcluster/captain/members | eval data=\"{\\\"site\\\":\\\"\"+site+\"\\\",\\\"captain\\\":\\\"\"+splunk_server+\"\\\",\\\"member\\\":{\\\"host\\\":\\\"\"+label+\"\\\",\\\"guid\\\":\\\"\"+title+\"\\\",\\\"status\\\":\\\"\"+status+\"\\\"}}\" | where isnotnull(data) | eval _time=now() | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.admissionRules.enabled",
    "search": "NOT() | append [rest splunk_server=local /services/workloads/status/admission-control-status | fields enabled]",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.admissionRules.report",
    "search": "|rest splunk_server=local /services/server/info | appendcols [|rest splunk_server=local /servicesNS/nobody/splunk_instrumentation/telemetry | fields telemetrySalt]| eval telemetrySalt=if(isnull(telemetrySalt), \"\", telemetrySalt), hashHost=sha1(telemetrySalt+splunk_server), roleCombine=mvjoin(server_roles, \", \") | fields guid, hashHost, roleCombine| appendcols [|savedsearch instrumentation.usage.admissionRules.enabled] | appendcols [|savedsearch instrumentation.usage.admissionRules.rules] | appendcols [|savedsearch instrumentation.usage.admissionRules.rulesTriggered] | fillnull value=0 | eval data=\"{\\\"host\\\": \\\"\"+hashHost+\"\\\", \\\"guid\\\": \\\"\"+guid+\"\\\", \\\"admissionRulesEnabled\\\": \\\"\"+enabled+\"\\\", \\\"serverRoles\\\": \\\"\"+roleCombine+\"\\\", \\\"rules\\\":{\\\"totalCount\\\":\\\"\"+ruleTotal+\"\\\"\"+if(ruleTotal>0, \", \"+ruleCombined, \"\")+\"}, \\\"rulesTriggered\\\":[\"+if(rulesTriggeredTotal>0, rulesTriggeredCombined, \"\")+\"]}\", _time=now(), date=strftime(_time, \"%Y-%m-%d\")| fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 3 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.admissionRules.rules",
    "search": "NOT() | append [rest splunk_server=local \"/services/workloads/rules?workload_rule_type=search_filter\" | `hash_admission_rule_predicate(index)` | `hash_admission_rule_predicate(app)` | `hash_admission_rule_predicate(user)` | `hash_admission_rule_predicate(role)` | eval data=\"\\\"\"+sha1(title)+\"\\\":{\\\"predicate\\\":\\\"\"+predicate+\"\\\"}\" | stats list(data) AS ruleList, count AS ruleTotal | eval ruleCombined=mvjoin(ruleList, \", \") | fields ruleTotal, ruleCombined]",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.admissionRules.rulesTriggered",
    "search": "index=_internal sourcetype=wlm_monitor prefilter_action=filter | stats count by prefilter_rule | fields prefilter_rule, count | eval data=\"{\\\"searchFilterRule\\\":\\\"\"+sha1(prefilter_rule)+\"\\\", \\\"filteredSearchesCount\\\":\\\"\"+count+\"\\\"}\" | stats list(data) AS rulesTriggered, count AS rulesTriggeredTotal | eval rulesTriggeredCombined=mvjoin(rulesTriggered, \", \") | fields rulesTriggeredTotal, rulesTriggeredCombined",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.app.page",
    "search": "index=_internal sourcetype=splunk_web_access uri_path=\"/*/app/*/*\" NOT uri_path=\"/*/static/*\" | eval uri_parts=split(uri_path, \"/\"),locale=mvindex(uri_parts,1), app=mvindex(uri_parts,3), page=mvindex(uri_parts,4) | bin _time span=1d | eventstats estdc(user) as appUsers count as appOccurrences by app _time | bin _time span=1d | stats latest(locale) as locale count as occurrences estdc(user) as users by app page appUsers appOccurrences _time | sort app -occurrences | streamstats count as pageRank by app _time | where pageRank<=10 | eval data=\"{\\\"app\\\":\\\"\"+app+\"\\\",\\\"page\\\":\\\"\"+page+\"\\\",\\\"locale\\\":\\\"\"+locale+\"\\\",\\\"occurrences\\\":\" + tostring(occurrences) + \",\\\"users\\\":\" + tostring(users) + \"}\" | eval data=if(pageRank==1,data+\";{\\\"app\\\":\\\"\"+app+\"\\\",\\\"locale\\\":\\\"\"+locale+\"\\\",\\\"occurrences\\\":\" + tostring(appOccurrences) + \",\\\"users\\\":\" + tostring(appUsers) + \"}\", data) | stats values(data) as data by app appOccurrences appUsers _time | sort _time -appOccurrences | streamstats count as appRank by _time | where appRank<=25 | mvexpand data | makemv delim=\";\" data | mvexpand data | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.authMethod.config",
    "search": "|rest splunk_server=local /services/admin/auth-services| join type=left splunk_server [|rest splunk_server=local /services/server/info | fields guid, splunk_server] | appendcols [|rest splunk_server=local /servicesNS/nobody/splunk_instrumentation/telemetry | fields telemetrySalt]| eval telemetrySalt=if(isnull(telemetrySalt), \"\", telemetrySalt), hashHost=sha1(telemetrySalt+splunk_server)| eval data=\"{\\\"host\\\": \\\"\"+hashHost+\"\\\",\\\"guid\\\": \\\"\"+guid+\"\\\", \\\"authentication method\\\": \\\"\"+active_authmodule+\"\\\",\\\"mfa type\\\": \" +\"\\\"\" + if(mfa_type==\"\", \"none\", mfa_type) +\"\\\"}\", _time=now(), date=strftime(_time, \"%Y-%m-%d\") | fields data _time date",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 3 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.bucketmerge.standalone",
    "search": "index=_internal source=*splunkd-utility.log* component=BucketMergerCmd command=merge OR command=dryrun OR command=list | table command, newBucketsCount, oldBucketsCount, durationSec | makejson command newBucketsCount oldBucketsCount durationSec output=\"data\" | eval _time=now(), date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.durableSearch",
    "search": "| rest servicesNS/-/-/admin/savedsearch | search NOT title=instrumentation.* AND NOT durable.track_time_type=\"\" | eval name=sha1(title), durableTrackTimeType='durable.track_time_type', durableLagTime='durable.lag_time', durableBackfillType='durable.backfill_type', durableMaxBackfillIntervals='durable.max_backfill_intervals', enableSummaryIndex=if('action.summary_index'==1, \"Yes\", \"No\")  | fields name, durableTrackTimeType, durableLagTime, durableBackfillType, durableMaxBackfillIntervals, enableSummaryIndex | makejson name durableTrackTimeType durableLagTime durableBackfillType durableMaxBackfillIntervals enableSummaryIndex output=\"data\" | eval _time=now(), date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.healthMonitor.currentState",
    "search": "| rest /services/apps/local splunk_server=local | search title=search | rename version as splunk_version | fields splunk_version \n | appendcols [| rest services/server/health-config/distributed_health_reporter splunk_server=local | eval dist_hr_enabled=if(isnull(disabled) or disabled=='' or disabled==0, 1, 0) | fields dist_hr_enabled] \n | appendcols [| search index=_internal earliest=-1d source=*splunkd_ui_access.log (\"server/health/splunkd/details\" OR \"server/health/deployment/details\") | stats count as click_count] \n | appendcols [ \n    | rest services/server/health/splunkd/details splunk_server=local \n    | fields + features.* health | rename health as features.health \n    | fields - *.reasons.* *.messages.* \n    | foreach features.* [ eval newname=\"splunkd.\"+replace(lower(\"<<MATCHSTR>>\"), \" \", \"_\") | rex field=newname mode=sed \"s/features\\.|\\.health//g\" | eval {newname}='<<FIELD>>'] \n    | fields - features.*, newname \n    | transpose column_name=\"features\" | rename \"row 1\" as current_color \n    | join type=outer features [ \n        | search index=_internal earliest=-1d source=*health.log component=PeriodicHealthReporter \n            | stats count as num,  values(color) as colors by node_path \n            | rename node_path as features \n            | eval colors = mvjoin(colors, \",\") \n            | eval worst_color = if(match(colors, \"red\"), \"red\", (if(match(colors, \"yellow\"), \"yellow\", \"green\"))) \n            | fields features, num, colors, worst_color \n    ] \n    | sort by features \n    | eval nodes=\"\", combin_column=1 \n    | foreach features* [eval nodes = \"{\\\"nodePath\\\": \\\"\" + '<<FIELD>>' + \"\\\", \\\"color\\\": \\\"\" + current_color + \"\\\", \\\"worstColorInLast24Hours\\\": \\\"\" + worst_color + \"\\\"}\"] \n    | stats count list(nodes) as node_list by combin_column \n    | eval node_status = mvjoin(node_list, \", \") \n ] \n | eval data = \"{\\\"splunkVersion\\\": \\\"\" + splunk_version + \"\\\", \\\"distribuedHealthReporter\\\": {\\\"enabled\\\": \" + dist_hr_enabled + \"}, \\\"healthReportClicks\\\": \" + click_count + \", \\\"nodeStatus\\\": [\" + node_status + \"]}\", _time=now(), date=strftime(_time, \"%Y-%m-%d\") \n | fields data _time date",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.healthMonitor.report",
    "search": "|rest splunk_server=local /services/server/health-config | eval thresh=\"\" | foreach indicator*red,indicator*yellow [eval thresh =if('<<FIELD>>'!=\"\", thresh+\"\\\"<<FIELD>>\\\":\" + '<<FIELD>>' + \",\", thresh)] | eval thresh=rtrim(thresh, \",\"), enabled=if(disabled=='' or disabled==0 or isnull(disabled), 1,0) | eval feature=\"\\\"\"+title+\"\\\":{\\\"threshold\\\": {\"+thresh+\"}, \\\"enabled\\\": \\\"\"+enabled+\"\\\"}\", distinct=if(like(title, \"feature%\"), \"feature\", \"alert\") | eval disable=coalesce('alert.disabled', disabled), action=coalesce('alert.actions','action.to','action.url', 'action.integration_url_override') | eval action=if(action==\"\" or isnull(action), \"empty\", action) | eval alert=\"\\\"\"+title+\"\\\": {\\\"disabled\\\": \\\"\"+disable+\"\\\", \\\"action/ action.to/ action.url/ action.integration_url_override\\\": \\\"\"+action+\"\\\"}\" | stats list(alert) AS alertList, list(feature) AS feaList by distinct | eval alertCombined=mvjoin(alertList, \",\"), feaCombined=mvjoin(feaList, \",\") | eval alertCombined=\"\\\"alert\\\":{\"+alertCombined+\"}\" | eval feaCombined=if(distinct==\"alert\", null, feaCombined), alertCombined=if(distinct==\"feature\", null, alertCombined) | eval dataCombined=coalesce(alertCombined, feaCombined) | stats list(dataCombined) AS dataList| eval data=mvjoin(dataList, \",\") | eval data=\"{\"+data+\"}\",_time=now(), date=strftime(_time, \"%Y-%m-%d\") | fields data _time date",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 3 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.indexing.sourcetype",
    "search": "index=_internal source=*metrics.log* TERM(group=per_sourcetype_thruput) | bin _time span=1d | stats sum(ev) as events, sum(kb) as size, estdc(host) as hosts by series _time | eval data=\"{\\\"name\\\":\\\"\"+replace(series,\"\\\"\", \"\\\\\\\"\") + \"\\\",\\\"events\\\":\"+tostring(events)+\",\\\"bytes\\\":\"+tostring(round(size*1024))+\",\\\"hosts\\\":\"+tostring(hosts)+\"}\" | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.kvstore",
    "search": "|rest splunk_server=local /services/kvstore/info | eval data = \"{\" | foreach usage.* [eval data = data + \"\\\"<<FIELD>>\\\":\\\"\" + '<<FIELD>>' + \"\\\", \" ] | eval data = rtrim(data, \", \") + \"}\", _time = now(), date=strftime(_time, \"%Y-%m-%d\") | fields data _time date",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.lookups.lookupDefinitions",
    "search": "|rest splunk_server=local /services/admin/transforms-lookup getsize=true | eval name = 'eai:acl.app' + \".\" + title | rename \"eai:acl.sharing\" AS sharing | eval is_temporal = if(isnull(time_field),0,1) | table name type is_temporal size sharing | join type=left name [rest splunk_server=local /services/admin/kvstore-collectionstats | table data | mvexpand data | spath input=data | table ns size | rename ns as name] | eval name=sha1(name) | makejson output=lookups | stats list(lookups) as lookups | eval data = \"{ \\\"lookups\\\" : [\" . mvjoin(lookups,\",\") . \"]}\", _time = now(), date=strftime(_time, \"%Y-%m-%d\") | fields data _time date",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.metrics",
    "search": "| mcatalog values(_dims) values(sourcetype) values(metric_type) where index=* earliest=-15m by metric_name, index | stats count(values(_dims)) AS dimension_count list(values(sourcetype)) AS sourcetype list(values(metric_type)) AS metrictype by metric_name, index | eval metrictype = if(isnull(metrictype), \"N/A\", metrictype) | fields metric_name, index,  dimension_count, sourcetype, metrictype | eval data=\"{ \\\"metricName\\\" : \\\"\"+metric_name+\"\\\", \\\"indexName\\\" : \\\"\"+index+\"\\\", \\\"dimensionCount\\\" : \\\"\"+dimension_count+\"\\\", \\\"sourcetype\\\" : \\\"\"+sourcetype+\"\\\", \\\"metricType\\\" : \\\"\"+metrictype+\"\\\"}\", _time = now(), date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.passwordPolicy.config",
    "search": "|rest splunk_server=local /services/admin/Splunk-auth/splunk_auth| join type=left splunk_server [|rest splunk_server=local /services/server/info | fields guid, splunk_server] | appendcols [|rest splunk_server=local /servicesNS/nobody/splunk_instrumentation/telemetry | fields telemetrySalt]| eval telemetrySalt=if(isnull(telemetrySalt), \"\", telemetrySalt), hashHost=sha1(telemetrySalt+splunk_server)| replace \"1\" with \"true\", \"0\" with \"false\" in enablePasswordHistory,expireUserAccounts, forceWeakPasswordChange, lockoutUsers, verboseLoginFailMsg | eval data=\"{\\\"host\\\": \\\"\"+hashHost+\"\\\",\\\"guid\\\": \\\"\"+guid+\"\\\", \\\"constant login time\\\":\\\"\"+constantLoginTime+\"\\\", \\\"enable password history\\\":\\\"\"+enablePasswordHistory+\"\\\", \\\"expiration alert in days\\\":\\\"\"+expireAlertDays+\"\\\", \\\"days until password expires\\\":\\\"\"+expirePasswordDays+\"\\\", \\\"enable password expiration\\\":\\\"\"+expireUserAccounts+\"\\\", \\\"force existing users to change weak passwords\\\":\\\"\"+forceWeakPasswordChange+\"\\\", \\\"failed login attempts\\\":\\\"\"+lockoutAttempts+\"\\\", \\\"lockout duration in minutes\\\":\\\"\"+lockoutMins+\"\\\", \\\"lockout threshold in minutes\\\":\\\"\"+lockoutThresholdMins+\"\\\", \\\"enable lockout users\\\":\\\"\"+lockoutUsers+\"\\\", \\\"minimum number of digits\\\":\\\"\"+minPasswordDigit+\"\\\", \\\"minimum number of characters\\\":\\\"\"+minPasswordLength+\"\\\", \\\"minimum number of lowercase letters\\\":\\\"\"+minPasswordLowercase+\"\\\", \\\"minimum number of special characters\\\":\\\"\"+minPasswordSpecial+\"\\\", \\\"minimum number of uppercase letters\\\":\\\"\"+minPasswordUppercase+\"\\\", \\\"password history count\\\":\\\"\"+passwordHistoryCount+\"\\\", \\\"enable verbose login fail message\\\":\\\"\"+verboseLoginFailMsg+\"\\\"}\",_time=now(), date=strftime(_time, \"%Y-%m-%d\") | fields data _time date",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 3 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.rest",
    "search": "index=_internal useragent=splunk-sdk* | eval endpointuri = case(like(uri_path, \"%/authorization/capabilities%\"), \"authorization/capabilities\", like(uri_path, \"%/authorization/roles%\"), \"authorization/roles\", like(uri_path, \"%/configs/conf-%s%\"), \"configs/conf-%s\", like(uri_path, \"%/properties/%\"), \"properties\", like(uri_path, \"%/saved/eventtypes%\"), \"saved/eventtypes\", like(uri_path, \"%/deployment/client%\"), \"deployment/client\", like(uri_path, \"%/deployment/tenants%\"), \"deployment/tenants\", like(uri_path, \"%/deployment/server%\"), \"deployment/server\", like(uri_path, \"%/deployment/serverclass%\"), \"deployment/serverclass\", like(uri_path, \"%/storage/passwords%\"), \"storage/passwords\", like(uri_path, \"%/services/receivers/stream%\"), \"/services/receivers/stream\", like(uri_path, \"%/services/receivers/simple%\"), \"/services/receivers/simple\", like(uri_path, \"%/authentication/users%\"), \"authentication/users\", like(uri_path, \"%/saved/searches%\"), \"saved/searches\", like(uri_path, \"%/data/modular_inputs%\"), \"data/modular_inputs\", like(uri_path, \"%/data/input%\"), \"data/input\", like(uri_path, \"%/data/indexes%\"), \"data/indexes\", like(uri_path, \"%/alerts/fired_alerts%\"), \"/alerts/fired_alerts\", like(uri_path, \"%messages%\"), \"messages\", like(uri_path, \"%/search/jobs%\"), \"search/jobs\" ) | stats count by endpointuri, status, method, useragent | eval _time=now(), date=strftime(_time, \"%Y-%m-%d\") | eval data=\"{ \\\"endpointUri\\\" : \\\"\"+endpointuri+\"\\\", \\\"status\\\" : \\\"\"+status+\"\\\", \\\"method\\\" : \\\"\"+method+\"\\\", \\\"useragent\\\" : \\\"\"+useragent+\"\\\", \\\"count\\\" : \\\"\"+count+\"\\\" }\" | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.rollup",
    "search": "| rest servicesNS/-/-/catalog/metricstore/rollup | eval summaryCount=0, target_index_list=\"\", metricOverrideCount=0, name=title, hasDimensionList=if(isnull(dimensionList), \"false\", \"true\") | foreach summaries*rollupIndex [| eval summaryCount=if(isnull('<<FIELD>>'), summaryCount, summaryCount+1)] | foreach aggregation* [| eval metricOverrideCount=if(isnull('<<FIELD>>'), metricOverrideCount, metricOverrideCount+1)] | foreach summaries*rollupIndex [eval target_index_list=if(isnotnull('<<FIELD>>'), target_index_list.\",\".'<<FIELD>>', target_index_list)] | eval targetIndex=split('target_index_list',\",\")| mvexpand  targetIndex | search NOT targetIndex=\"\" | join type=left targetIndex [| rest /services/data/indexes datatype=metric | eval targetIndexDBSizeGB_temp=tostring(round(coalesce(currentDBSizeMB,0) / 1024, 2)) | stats sum(targetIndexDBSizeGB_temp) as targetIndexDBSizeGB by title | rename title as targetIndex | fields targetIndex, targetIndexDBSizeGB] | fields name, defaultAggregation, summaryCount, hasDimensionList, metricOverrideCount, targetIndex, targetIndexDBSizeGB | eval targetIndexDBSizeGB=if(targetIndexDBSizeGB==0, \"0 (Check Index to Verify)\", targetIndexDBSizeGB) | makejson name defaultAggregation targetIndex targetIndexDBSizeGB hasDimensionList summaryCount(int) metricOverrideCount(int) output=\"data\" | eval _time=now(), date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.savedSearches.alerts",
    "search": "| rest servicesNS/-/-/admin/savedsearch | search NOT title=instrumentation.* | eval name=sha1(title), alertConditionType=alert_type, actionList=actions, triggerMode=if('alert.digest_mode'==1, \"Once\", \"For each result\"), alertSeverity='alert.severity', alertSuppress=if('alert.suppress'==1, \"Yes\", \"No\"), alertSuppressGroup=if('alert.suppress.group_name'==\"\", \"N/A\", sha1('alert.suppress.group_name')), alertTrackable=if('alert.track'==1, \"Yes\", \"No\"), cronSchedule=cron_schedule, dispatchAllowPartialResults=if('dispatch.allow_partial_results'==1, \"Yes\", \"No\") | fields name, alertConditionType, actionList, triggerMode, alertSeverity, alertTrackable, alertSuppress, alertSuppressGroup, cronSchedule, dispatchAllowPartialResults | makejson name alertConditionType actionList triggerMode alertSeverity alertTrackable alertSuppress cronSchedule alertSuppressGroup dispatchAllowPartialResults output=\"data\" | eval _time=now(), date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.search.concurrent",
    "search": "index=_introspection sourcetype=splunk_resource_usage component::PerProcess data.search_props.sid::* | bin _time span=10s | stats estdc(data.search_props.sid) AS concurrent_searches by _time host | bin _time span=1d | stats `instrumentation_distribution_values(concurrent_searches)` by host _time | eval data=\"{\\\"host\\\":\\\"\"+host+\"\\\",\\\"searches\\\":{\" + `instrumentation_distribution_strings(\"concurrent_searches\",1,0)` +\"}}\" | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.search.report_acceleration",
    "search": "| localop | rest /servicesNS/-/-/admin/summarization | stats count as existing_report_accelerations, sum(summary.access_count) as access_count_of_existing_report_accelerations | makejson access_count_of_existing_report_accelerations(int) existing_report_accelerations(int) output=\"data\" | eval _time=now(), date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.search.searchTelemetry",
    "search": "index=_introspection sourcetype=search_telemetry | rename search_commands{}.name as name, search_commands{}.duration as duration | stats perc50(duration), perc90(duration), perc95(duration), perc99(duration), sum(duration) as totalDuration, sum(bytes_read) as sumBytesRead, count(bytes_read) as countBytesRead max(bytes_read) as maxBytesRead by name, type | makejson output=searchTypeInformation | fields searchTypeInformation | mvcombine delim=\",\" searchTypeInformation | nomv searchTypeInformation | eval _time=now(), date=strftime(_time, \"%Y-%m-%d\") | eval data=\"{ \\\"searchTypeInformation\\\" : [\".searchTypeInformation.\"]}\" | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.search.searchtelemetry.sourcetypeUsage",
    "search": "index=_audit | stats count(sourcetype_count__*) as * | makejson output=sourcetypeUsage | fields sourcetypeUsage | mvcombine delim=\",\" sourcetypeUsage | nomv sourcetypeUsage | eval _time=now(), date=strftime(_time, \"%Y-%m-%d\") | eval data=\"{ \\\"sourcetypeUsage\\\" : [\".sourcetypeUsage.\"]}\" | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.search.type",
    "search": "index=_introspection sourcetype=splunk_resource_usage component::PerProcess data.search_props.sid::* | rename data.search_props.type as searchType | bin _time span=1d | stats estdc(data.search_props.sid) AS search_count by searchType _time | eval data=\"\\\"\"+searchType+\"\\\":\"+tostring(search_count) | stats delim=\",\" values(data) as data by _time | rename _time as date | mvcombine data | eval data=\"{\"+data+\"}\" | rename date as _time | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.smartStore.capacity",
    "search": "|rest splunk_server=local /services/server/status/partitions-space | makejson available, capacity, free, fs_type, output=\"cap\" | eval cap=\"\\\"\"+title+\"\\\": \"+cap+\"\" | stats list(cap) AS capList BY splunk_server | eval capCombined=\"\\\"total storage capacity\\\":{\" + mvjoin(capList, \", \") + \"}\" | fields capCombined, splunk_server",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.smartStore.config",
    "search": "|savedsearch instrumentation.usage.smartStore.global | join type=left splunk_server [|savedsearch instrumentation.usage.smartStore.perIndex] | join type=left splunk_server [|savedsearch instrumentation.usage.smartStore.capacity] | eval data=\"{\"+globalConfig+\", \"+capCombined+\", \"+indexConfig+\", \"+s2Enabled+\"}\",_time=now(), date=strftime(_time, \"%Y-%m-%d\") | fields data _time date",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 3 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.smartStore.global",
    "search": "|rest splunk_server=local /services/configs/conf-server | where title in (\"cachemanager\",\"diskUsage\", \"clustering\") | eval data=\"\\\"\"+title+\"\\\":\",hotlist_recency_secs=if(isnull(hotlist_recency_secs), \"none\", hotlist_recency_secs), hotlist_bloom_filter_recency_hours=if(isnull(hotlist_bloom_filter_recency_hours), \"none\", hotlist_bloom_filter_recency_hours) | eval data=if(title=\"diskUsage\", data+\"{\\\"minFreeSpace\\\":\\\"\"+minFreeSpace+\"\\\"}\", data), data=if(title=\"cachemanager\", data+\"{\\\"eviction_padding\\\":\\\"\"+eviction_padding+\"\\\",\\\"max_cache_size\\\":\\\"\"+max_cache_size+\"\\\", \\\"hotlist_recency_secs\\\":\\\"\"+hotlist_recency_secs+\"\\\", \\\"hotlist_bloom_filter_recency_hours\\\":\\\"\"+hotlist_bloom_filter_recency_hours+\"\\\"}\", data), data=if(title=\"clustering\", data+\"{\\\"mode\\\":\\\"\"+mode+\"\\\"\"+if(mode=\"master\", \",\\\"search_factor\\\":\\\"\"+search_factor+\"\\\",\\\"multisite\\\":\\\"\"+multisite+\"\\\",\\\"site_replication_factor\\\":\\\"\"+site_replication_factor+\"\\\",\\\"site_search_factor\\\":\\\"\"+site_search_factor+\"\\\"}\", \"}\"), data) | stats list(data) AS dataList BY splunk_server | eval globalConfig=\"\\\"global config\\\":{\" + mvjoin(dataList, \",\") + \"}\" | fields globalConfig, splunk_server",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.smartStore.perIndex",
    "search": "|rest splunk_server=local /services/configs/conf-indexes | appendcols [|rest splunk_server=local /servicesNS/nobody/splunk_instrumentation/telemetry | fields telemetrySalt]| eval title_dist=if(match(title, \"^([^_].*?)\\s*\"),\"external\",\"internal\"), s2Enabled=if(isnotnull(remotePath),\"SmartStore enabled\", \"non-SmartStore enabled\"),hotlist_recency_secs=if(isnull(hotlist_recency_secs), \"none\", hotlist_recency_secs), hotlist_bloom_filter_recency_hours=if(isnull(hotlist_bloom_filter_recency_hours), \"none\", hotlist_bloom_filter_recency_hours) | makejson frozenTimePeriodInSecs, hotlist_recency_secs, hotlist_bloom_filter_recency_hours, maxHotSpanSecs, maxGlobalDataSizeMB, output=\"indexConfig\" | eval telemetrySalt=if(isnull(telemetrySalt), \"\", telemetrySalt), hashTitle=sha1(telemetrySalt+title), title_combine=title_dist+\"_\"+hashTitle, indexConfig=\"\\\"\"+title_combine+\"\\\":\" + indexConfig | stats list(hashTitle) AS titleList,list(indexConfig) AS indexList BY s2Enabled, splunk_server | eval indexConfig=mvjoin(indexList, \",\"), titleCombined=\"\\\"\"+s2Enabled+\"\\\":\\\"\" + mvjoin(titleList, \",\") +\"\\\"\" | stats list(titleCombined) AS s2List, list(indexConfig) AS indexList BY splunk_server| eval s2Enabled=\"\\\"list of indexes\\\":{\" + mvjoin(s2List, \",\") + \"}\", indexConfig=\"\\\"per index config\\\":{\" + mvjoin(indexList, \",\") + \"}\"  | fields s2Enabled, indexConfig, splunk_server",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.streamingMetricAlerts",
    "search": "| rest servicesNS/-/-/alerts/metric_alerts | eval name=sha1(title), hasFilter=if(filter==\"\", \"No\", \"Yes\"), hasGroupby=if(groupby==\"\", \"No\", \"Yes\"), triggerActionPerGroup=if('trigger.action_per_group'==1, \"Yes\", \"No\"), triggerEvaluationPerGroup=if('trigger.evaluation_per_group'==1, \"Yes\", \"No\"), triggerSuppress=if('trigger.suppress'==1, \"Yes\", \"No\"), triggerPrepare=if(isnotnull('trigger.prepare'), \"Yes\", \"No\"), alertTrackable=if('splunk_ui.track'==1, \"Yes\", \"No\"), triggerThreshold=if(isnotnull('trigger.threshold'), 'trigger.threshold', \"N/A\"), actionList=\"\", hasDescription=if(description==\"\", \"No\", \"Yes\"), alertSeverity=if(isnull('splunk_ui.severity'), \"N/A\", 'splunk_ui.severity'), triggerExpires='trigger.expires', triggerMaxTracked='trigger.max_tracked' | eval actionList=if('action.email'==1, actionList.\",\".\"email\", actionList), actionList=if('action.logevent'==1, actionList.\",\".\"logevent\", actionList), actionList=if('action.rss'==1, actionList.\",\".\"rss\", actionList), actionList=if('action.script'==1, actionList.\",\".\"script\", actionList), actionList=if('action.webhook'==1, actionList.\",\".\"webhook\", actionList) | eval actionList=if(actionList==\"\", actionList, substr(actionList,2)) | eval hasLabels=\"No\", hasComplexCondition=\"No\" | foreach label* [| eval hasLabels=if(isnull('<<FIELD>>'), hasLabels, \"Yes\")] | eval hasMultipleMetricIndexes=if(match(metric_indexes, \",\"), \"Yes\", \"No\"), hasComplexCondition=if(match(condition, \"OR\") OR match(condition, \"AND\"), \"Yes\", hasComplexCondition) | fields name, hasFilter, hasGroupby, triggerActionPerGroup, triggerEvaluationPerGroup, triggerThreshold, triggerSuppress, triggerPrepare, alertTrackable, actionList, hasDescription, alertSeverity, triggerExpires, triggerMaxTracked, hasLabels, hasMultipleMetricIndexes, hasComplexCondition | makejson name hasFilter hasGroupby triggerActionPerGroup triggerEvaluationPerGroup triggerThreshold triggerSuppress triggerPrepare alertTrackable actionList hasDescription alertSeverity triggerExpires triggerMaxTracked hasLabels hasMultipleMetricIndexes hasComplexCondition output=\"data\" | eval _time=now(), date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.users.active",
    "search": "index=_audit sourcetype=audittrail TERM(action=search) user!=\"splunk-system-user\" user!=\"n/a\" | bin _time span=1d | stats estdc(user) as active by _time | eval data=\"{\\\"active\\\":\"+tostring(active)+\"}\" | eval date=strftime(_time, \"%Y-%m-%d\") | fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.workloadManagement.category",
    "search": "NOT() | append [rest splunk_server=local /services/workloads/categories | eval data=\"\\\"\"+title+\"\\\":{\\\"allocated cpu percent\\\":\\\"\"+cpu_allocated_percent+\"\\\", \\\"allocated mem limit\\\":\\\"\"+mem_allocated_percent+\"\\\"}\" | stats list(data) AS categoryList | eval categoryCombined=mvjoin(categoryList, \", \") | fields categoryCombined]",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.workloadManagement.enabled",
    "search": "NOT() | append [rest splunk_server=local /services/workloads/status | eval support='general.isSupported', enabled='general.enabled', os_name='general.os_name', os_version='general.os_version'| fields support, enabled, os_name, os_version]",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.workloadManagement.pools",
    "search": "NOT() | append [rest splunk_server=local /services/workloads/pools | eval isDeafultPool=if(default_category_pool=1, \"True\", \"False\"), poolList=\"\\\"\"+title+\"\\\":{\\\"allocated cpu percent\\\":\\\"\"+cpu_allocated_percent+\"\\\", \\\"allocated mem limit\\\":\\\"\"+mem_allocated_percent+\"\\\", \\\"default category pool\\\":\\\"\"+isDeafultPool+\"\\\"}\" | stats list(poolList) AS poolList, count BY category | eval poolList=\"\\\"\"+category+\"\\\":{\\\"count\\\":\"+count+\",\"+mvjoin(poolList, \", \")+\"}\" | stats sum(count) AS poolTotal list(poolList) AS poolList| eval poolCombined=mvjoin(poolList, \", \") | fields poolCombined, poolTotal]",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.workloadManagement.report",
    "search": "|rest splunk_server=local /services/server/info | appendcols [|rest splunk_server=local /servicesNS/nobody/splunk_instrumentation/telemetry | fields telemetrySalt]| eval telemetrySalt=if(isnull(telemetrySalt), \"\", telemetrySalt), hashHost=sha1(telemetrySalt+splunk_server), roleCombine=mvjoin(server_roles, \", \") | fields guid, hashHost, roleCombine| appendcols [|savedsearch instrumentation.usage.workloadManagement.enabled] | appendcols [|savedsearch instrumentation.usage.workloadManagement.category]| appendcols [|savedsearch instrumentation.usage.workloadManagement.pools] | appendcols [|savedsearch instrumentation.usage.workloadManagement.rules] | fillnull value=0 | eval data=\"{\\\"host\\\": \\\"\"+hashHost+\"\\\", \\\"guid\\\": \\\"\"+guid+\"\\\", \\\"wlm supported\\\": \\\"\"+support+\"\\\", \\\"os\\\": \\\"\"+os_name+\"\\\", \\\"osVersion\\\": \\\"\"+os_version+\"\\\", \\\"wlm enabled\\\": \\\"\"+enabled+\"\\\", \\\"server roles\\\": \\\"\"+roleCombine+\"\\\"\", poolTotal=if(isnull(poolTotal),0, poolTotal), ruleTotal=if(isnull(ruleTotal),0, ruleTotal) | eval data=if(support==1, data+\", \\\"categories\\\":{\"+categoryCombined+\"}, \\\"pools\\\":{\\\"total count\\\":\\\"\"+poolTotal+\"\\\"\"+ if(poolTotal>0, \", \"+poolCombined+\"\", \"\") + \"}, \\\"rules\\\":{\\\"total count\\\":\\\"\"+ruleTotal+\"\\\"\"+if(ruleTotal>0, \", \"+ruleCombined, \"\")+\"}}\", data+\"}\"), _time=now(), date=strftime(_time, \"%Y-%m-%d\")| fields _time date data",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1w",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "now",
    "cron_schedule": "0 3 * * 1",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "instrumentation.usage.workloadManagement.rules",
    "search": "NOT() | append [rest splunk_server=local /services/workloads/rules | eval data=\"\\\"\"+title+\"\\\":{\\\"order\\\":\\\"\"+order+\"\\\", \\\"predicate\\\":\\\"\"+predicate+\"\\\", \\\"workload pool\\\":\\\"\"+workload_pool+\"\\\"}\" | stats list(data) AS ruleList, count AS ruleTotal by splunk_server | eval ruleCombined=mvjoin(ruleList, \", \") | fields ruleTotal, ruleCombined]",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "splunk_instrumentation",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "nonreporting_search",
    "search": "index=_internal | head 1000",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "simple_xml_examples",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "notify",
    "search": "",
    "updated": "",
    "disabled": "",
    "eai:acl.perms.write": "",
    "eai:acl.perms.read": "",
    "dispatchAs": "",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "",
    "description": "",
    "eai:acl.app": "",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "",
    "schedule_priority": "",
    "eai:acl.owner": "",
    "user": "splunk-system-user",
    "status": "200",
    "first_time": "1667934000.644",
    "last_time": "1668022625.566"
}
{
    "title": "realtime_search",
    "search": "index=_internal | head 1000",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "rt-1m",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "simple_xml_examples",
    "dispatch.latest_time": "rtnow",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "reporting_search_table",
    "search": "index=_internal | head 1000 | top limit=100 sourcetype | eval percent = round(percent,2)",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "simple_xml_examples",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "reporting_search_timechart",
    "search": "index=_internal | head 1000 | timechart count",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "power",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "",
    "eai:acl.sharing": "app",
    "description": "",
    "eai:acl.app": "simple_xml_examples",
    "dispatch.latest_time": "",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "splunk_app_db_connect.apimetricscollector: calculate indexed data volume",
    "search": "| search index=_internal sourcetype=splunkd source=*metrics.log splunk_server=\"*\" group=\"per_sourcetype_thruput\" \n         | stats sum(kb) as sum_kb by series | eval sum_mb=sum_kb/1024 \n         | filterdbxsourcetype",
    "updated": "1969-12-31T19:00:00-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "db_connect_admin",
    "eai:acl.perms.read": "db_connect_user",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-1d@d",
    "eai:acl.sharing": "app",
    "description": "Calculate the amount of data indexed in Splunk by sourcetype",
    "eai:acl.app": "splunk_app_db_connect",
    "dispatch.latest_time": "@d",
    "cron_schedule": "20 0 * * *",
    "is_scheduled": "1",
    "schedule_priority": "default",
    "eai:acl.owner": "nobody",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "test",
    "search": "| makeresults | eval app=\"search\" | eval search_name=\"test\"",
    "updated": "2022-03-08T11:46:07-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "admin",
    "eai:acl.perms.read": "*",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "global",
    "description": "",
    "eai:acl.app": "search",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "admin",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
{
    "title": "test2",
    "search": "| makeresults | eval app=\"search\" | eval search_name=\"test\" | changeownership app=app search_name=search_name",
    "updated": "2022-03-07T15:52:59-05:00",
    "disabled": "0",
    "eai:acl.perms.write": "",
    "eai:acl.perms.read": "",
    "dispatchAs": "owner",
    "dispatch.earliest_time": "-24h@h",
    "eai:acl.sharing": "user",
    "description": "",
    "eai:acl.app": "search",
    "dispatch.latest_time": "now",
    "cron_schedule": "",
    "is_scheduled": "0",
    "schedule_priority": "default",
    "eai:acl.owner": "ess_job",
    "user": "",
    "status": "",
    "first_time": "",
    "last_time": ""
}
